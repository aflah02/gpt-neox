[2025-01-06 10:02:46,926] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
NeoXArgs.from_ymls() ['./configs/1-3B.yml', './configs/local_setup_wandb_modified.yml']
-------------------- arguments --------------------
  attention_config ................ ['flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash']updated
  batch_size ...................... 54..........................updated
  checkpoint_activations .......... True........................updated
  checkpoint_factor ............... 10000.......................updated
  config_files .................... {'1-3B.yml': '# GPT-2 pretraining setup\n{\n   # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages\n   # across the node boundaries )\n   "pipe_parallel_size": 1,\n   "model_parallel_size": 1,\n\n   # model settings\n   "num_layers": 24,\n   "hidden_size": 2048,\n   "num_attention_heads": 16,\n   "seq_length": 2048,\n   "max_position_embeddings": 2048,\n   "norm": "layernorm",\n   "pos_emb": "rotary",\n   "no_weight_tying": true,\n   "gpt_j_residual": false,\n   "output_layer_parallelism": "column",\n\n   # these should provide some speedup but takes a while to build, set to true if desired\n   "scaled_upper_triang_masked_softmax_fusion": false,\n   "bias_gelu_fusion": false,\n   "rope_fusion": false,\n   "layernorm_fusion": false,\n\n   # init methods\n   "init_method": "small_init",\n   "output_layer_init_method": "wang_init",\n\n   # optimizer settings\n   "optimizer": {\n     "type": "Adam",\n     "params": {\n       "lr": 0.0002,\n       "betas": [0.9, 0.95],\n       "eps":  1.0e-8,\n     }\n   },\n   "min_lr": 0.00002,\n\n   # for all zero_optimization options, see https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training\n   "zero_optimization": {\n    "stage": 1,\n    "allgather_partitions": True,\n    "allgather_bucket_size": 500000000,\n    "overlap_comm": True,\n    "reduce_scatter": True,\n    "reduce_bucket_size": 500000000,\n    "contiguous_gradients": True,\n  },\n\n   # batch / data settings\n   "train_micro_batch_size_per_gpu": 54,\n   "data_impl": "mmap",\n\n   # activation checkpointing\n   "checkpoint_activations": true,\n   "checkpoint_num_layers": 1,\n   "partition_activations": true,\n   "synchronize_each_layer": true,\n\n   # regularization\n   "gradient_clipping": 1.0,\n   "weight_decay": 0.1,\n   "hidden_dropout": 0,\n   "attention_dropout": 0,\n\n   # Flash Attention\n   "attention_config": [[["flash"], 24]],\n\n   # precision settings\n   "fp16": {\n     "fp16": true,\n     "enabled": true,\n     "loss_scale": 0,\n     "loss_scale_window": 1000,\n     "hysteresis": 2,\n     "min_loss_scale": 1\n   },\n\n   # misc. training settings\n   "train_iters": 320000,\n   "lr_decay_iters": 320000,\n   "distributed_backend": "nccl",\n   "lr_decay_style": "cosine",\n   "warmup": 0.01,\n   "checkpoint_factor": 10000,\n   "eval_interval": 1000,\n   "eval_iters": 10,\n\n   # logging\n   "log_interval": 10,\n   "steps_per_print": 10,\n   "keep_last_n_checkpoints": 4,\n   "wall_clock_breakdown": true,\n}\n', 'local_setup_wandb_modified.yml': '# Suggested data paths when using GPT-NeoX locally\n{\n  "data_path": "data/enwik8/enwik8_text_document",\n\n  # or for weighted datasets:\n  # "train-data-paths": ["data/enwik8/enwik8_text_document", "data/enwik8/enwik8_text_document"],\n  # "test-data-paths": ["data/enwik8/enwik8_text_document", "data/enwik8/enwik8_text_document"],\n  # "valid-data-paths": ["data/enwik8/enwik8_text_document", "data/enwik8/enwik8_text_document"],\n  # "train-data-weights": [1., 2.],\n  # "test-data-weights": [2., 1.],\n  # "valid-data-weights": [0.5, 0.4],\n\n  # If weight_by_num_documents is True, Builds dataset weights from a multinomial distribution over groups of data according to the number of documents in each group.\n  # WARNING: setting this to True will override any user provided weights\n  # "weight_by_num_documents": false,\n  # "weighted_sampler_alpha": 0.3,\n\n  "vocab_file": "data/gpt2-vocab.json",\n  "merge_file": "data/gpt2-merges.txt",\n\n  "save": "checkpoints",\n  "load": "checkpoints",\n  "checkpoint_validation_with_forward_pass": False,\n\n  "tensorboard_dir": "tensorboard",\n  "log_dir": "logs",\n  "use_wandb": True,\n  "wandb_host": "https://api.wandb.ai",\n  "wandb_project": "neox",\n  "wandb_run_name": "1.3B-FA-BS-54-1x8xA100",\n\n  "peak_theoretical_tflops": 312\n}\n'}updated
  data_impl ....................... mmap........................updated
  data_path ....................... data/enwik8/enwik8_text_documentupdated
  dynamic_loss_scale .............. True........................updated
  eval_iters ...................... 10..........................updated
  fp16 ............................ {'fp16': True, 'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}updated
  global_num_gpus ................. 8...........................updated
  hidden_size ..................... 2048........................updated
  init_method ..................... small_init..................updated
  is_pipe_parallel ................ True........................updated
  keep_last_n_checkpoints ......... 4...........................updated
  load ............................ checkpoints.................updated
  log_dir ......................... logs........................updated
  log_interval .................... 10..........................updated
  lr .............................. 0.0002......................updated
  lr_decay_iters .................. 320000......................updated
  lr_decay_style .................. cosine......................updated
  max_position_embeddings ......... 2048........................updated
  merge_file ...................... data/gpt2-merges.txt........updated
  min_lr .......................... 2e-05.......................updated
  no_weight_tying ................. True........................updated
  num_attention_heads ............. 16..........................updated
  num_layers ...................... 24..........................updated
  optimizer ....................... {'type': 'Adam', 'params': {'lr': 0.0002, 'betas': [0.9, 0.95], 'eps': 1e-08}}updated
  optimizer_type .................. Adam........................updated
  output_layer_init_method ........ wang_init...................updated
  partition_activations ........... True........................updated
  peak_theoretical_tflops ......... 312.........................updated
  pipe_parallel_size .............. 1...........................updated
  pos_emb ......................... rotary......................updated
  precision ....................... fp16........................updated
  save ............................ checkpoints.................updated
  seq_length ...................... 2048........................updated
  sparsity_config ................. {}..........................updated
  synchronize_each_layer .......... True........................updated
  tensorboard_dir ................. tensorboard.................updated
  text_gen_type ................... unconditional...............updated
  train_batch_size ................ 432.........................updated
  train_iters ..................... 320000......................updated
  train_micro_batch_size_per_gpu .. 54..........................updated
  use_wandb ....................... True........................updated
  user_script ..................... train.py....................updated
  vocab_file ...................... data/gpt2-vocab.json........updated
  wall_clock_breakdown ............ True........................updated
  wandb_group ..................... mqf6ylfi_48rsp80h...........updated
  wandb_run_name .................. 1.3B-FA-BS-54-1x8xA100......updated
  zero_allgather_bucket_size ...... 500000000...................updated
  zero_contiguous_gradients ....... True........................updated
  zero_optimization ............... {'stage': 1, 'allgather_partitions': True, 'allgather_bucket_size': 500000000, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 500000000, 'contiguous_gradients': True}updated
  zero_reduce_bucket_size ......... 500000000...................updated
  zero_reduce_scatter ............. True........................updated
  zero_stage ...................... 1...........................updated
  account ......................... None........................default
  activation ...................... gelu........................default
  activation_checkpointing ........ None........................default
  adlr_autoresume ................. False.......................default
  adlr_autoresume_interval ........ 1000........................default
  allow_chopped ................... True........................default
  amp ............................. None........................default
  apply_query_key_layer_scaling ... False.......................default
  attention_dropout ............... 0...........................default
  attention_softmax_in_fp32 ....... False.......................default
  autotuning ...................... None........................default
  autotuning_run .................. None........................default
  base_shapes_file ................ None........................default
  bf16 ............................ None........................default
  bias_dropout_fusion ............. False.......................default
  bias_gelu_fusion ................ False.......................default
  char_level_ppl .................. False.......................default
  checkpoint ...................... None........................default
  checkpoint_in_cpu ............... False.......................default
  checkpoint_num_layers ........... 1...........................default
  checkpoint_scale ................ linear......................default
  checkpoint_validation_with_forward_pass  False................default
  clip_grad ....................... 1.0.........................default
  comet_experiment ................ None........................default
  comet_experiment_name ........... None........................default
  comet_others .................... None........................default
  comet_project ................... None........................default
  comet_tags ...................... None........................default
  comet_workspace ................. None........................default
  comment ......................... None........................default
  comms_logger .................... None........................default
  communication_data_type ......... None........................default
  compression_training ............ None........................default
  contiguous_checkpointing ........ False.......................default
  coord_check ..................... False.......................default
  create_moe_param_group .......... True........................default
  csv_monitor ..................... None........................default
  curriculum_learning ............. None........................default
  curriculum_seqlen ............... 0...........................default
  data_efficiency ................. None........................default
  data_types ...................... None........................default
  dataset_impl .................... gpt2........................default
  deepscale ....................... False.......................default
  deepscale_config ................ None........................default
  deepspeed ....................... True........................default
  deepspeed_activation_checkpointing  True......................default
  deepspeed_extra_args ............ None........................default
  deepspeed_mpi ................... False.......................default
  deepspeed_slurm ................. False.......................default
  detect_nvlink_pairs ............. False.......................default
  dim_att ......................... None........................default
  distributed_backend ............. nccl........................default
  do_test ......................... None........................default
  do_train ........................ None........................default
  do_valid ........................ None........................default
  dpo_beta ........................ 0.1.........................default
  dpo_fp32 ........................ True........................default
  dpo_reference_free .............. False.......................default
  dump_state ...................... False.......................default
  elasticity ...................... None........................default
  enable_expert_tensor_parallelism  False.......................default
  eod_mask_loss ................... False.......................default
  eval_interval ................... 1000........................default
  eval_results_prefix ............. ............................default
  eval_tasks ...................... None........................default
  exclude ......................... None........................default
  exit_interval ................... None........................default
  expansion_factor ................ None........................default
  expert_interval ................. 2...........................default
  extra_save_iters ................ None........................default
  ffn_dim ......................... None........................default
  finetune ........................ False.......................default
  flops_profiler .................. None........................default
  force_multi ..................... False.......................default
  fp16_lm_cross_entropy ........... False.......................default
  fp32_allreduce .................. False.......................default
  fp32_reinforce .................. True........................default
  git_hash ........................ 0af74c0.....................default
  gmlp_attn_dim ................... 64..........................default
  gpt_j_residual .................. False.......................default
  gpt_j_tied ...................... False.......................default
  gradient_accumulation_steps ..... 1...........................default
  gradient_clipping ............... 1.0.........................default
  gradient_noise_scale_cpu_offload  False.......................default
  gradient_noise_scale_n_batches .. 5...........................default
  gradient_predivide_factor ....... 1.0.........................default
  head_size ....................... None........................default
  hidden_dropout .................. 0...........................default
  hostfile ........................ None........................default
  hysteresis ...................... 2...........................default
  include ......................... None........................default
  init_method_std ................. 0.02........................default
  intermediate_size ............... None........................default
  iteration ....................... None........................default
  kl_div_beta ..................... 0.1.........................default
  kl_impl ......................... mse.........................default
  kto_beta ........................ 0.1.........................default
  kto_desirable_weight ............ 1.0.........................default
  kto_fp32 ........................ True........................default
  kto_undesirable_weight .......... 1.0.........................default
  launcher ........................ pdsh........................default
  layernorm_epsilon ............... 1e-05.......................default
  layernorm_fusion ................ False.......................default
  lazy_mpu_init ................... False.......................default
  local_rank ...................... None........................default
  log_grad_norm ................... False.......................default
  log_grad_pct_zeros .............. False.......................default
  log_gradient_noise_scale ........ False.......................default
  log_optimizer_states ............ False.......................default
  log_param_norm .................. False.......................default
  loss_scale ...................... None........................default
  loss_scale_window ............... 1000.0......................default
  lr_decay_fraction ............... None........................default
  make_vocab_size_divisible_by .... 128.........................default
  mamba_causal_conv_fusion ........ False.......................default
  mamba_inner_func_fusion ......... False.......................default
  mamba_selective_fp32_params ..... True........................default
  mamba_selective_scan_fusion ..... False.......................default
  mamba_use_bias_in_conv .......... True........................default
  mamba_use_bias_in_linears ....... False.......................default
  master_addr ..................... None........................default
  master_port ..................... 29500.......................default
  maximum_tokens .................. 64..........................default
  memory_profiling ................ False.......................default
  memory_profiling_path ........... None........................default
  min_scale ....................... 1.0.........................default
  mlp_multiple_of ................. 1...........................default
  mmap_warmup ..................... False.......................default
  model_parallel_size ............. 1...........................default
  moe_eval_capacity_factor ........ 1.0.........................default
  moe_expert_parallel_size ........ 1...........................default
  moe_glu ......................... False.......................default
  moe_jitter_eps .................. None........................default
  moe_lbl_in_fp32 ................. False.......................default
  moe_loss_coeff .................. 0.1.........................default
  moe_min_capacity ................ 4...........................default
  moe_num_experts ................. 1...........................default
  moe_token_dropping .............. False.......................default
  moe_top_k ....................... 1...........................default
  moe_train_capacity_factor ....... 1.0.........................default
  moe_type ........................ megablocks..................default
  moe_use_residual ................ True........................default
  mup_attn_temp ................... 1.0.........................default
  mup_embedding_mult .............. 1.0.........................default
  mup_init_scale .................. 1.0.........................default
  mup_output_temp ................. 1.0.........................default
  mup_rp_embedding_mult ........... 1.0.........................default
  mup_width_scale ................. 2...........................default
  neg_test_data_paths ............. None........................default
  neg_test_label_data_paths ....... None........................default
  neg_train_data_paths ............ None........................default
  neg_train_label_data_paths ...... None........................default
  neg_valid_data_paths ............ None........................default
  neg_valid_label_data_paths ...... None........................default
  no_load_optim ................... False.......................default
  no_load_rng ..................... False.......................default
  no_save_optim ................... False.......................default
  no_save_rng ..................... False.......................default
  no_ssh_check .................... False.......................default
  norm ............................ layernorm...................default
  num_gpus ........................ None........................default
  num_kv_heads .................... None........................default
  num_nodes ....................... -1..........................default
  num_samples ..................... 1...........................default
  num_unique_layers ............... None........................default
  num_workers ..................... 2...........................default
  online_dataserver_ips ........... localhost...................default
  online_dataserver_ports ......... 10000.......................default
  onnx_safe ....................... False.......................default
  opt_pos_emb_offset .............. 0...........................default
  output_layer_parallelism ........ column......................default
  override_lr_scheduler ........... False.......................default
  pack_impl ....................... packed......................default
  padded_vocab_size ............... None........................default
  param_sharing_style ............. grouped.....................default
  pipe_partition_method ........... type:transformer|mlp........default
  pos_test_data_paths ............. None........................default
  pos_test_label_data_paths ....... None........................default
  pos_train_data_paths ............ None........................default
  pos_train_label_data_paths ...... None........................default
  pos_valid_data_paths ............ None........................default
  pos_valid_label_data_paths ...... None........................default
  precompute_model_name ........... None........................default
  prescale_gradients .............. False.......................default
  profile ......................... False.......................default
  profile_backward ................ False.......................default
  profile_step_start .............. 10..........................default
  profile_step_stop ............... 12..........................default
  prompt_end ...................... 
...........................default
  rank ............................ None........................default
  recompute ....................... False.......................default
  reinforce_leave_one_out ......... False.......................default
  return_logits ................... False.......................default
  rms_norm_epsilon ................ 1e-08.......................default
  rmsnorm_fusion .................. False.......................default
  rope_fusion ..................... False.......................default
  rotary_emb_base ................. 10000.......................default
  rotary_pct ...................... 1.0.........................default
  rotary_save_freqs_buffer ........ False.......................default
  rpe_max_distance ................ 128.........................default
  rpe_num_buckets ................. 32..........................default
  s3_chunk_size ................... 104857600...................default
  s3_path ......................... None........................default
  sample_input_file ............... None........................default
  sample_output_file .............. samples.txt.................default
  save_base_shapes ................ False.......................default
  scaled_masked_softmax_fusion .... False.......................default
  scaled_upper_triang_masked_softmax_fusion  False..............default
  scalenorm_epsilon ............... 1e-08.......................default
  scheduler ....................... None........................default
  seed ............................ 1234........................default
  sequence_parallel ............... False.......................default
  serve_model_weights ............. False.......................default
  short_seq_prob .................. 0.1.........................default
  sliding_window_width ............ None........................default
  soft_prompt_tuning .............. None........................default
  sparse_attention ................ None........................default
  sparse_gradients ................ False.......................default
  split ........................... 969, 30, 1..................default
  steps_per_print ................. 10..........................default
  te_columnparallel ............... False.......................default
  te_fp8_amax_compute_algo ........ most_recent.................default
  te_fp8_amax_history_len ......... 1...........................default
  te_fp8_format ................... hybrid......................default
  te_fp8_margin ................... 0...........................default
  te_fp8_mha ...................... False.......................default
  te_fp8_wgrad .................... True........................default
  te_layernorm_mlp ................ False.......................default
  te_mha .......................... False.......................default
  te_rowparallel .................. False.......................default
  temperature ..................... 0.0.........................default
  tensorboard ..................... None........................default
  test_data_paths ................. None........................default
  test_data_weights ............... None........................default
  test_label_data_paths ........... None........................default
  test_reward_data_paths .......... None........................default
  tokenizer_type .................. GPT2BPETokenizer............default
  top_k ........................... 0...........................default
  top_p ........................... 0.0.........................default
  train_data_paths ................ None........................default
  train_data_weights .............. None........................default
  train_epochs .................... None........................default
  train_impl ...................... normal......................default
  train_label_data_paths .......... None........................default
  train_reward_data_paths ......... None........................default
  use_bias_in_attn_linear ......... True........................default
  use_bias_in_mlp ................. True........................default
  use_bias_in_norms ............... True........................default
  use_bnb_optimizer ............... False.......................default
  use_checkpoint_lr_scheduler ..... False.......................default
  use_comet ....................... None........................default
  use_cpu_initialization .......... False.......................default
  use_flashattn_swiglu ............ False.......................default
  use_mup ......................... False.......................default
  use_qk_layernorm ................ False.......................default
  use_shared_fs ................... True........................default
  use_tutel ....................... False.......................default
  valid_data_paths ................ None........................default
  valid_data_weights .............. None........................default
  valid_label_data_paths .......... None........................default
  valid_reward_data_paths ......... None........................default
  wandb ........................... None........................default
  wandb_host ...................... https://api.wandb.ai........default
  wandb_init_all_ranks ............ False.......................default
  wandb_project ................... neox........................default
  wandb_team ...................... None........................default
  warmup .......................... 0.01........................default
  weight_by_num_documents ......... False.......................default
  weight_decay .................... 0.1.........................default
  weight_server_port .............. 6000........................default
  weighted_sampler_alpha .......... 1.0.........................default
  world_size ...................... None........................default
  z_loss .......................... 0.0.........................default
---------------- end of arguments ----------------
NeoXArgs.configure_distributed_args() using world size: 1 and model-parallel size: 1 
[2025-01-06 10:03:42,956] [WARNING] [runner.py:217:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7: setting --include=localhost:0,1,2,3,4,5,6,7
[2025-01-06 10:03:42,957] [INFO] [runner.py:586:main] cmd = /NS/venvs/work/afkhan/neoxolmo/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train.py --deepspeed_config eyJ0cmFpbl9iYXRjaF9zaXplIjogNDMyLCAidHJhaW5fbWljcm9fYmF0Y2hfc2l6ZV9wZXJfZ3B1IjogNTQsICJvcHRpbWl6ZXIiOiB7InR5cGUiOiAiQWRhbSIsICJwYXJhbXMiOiB7ImxyIjogMC4wMDAyLCAiYmV0YXMiOiBbMC45LCAwLjk1XSwgImVwcyI6IDFlLTA4fX0sICJmcDE2IjogeyJmcDE2IjogdHJ1ZSwgImVuYWJsZWQiOiB0cnVlLCAibG9zc19zY2FsZSI6IDAsICJsb3NzX3NjYWxlX3dpbmRvdyI6IDEwMDAsICJoeXN0ZXJlc2lzIjogMiwgIm1pbl9sb3NzX3NjYWxlIjogMX0sICJ6ZXJvX29wdGltaXphdGlvbiI6IHsic3RhZ2UiOiAxLCAiYWxsZ2F0aGVyX3BhcnRpdGlvbnMiOiB0cnVlLCAiYWxsZ2F0aGVyX2J1Y2tldF9zaXplIjogNTAwMDAwMDAwLCAib3ZlcmxhcF9jb21tIjogdHJ1ZSwgInJlZHVjZV9zY2F0dGVyIjogdHJ1ZSwgInJlZHVjZV9idWNrZXRfc2l6ZSI6IDUwMDAwMDAwMCwgImNvbnRpZ3VvdXNfZ3JhZGllbnRzIjogdHJ1ZX0sICJ3YWxsX2Nsb2NrX2JyZWFrZG93biI6IHRydWV9 --megatron_config {"train_batch_size": 432, "train_micro_batch_size_per_gpu": 54, "optimizer": {"type": "Adam", "params": {"lr": 0.0002, "betas": [0.9, 0.95], "eps": 1e-08}}, "fp16": {"fp16": true, "enabled": true, "loss_scale": 0, "loss_scale_window": 1000, "hysteresis": 2, "min_loss_scale": 1}, "zero_optimization": {"stage": 1, "allgather_partitions": true, "allgather_bucket_size": 500000000, "overlap_comm": true, "reduce_scatter": true, "reduce_bucket_size": 500000000, "contiguous_gradients": true}, "wall_clock_breakdown": true, "precision": "fp16", "num_layers": 24, "hidden_size": 2048, "num_attention_heads": 16, "seq_length": 2048, "max_position_embeddings": 2048, "pos_emb": "rotary", "no_weight_tying": true, "attention_config": ["flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash"], "sparsity_config": {}, "init_method": "small_init", "output_layer_init_method": "wang_init", "lr_decay_style": "cosine", "lr_decay_iters": 320000, "min_lr": 2e-05, "optimizer_type": "Adam", "zero_stage": 1, "zero_reduce_scatter": true, "zero_contiguous_gradients": true, "zero_reduce_bucket_size": 500000000, "zero_allgather_bucket_size": 500000000, "lr": 0.0002, "data_path": "data/enwik8/enwik8_text_document", "data_impl": "mmap", "save": "checkpoints", "config_files": {"1-3B.yml": "# GPT-2 pretraining setup\n{\n   # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages\n   # across the node boundaries )\n   \"pipe_parallel_size\": 1,\n   \"model_parallel_size\": 1,\n\n   # model settings\n   \"num_layers\": 24,\n   \"hidden_size\": 2048,\n   \"num_attention_heads\": 16,\n   \"seq_length\": 2048,\n   \"max_position_embeddings\": 2048,\n   \"norm\": \"layernorm\",\n   \"pos_emb\": \"rotary\",\n   \"no_weight_tying\": true,\n   \"gpt_j_residual\": false,\n   \"output_layer_parallelism\": \"column\",\n\n   # these should provide some speedup but takes a while to build, set to true if desired\n   \"scaled_upper_triang_masked_softmax_fusion\": false,\n   \"bias_gelu_fusion\": false,\n   \"rope_fusion\": false,\n   \"layernorm_fusion\": false,\n\n   # init methods\n   \"init_method\": \"small_init\",\n   \"output_layer_init_method\": \"wang_init\",\n\n   # optimizer settings\n   \"optimizer\": {\n     \"type\": \"Adam\",\n     \"params\": {\n       \"lr\": 0.0002,\n       \"betas\": [0.9, 0.95],\n       \"eps\":  1.0e-8,\n     }\n   },\n   \"min_lr\": 0.00002,\n\n   # for all zero_optimization options, see https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training\n   \"zero_optimization\": {\n    \"stage\": 1,\n    \"allgather_partitions\": True,\n    \"allgather_bucket_size\": 500000000,\n    \"overlap_comm\": True,\n    \"reduce_scatter\": True,\n    \"reduce_bucket_size\": 500000000,\n    \"contiguous_gradients\": True,\n  },\n\n   # batch / data settings\n   \"train_micro_batch_size_per_gpu\": 54,\n   \"data_impl\": \"mmap\",\n\n   # activation checkpointing\n   \"checkpoint_activations\": true,\n   \"checkpoint_num_layers\": 1,\n   \"partition_activations\": true,\n   \"synchronize_each_layer\": true,\n\n   # regularization\n   \"gradient_clipping\": 1.0,\n   \"weight_decay\": 0.1,\n   \"hidden_dropout\": 0,\n   \"attention_dropout\": 0,\n\n   # Flash Attention\n   \"attention_config\": [[[\"flash\"], 24]],\n\n   # precision settings\n   \"fp16\": {\n     \"fp16\": true,\n     \"enabled\": true,\n     \"loss_scale\": 0,\n     \"loss_scale_window\": 1000,\n     \"hysteresis\": 2,\n     \"min_loss_scale\": 1\n   },\n\n   # misc. training settings\n   \"train_iters\": 320000,\n   \"lr_decay_iters\": 320000,\n   \"distributed_backend\": \"nccl\",\n   \"lr_decay_style\": \"cosine\",\n   \"warmup\": 0.01,\n   \"checkpoint_factor\": 10000,\n   \"eval_interval\": 1000,\n   \"eval_iters\": 10,\n\n   # logging\n   \"log_interval\": 10,\n   \"steps_per_print\": 10,\n   \"keep_last_n_checkpoints\": 4,\n   \"wall_clock_breakdown\": true,\n}\n", "local_setup_wandb_modified.yml": "# Suggested data paths when using GPT-NeoX locally\n{\n  \"data_path\": \"data/enwik8/enwik8_text_document\",\n\n  # or for weighted datasets:\n  # \"train-data-paths\": [\"data/enwik8/enwik8_text_document\", \"data/enwik8/enwik8_text_document\"],\n  # \"test-data-paths\": [\"data/enwik8/enwik8_text_document\", \"data/enwik8/enwik8_text_document\"],\n  # \"valid-data-paths\": [\"data/enwik8/enwik8_text_document\", \"data/enwik8/enwik8_text_document\"],\n  # \"train-data-weights\": [1., 2.],\n  # \"test-data-weights\": [2., 1.],\n  # \"valid-data-weights\": [0.5, 0.4],\n\n  # If weight_by_num_documents is True, Builds dataset weights from a multinomial distribution over groups of data according to the number of documents in each group.\n  # WARNING: setting this to True will override any user provided weights\n  # \"weight_by_num_documents\": false,\n  # \"weighted_sampler_alpha\": 0.3,\n\n  \"vocab_file\": \"data/gpt2-vocab.json\",\n  \"merge_file\": \"data/gpt2-merges.txt\",\n\n  \"save\": \"checkpoints\",\n  \"load\": \"checkpoints\",\n  \"checkpoint_validation_with_forward_pass\": False,\n\n  \"tensorboard_dir\": \"tensorboard\",\n  \"log_dir\": \"logs\",\n  \"use_wandb\": True,\n  \"wandb_host\": \"https://api.wandb.ai\",\n  \"wandb_project\": \"neox\",\n  \"wandb_run_name\": \"1.3B-FA-BS-54-1x8xA100\",\n\n  \"peak_theoretical_tflops\": 312\n}\n"}, "load": "checkpoints", "checkpoint_factor": 10000, "batch_size": 54, "train_iters": 320000, "eval_iters": 10, "keep_last_n_checkpoints": 4, "vocab_file": "data/gpt2-vocab.json", "merge_file": "data/gpt2-merges.txt", "checkpoint_activations": true, "synchronize_each_layer": true, "partition_activations": true, "dynamic_loss_scale": true, "pipe_parallel_size": 1, "world_size": 1, "is_pipe_parallel": true, "use_wandb": true, "wandb_group": "mqf6ylfi_48rsp80h", "wandb_run_name": "1.3B-FA-BS-54-1x8xA100", "log_dir": "logs", "tensorboard_dir": "tensorboard", "peak_theoretical_tflops": 312, "log_interval": 10, "text_gen_type": "unconditional", "local_rank": 0, "rank": 0, "user_script": "train.py", "global_num_gpus": 8}
[2025-01-06 10:03:54,252] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 10:04:09,423] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2025-01-06 10:04:09,423] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2025-01-06 10:04:09,423] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2025-01-06 10:04:09,423] [INFO] [launch.py:163:main] dist_world_size=8
[2025-01-06 10:04:09,423] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2025-01-06 10:04:17,230] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 10:04:17,245] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 10:04:17,249] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 10:04:17,253] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 10:04:17,258] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 10:04:17,258] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 10:04:17,258] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 10:04:17,826] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mambaUnable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mambaUnable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba

Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba

Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3



For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transferFor s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transferFor s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer

For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer

For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
NeoXArgs.configure_distributed_args() using world size: 8 and model-parallel size: 1 
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
WARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later and do you have tensorboard installed?), no TensorBoard logs will be written.
[2025-01-06 10:04:33,821] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-06 10:04:33,823] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-06 10:04:33,827] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-06 10:04:33,832] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-06 10:04:33,835] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-06 10:04:33,838] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-06 10:04:33,841] [INFO] [comm.py:637:init_distributed] cdb=None
> initializing torch distributed ...
[2025-01-06 10:04:40,922] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-06 10:04:40,922] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
> initializing model parallel with size 1
MPU DP: [0, 1, 2, 3, 4, 5, 6, 7]
MPU PP: [0]
MPU PP: [1]
MPU PP: [2]
MPU PP: [3]
MPU PP: [4]
MPU PP: [5]
MPU PP: [6]
MPU PP: [7]
MPU MP: [0]
MPU MP: [1]
MPU MP: [2]
MPU MP: [3]
MPU MP: [4]
MPU MP: [5]
MPU MP: [6]
MPU MP: [7]
> setting random seeds to 1234 ...
[2025-01-06 10:04:45,752] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory '/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data'
> building train, validation, and test datasets ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > dataset split:
    train:
     document indices in [0, 1) total of 1 documents
    validation:
     document indices in [1, 1) total of 0 documents
    test:
     document indices in [1, 1) total of 0 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > elapsed time to build and save doc-idx mapping (seconds): 0.012705
    using:
     number of documents:       1
     number of epochs:          10214
     sequence length:           2048
     total number of samples:   138250120
 > elapsed time to build and save sample-idx mapping (seconds): 7.579011
 > elapsed time to build and save shuffle-idx mapping (seconds): 9.402608
 > loading doc-idx mapping from data/enwik8/enwik8_text_document_train_indexmap_138240000ns_2048sl_1234s_packedpi_ac_doc_idx.npy
 > loading sample-idx mapping from data/enwik8/enwik8_text_document_train_indexmap_138240000ns_2048sl_1234s_packedpi_ac_sample_idx.npy
 > loading shuffle-idx mapping from data/enwik8/enwik8_text_document_train_indexmap_138240000ns_2048sl_1234s_packedpi_ac_shuffle_idx.npy
    loaded indexed file in 0.084 seconds
    total number of samples: 138250121
    total number of epochs: 10214
building GPT2 model ...
SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=1, model=0): 1, ProcessCoord(pipe=0, data=2, model=0): 2, ProcessCoord(pipe=0, data=3, model=0): 3, ProcessCoord(pipe=0, data=4, model=0): 4, ProcessCoord(pipe=0, data=5, model=0): 5, ProcessCoord(pipe=0, data=6, model=0): 6, ProcessCoord(pipe=0, data=7, model=0): 7}
[2025-01-06 10:05:10,351] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer|mlp
stage=0 layers=29
     0: EmbeddingPipe
     1: _pre_transformer_block
     2: ParallelTransformerLayerPipe
     3: ParallelTransformerLayerPipe
     4: ParallelTransformerLayerPipe
     5: ParallelTransformerLayerPipe
     6: ParallelTransformerLayerPipe
     7: ParallelTransformerLayerPipe
     8: ParallelTransformerLayerPipe
     9: ParallelTransformerLayerPipe
    10: ParallelTransformerLayerPipe
    11: ParallelTransformerLayerPipe
    12: ParallelTransformerLayerPipe
    13: ParallelTransformerLayerPipe
    14: ParallelTransformerLayerPipe
    15: ParallelTransformerLayerPipe
    16: ParallelTransformerLayerPipe
    17: ParallelTransformerLayerPipe
    18: ParallelTransformerLayerPipe
    19: ParallelTransformerLayerPipe
    20: ParallelTransformerLayerPipe
    21: ParallelTransformerLayerPipe
    22: ParallelTransformerLayerPipe
    23: ParallelTransformerLayerPipe
    24: ParallelTransformerLayerPipe
    25: ParallelTransformerLayerPipe
    26: _post_transformer_block
    27: NormPipe
    28: ParallelLinearPipe
  loss: partial
WARNING: APEX not installed - defaulting to deepspeed's fused adam
WARNING: APEX not installed - defaulting to deepspeed's fused adam
WARNING: APEX not installed - defaulting to deepspeed's fused adam
WARNING: APEX not installed - defaulting to deepspeed's fused adam
WARNING: APEX not installed - defaulting to deepspeed's fused adam
Configuring Optimizer type: Adam with params: {'lr': 0.0002, 'betas': [0.9, 0.95], 'eps': 1e-08}
WARNING: APEX not installed - defaulting to deepspeed's fused adam
WARNING: APEX not installed - defaulting to deepspeed's fused adam
WARNING: APEX not installed - defaulting to deepspeed's fused adam
[1/2] /usr/bin/nvcc --generate-dependencies-with-compile --dependency-output multi_tensor_adam.cuda.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/ops/csrc/includes -I/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/ops/csrc/adam -isystem /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/include -isystem /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -isystem /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/include/TH -isystem /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/include/THC -isystem /usr/include/python3.11 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -std=c++17 -c /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o 
[2/2] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/lib64 -lcudart -o fused_adam.so
Time to load fused_adam op: 108.1589024066925 seconds
Time to load fused_adam op: 108.22860050201416 seconds
Time to load fused_adam op: 108.2287974357605 seconds
Time to load fused_adam op: 108.22911500930786 seconds
Time to load fused_adam op: 108.22937798500061 seconds
Time to load fused_adam op: 108.22982978820801 seconds
Time to load fused_adam op: 108.23021984100342 secondsTime to load fused_adam op: 108.2303900718689 seconds

> learning rate decay style: cosine
DeepSpeed is enabled.
[2025-01-06 10:07:30,167] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.4+02e2ebf, git-hash=02e2ebf, git-branch=HEAD
[2025-01-06 10:07:32,799] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-01-06 10:07:32,800] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-01-06 10:07:32,800] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-06 10:07:32,809] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-01-06 10:07:32,809] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-01-06 10:07:32,809] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer
[2025-01-06 10:07:32,809] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-01-06 10:07:32,809] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-01-06 10:07:32,810] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-01-06 10:07:32,810] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-01-06 10:07:42,704] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 10:07:42,920] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 10:07:42,968] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 10:07:43,131] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 10:07:43,135] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 10:07:43,135] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 10:07:43,244] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2025-01-06 10:07:43,244] [INFO] [utils.py:803:see_memory_usage] MA 3.3 GB         Max_MA 3.3 GB         CA 3.3 GB         Max_CA 3 GB 
[2025-01-06 10:07:43,244] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 20.87 GB, percent = 1.0%
[2025-01-06 10:07:43,278] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 10:07:43,377] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2025-01-06 10:07:43,378] [INFO] [utils.py:803:see_memory_usage] MA 4.61 GB         Max_MA 5.27 GB         CA 5.28 GB         Max_CA 5 GB 
[2025-01-06 10:07:43,378] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 21.06 GB, percent = 1.0%
[2025-01-06 10:07:43,378] [INFO] [stage_1_and_2.py:517:__init__] optimizer state initialized
[2025-01-06 10:07:43,461] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2025-01-06 10:07:43,461] [INFO] [utils.py:803:see_memory_usage] MA 4.61 GB         Max_MA 4.61 GB         CA 5.28 GB         Max_CA 5 GB 
[2025-01-06 10:07:43,461] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 21.07 GB, percent = 1.0%
[2025-01-06 10:07:43,462] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2025-01-06 10:07:43,462] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-01-06 10:07:43,462] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7f4c08e36250>
[2025-01-06 10:07:43,463] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 10:07:43,463] [INFO] [config.py:979:print] DeepSpeedEngine configuration:
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   amp_enabled .................. False
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   amp_params ................... False
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   bfloat16_enabled ............. False
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f4bb00c56d0>
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   communication_data_type ...... None
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   dataloader_drop_last ......... False
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   disable_allgather ............ False
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   dump_state ................... False
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-06 10:07:43,463] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   elasticity_enabled ........... False
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   fp16_auto_cast ............... False
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   fp16_enabled ................. True
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   global_rank .................. 0
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   grad_accum_dtype ............. None
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 1
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   gradient_clipping ............ 0.0
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 65536
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   load_universal_checkpoint .... False
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   loss_scale ................... 0
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   memory_breakdown ............. False
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   mics_shard_size .............. -1
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   optimizer_name ............... adam
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   optimizer_params ............. {'lr': 0.0002, 'betas': [0.9, 0.95], 'eps': 1e-08}
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   pld_enabled .................. False
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   pld_params ................... False
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   prescale_gradients ........... False
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   scheduler_name ............... None
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   scheduler_params ............. None
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   sparse_attention ............. None
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   steps_per_print .............. 10
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   train_batch_size ............. 432
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  54
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   use_node_local_storage ....... False
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   wall_clock_breakdown ......... True
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   weight_quantization_config ... None
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   world_size ................... 8
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  False
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   zero_enabled ................. True
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-06 10:07:43,464] [INFO] [config.py:983:print]   zero_optimization_stage ...... 1
[2025-01-06 10:07:43,464] [INFO] [config.py:969:print_user_config]   json = {
    "train_batch_size": 432, 
    "train_micro_batch_size_per_gpu": 54, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0002, 
            "betas": [0.9, 0.95], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "fp16": true, 
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 1, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "wall_clock_breakdown": true
}
[2025-01-06 10:07:43,464] [INFO] [engine.py:99:__init__] CONFIG: micro_batches=1 micro_batch_size=54
[2025-01-06 10:07:43,464] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 10:07:43,520] [INFO] [engine.py:158:__init__] RANK=0 STAGE=0 LAYERS=29 [0, 29) STAGE_PARAMS=1414647808 (1414.648M) TOTAL_PARAMS=1414647808 (1414.648M) UNIQUE_PARAMS=1414647808 (1414.648M)
 > number of parameters on model parallel rank 0: 1414647808
 > total params: 1,414,647,808
[2025-01-06 10:07:43,526] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2025-01-06 10:07:43,526] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2025-01-06 10:07:43,526] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2025-01-06 10:07:43,526] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2025-01-06 10:07:43,526] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2025-01-06 10:07:43,526] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2025-01-06 10:07:43,526] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2025-01-06 10:07:43,526] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Unable to load checkpoint.
Loading checkpoint and starting from iteration 0
setting training data start iteration to 0
done with setups ...
time (ms) | train/valid/test data loaders: 24235.82 | model and optimizer: 153180.41 | train/valid/test data iterators: 573.85
training ...
[2025-01-06 10:07:46,424] [INFO] [checkpointing.py:540:forward] Activation Checkpointing Information
[2025-01-06 10:07:46,425] [INFO] [checkpointing.py:541:forward] ----Partition Activations True, CPU CHECKPOINTING False
[2025-01-06 10:07:46,425] [INFO] [checkpointing.py:542:forward] ----contiguous Memory Checkpointing False with 24 total layers
[2025-01-06 10:07:46,425] [INFO] [checkpointing.py:544:forward] ----Synchronization True
[2025-01-06 10:07:46,425] [INFO] [checkpointing.py:545:forward] ----Profiling time in checkpointing False
[2025-01-06 10:08:01,098] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 156.39 | optimizer_gradients: 2.64 | optimizer_step: 4.38
[2025-01-06 10:08:27,212] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 151.12 | optimizer_gradients: 2.57 | optimizer_step: 4.34
[2025-01-06 10:08:35,824] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 151.51 | optimizer_gradients: 2.56 | optimizer_step: 4.34
[2025-01-06 10:08:44,525] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 157.60 | optimizer_gradients: 2.57 | optimizer_step: 4.36
[2025-01-06 10:08:53,245] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 149.13 | optimizer_gradients: 2.55 | optimizer_step: 4.37
[2025-01-06 10:09:01,956] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 152.49 | optimizer_gradients: 2.56 | optimizer_step: 4.33
[2025-01-06 10:09:10,650] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 147.89 | optimizer_gradients: 2.56 | optimizer_step: 4.35
[2025-01-06 10:09:19,345] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 151.38 | optimizer_gradients: 2.55 | optimizer_step: 4.34
[2025-01-06 10:09:28,070] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 155.39 | optimizer_gradients: 2.57 | optimizer_step: 4.34
[2025-01-06 10:09:36,784] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 148.11 | optimizer_gradients: 2.55 | optimizer_step: 4.35
[2025-01-06 10:09:36,784] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[6.25e-07, 6.25e-07], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 10:09:36,785] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 2408.39 | fwd_microstep: 22803.36 | bwd_microstep: 58574.57 | bwd_inner_microstep: 58573.95 | bwd_allreduce_microstep: 0.12 | step_microstep: 2175.05
[2025-01-06 10:09:36,786] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 22803.10 | bwd: 58574.34 | bwd_inner: 58573.82 | bwd_allreduce: 0.11 | step: 2175.13
steps: 10 loss: 10.1958 iter time (s): 9.484 samples/sec: 45.548
[2025-01-06 10:09:36,786] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 38.337 | iteration       10/  320000 | elapsed time per iteration (ms): 11268.5 | learning rate: 6.250E-07 | approx flops per GPU: 116.7TFLOPS | MFU: 28.06% | HFU: 37.41% | lm_loss: 1.076191E+01 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after 10 iterations memory (MB) | allocated: 15358.6494140625 | max allocated: 58430.33837890625 | reserved: 67302.0 | max reserved: 67302.0
time (ms)
[2025-01-06 10:09:45,514] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.12 | optimizer_gradients: 2.56 | optimizer_step: 4.33
[2025-01-06 10:09:54,229] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.16 | optimizer_gradients: 2.55 | optimizer_step: 4.37
[2025-01-06 10:10:02,945] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 157.93 | optimizer_gradients: 2.55 | optimizer_step: 4.36
[2025-01-06 10:10:11,677] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 150.51 | optimizer_gradients: 2.55 | optimizer_step: 4.33
[2025-01-06 10:10:20,443] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.90 | optimizer_gradients: 2.55 | optimizer_step: 4.32
[2025-01-06 10:10:29,188] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.75 | optimizer_gradients: 2.55 | optimizer_step: 4.36
[2025-01-06 10:10:37,906] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 154.49 | optimizer_gradients: 2.55 | optimizer_step: 4.34
[2025-01-06 10:10:46,651] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 153.53 | optimizer_gradients: 2.55 | optimizer_step: 4.33
[2025-01-06 10:10:55,389] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.60 | optimizer_gradients: 2.55 | optimizer_step: 4.36
[2025-01-06 10:11:04,130] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.44 | optimizer_gradients: 2.55 | optimizer_step: 4.36
[2025-01-06 10:11:04,130] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[1.25e-06, 1.25e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 10:11:04,131] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 799.16 | fwd_microstep: 20274.93 | bwd_microstep: 57850.08 | bwd_inner_microstep: 57849.52 | bwd_allreduce_microstep: 0.07 | step_microstep: 1717.32
[2025-01-06 10:11:04,132] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 20274.83 | bwd: 57849.92 | bwd_inner: 57849.44 | bwd_allreduce: 0.07 | step: 1717.41
steps: 20 loss: 9.1860 iter time (s): 8.730 samples/sec: 49.485
[2025-01-06 10:11:04,132] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 49.458 | iteration       20/  320000 | elapsed time per iteration (ms): 8734.6 | learning rate: 1.250E-06 | approx flops per GPU: 150.6TFLOPS | MFU: 36.20% | HFU: 48.26% | lm_loss: 9.562173E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 10:11:12,894] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.28 | optimizer_gradients: 2.56 | optimizer_step: 4.35
[2025-01-06 10:11:21,647] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 155.10 | optimizer_gradients: 2.55 | optimizer_step: 4.37
[2025-01-06 10:11:30,397] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 160.15 | optimizer_gradients: 2.57 | optimizer_step: 4.36
[2025-01-06 10:11:39,146] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.40 | optimizer_gradients: 2.57 | optimizer_step: 4.37
[2025-01-06 10:11:47,914] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.82 | optimizer_gradients: 2.55 | optimizer_step: 4.32
[2025-01-06 10:11:56,661] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.37 | optimizer_gradients: 2.57 | optimizer_step: 4.38
[2025-01-06 10:12:05,413] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 153.72 | optimizer_gradients: 2.55 | optimizer_step: 4.35
[2025-01-06 10:12:14,151] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 149.14 | optimizer_gradients: 2.54 | optimizer_step: 4.36
[2025-01-06 10:12:22,939] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 153.76 | optimizer_gradients: 2.57 | optimizer_step: 4.34
[2025-01-06 10:12:31,726] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 151.50 | optimizer_gradients: 2.55 | optimizer_step: 4.37
[2025-01-06 10:12:31,726] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[1.875e-06, 1.875e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 10:12:31,727] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 789.84 | fwd_microstep: 20241.87 | bwd_microstep: 57939.78 | bwd_inner_microstep: 57939.30 | bwd_allreduce_microstep: 0.07 | step_microstep: 1704.84
[2025-01-06 10:12:31,728] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 20241.75 | bwd: 57939.67 | bwd_inner: 57939.22 | bwd_allreduce: 0.07 | step: 1704.95
steps: 30 loss: 8.7450 iter time (s): 8.754 samples/sec: 49.350
[2025-01-06 10:12:31,728] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 49.317 | iteration       30/  320000 | elapsed time per iteration (ms): 8759.6 | learning rate: 1.875E-06 | approx flops per GPU: 150.1TFLOPS | MFU: 36.09% | HFU: 48.12% | lm_loss: 8.933450E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 10:12:40,475] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 155.52 | optimizer_gradients: 2.56 | optimizer_step: 4.35
[2025-01-06 10:12:49,233] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 153.67 | optimizer_gradients: 2.56 | optimizer_step: 4.36
[2025-01-06 10:12:58,016] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.03 | optimizer_gradients: 2.57 | optimizer_step: 4.32
[2025-01-06 10:13:06,804] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 157.71 | optimizer_gradients: 2.57 | optimizer_step: 4.36
[2025-01-06 10:13:15,558] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 148.05 | optimizer_gradients: 2.56 | optimizer_step: 4.37
[2025-01-06 10:13:24,341] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 152.71 | optimizer_gradients: 2.57 | optimizer_step: 4.35
[2025-01-06 10:13:33,140] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 149.64 | optimizer_gradients: 2.56 | optimizer_step: 4.34
[2025-01-06 10:13:41,929] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 149.50 | optimizer_gradients: 2.57 | optimizer_step: 4.37
[2025-01-06 10:13:50,689] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 149.21 | optimizer_gradients: 2.55 | optimizer_step: 4.33
[2025-01-06 10:13:59,475] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 154.72 | optimizer_gradients: 2.56 | optimizer_step: 4.35
[2025-01-06 10:13:59,475] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[2.5e-06, 2.5e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 10:13:59,476] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 592.25 | fwd_microstep: 20423.02 | bwd_microstep: 58155.07 | bwd_inner_microstep: 58154.54 | bwd_allreduce_microstep: 0.07 | step_microstep: 1676.49
[2025-01-06 10:13:59,477] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 20422.90 | bwd: 58154.94 | bwd_inner: 58154.48 | bwd_allreduce: 0.07 | step: 1676.62
steps: 40 loss: 8.4324 iter time (s): 8.766 samples/sec: 49.280
[2025-01-06 10:13:59,478] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 49.231 | iteration       40/  320000 | elapsed time per iteration (ms): 8774.9 | learning rate: 2.500E-06 | approx flops per GPU: 149.9TFLOPS | MFU: 36.03% | HFU: 48.04% | lm_loss: 8.589217E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 10:14:08,264] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 150.92 | optimizer_gradients: 2.58 | optimizer_step: 4.34
[2025-01-06 10:14:17,070] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.82 | optimizer_gradients: 2.56 | optimizer_step: 4.35
[2025-01-06 10:14:25,881] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 156.74 | optimizer_gradients: 2.56 | optimizer_step: 4.32
[2025-01-06 10:14:34,694] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.40 | optimizer_gradients: 2.57 | optimizer_step: 4.34
[2025-01-06 10:14:43,507] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 156.09 | optimizer_gradients: 2.57 | optimizer_step: 4.37
[2025-01-06 10:14:52,311] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 147.19 | optimizer_gradients: 2.56 | optimizer_step: 4.36
[2025-01-06 10:15:01,132] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 150.53 | optimizer_gradients: 2.55 | optimizer_step: 4.35
[2025-01-06 10:15:09,940] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 154.47 | optimizer_gradients: 2.55 | optimizer_step: 4.35
[2025-01-06 10:15:18,736] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 150.42 | optimizer_gradients: 2.55 | optimizer_step: 4.35
[2025-01-06 10:15:27,521] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 156.43 | optimizer_gradients: 2.55 | optimizer_step: 4.36
[2025-01-06 10:15:27,521] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[3.125e-06, 3.125e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 10:15:27,522] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 621.31 | fwd_microstep: 20526.87 | bwd_microstep: 58367.60 | bwd_inner_microstep: 58367.05 | bwd_allreduce_microstep: 0.07 | step_microstep: 1693.34
[2025-01-06 10:15:27,522] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 20526.76 | bwd: 58367.45 | bwd_inner: 58366.96 | bwd_allreduce: 0.07 | step: 1693.45
steps: 50 loss: 8.0977 iter time (s): 8.791 samples/sec: 49.141
[2025-01-06 10:15:27,523] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 49.066 | iteration       50/  320000 | elapsed time per iteration (ms): 8804.5 | learning rate: 3.125E-06 | approx flops per GPU: 149.4TFLOPS | MFU: 35.91% | HFU: 47.88% | lm_loss: 8.238145E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 10:15:36,322] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 156.40 | optimizer_gradients: 2.55 | optimizer_step: 4.35
[2025-01-06 10:15:45,118] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.53 | optimizer_gradients: 2.57 | optimizer_step: 4.38
[2025-01-06 10:15:53,954] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.62 | optimizer_gradients: 2.56 | optimizer_step: 4.36
[2025-01-06 10:16:02,783] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.83 | optimizer_gradients: 2.56 | optimizer_step: 4.36
[2025-01-06 10:16:11,564] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 147.65 | optimizer_gradients: 2.57 | optimizer_step: 4.35
[2025-01-06 10:16:20,367] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 157.53 | optimizer_gradients: 2.56 | optimizer_step: 4.37
[2025-01-06 10:16:29,159] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.57 | optimizer_gradients: 2.56 | optimizer_step: 4.36
[2025-01-06 10:16:38,011] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 156.18 | optimizer_gradients: 2.55 | optimizer_step: 4.36
[2025-01-06 10:16:46,853] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 149.32 | optimizer_gradients: 2.56 | optimizer_step: 4.36
[2025-01-06 10:16:55,688] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 157.71 | optimizer_gradients: 2.55 | optimizer_step: 4.36
[2025-01-06 10:16:55,689] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[3.75e-06, 3.75e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 10:16:55,690] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 657.96 | fwd_microstep: 20527.01 | bwd_microstep: 58357.83 | bwd_inner_microstep: 58357.35 | bwd_allreduce_microstep: 0.07 | step_microstep: 1706.86
[2025-01-06 10:16:55,690] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 20526.88 | bwd: 58357.73 | bwd_inner: 58357.28 | bwd_allreduce: 0.07 | step: 1706.98
steps: 60 loss: 7.7723 iter time (s): 8.808 samples/sec: 49.049
[2025-01-06 10:16:55,691] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 48.998 | iteration       60/  320000 | elapsed time per iteration (ms): 8816.8 | learning rate: 3.750E-06 | approx flops per GPU: 149.2TFLOPS | MFU: 35.86% | HFU: 47.81% | lm_loss: 7.860480E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 10:17:04,515] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.34 | optimizer_gradients: 2.55 | optimizer_step: 4.35
[2025-01-06 10:17:13,322] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 154.72 | optimizer_gradients: 2.57 | optimizer_step: 4.35
[2025-01-06 10:17:22,139] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 154.34 | optimizer_gradients: 2.56 | optimizer_step: 4.33
[2025-01-06 10:17:30,943] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 148.43 | optimizer_gradients: 2.57 | optimizer_step: 4.35
[2025-01-06 10:17:39,756] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.07 | optimizer_gradients: 2.55 | optimizer_step: 4.34
[2025-01-06 10:17:48,565] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 148.33 | optimizer_gradients: 2.55 | optimizer_step: 4.34
[2025-01-06 10:17:57,401] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 153.40 | optimizer_gradients: 2.55 | optimizer_step: 4.36
[2025-01-06 10:18:06,247] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 157.86 | optimizer_gradients: 2.55 | optimizer_step: 4.37
[2025-01-06 10:18:15,085] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 148.04 | optimizer_gradients: 2.56 | optimizer_step: 4.35
[2025-01-06 10:18:23,902] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 150.21 | optimizer_gradients: 2.55 | optimizer_step: 4.37
[2025-01-06 10:18:23,902] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[4.3750000000000005e-06, 4.3750000000000005e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 10:18:23,903] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 735.93 | fwd_microstep: 20526.94 | bwd_microstep: 58357.73 | bwd_inner_microstep: 58357.25 | bwd_allreduce_microstep: 0.07 | step_microstep: 1677.96
[2025-01-06 10:18:23,903] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 20526.83 | bwd: 58357.62 | bwd_inner: 58357.17 | bwd_allreduce: 0.07 | step: 1678.07
steps: 70 loss: 7.5155 iter time (s): 8.815 samples/sec: 49.008
[2025-01-06 10:18:23,904] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 48.972 | iteration       70/  320000 | elapsed time per iteration (ms): 8821.3 | learning rate: 4.375E-06 | approx flops per GPU: 149.1TFLOPS | MFU: 35.84% | HFU: 47.79% | lm_loss: 7.593434E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 10:18:32,738] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 157.93 | optimizer_gradients: 2.56 | optimizer_step: 4.36
[2025-01-06 10:18:41,548] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.16 | optimizer_gradients: 2.56 | optimizer_step: 4.34
[2025-01-06 10:18:50,382] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 156.15 | optimizer_gradients: 2.57 | optimizer_step: 4.38
[2025-01-06 10:18:59,185] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.69 | optimizer_gradients: 2.55 | optimizer_step: 4.34
[2025-01-06 10:19:08,013] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 155.34 | optimizer_gradients: 2.54 | optimizer_step: 4.35
[2025-01-06 10:19:16,840] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.58 | optimizer_gradients: 2.57 | optimizer_step: 4.37
[2025-01-06 10:19:25,688] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 151.33 | optimizer_gradients: 2.55 | optimizer_step: 4.35
[2025-01-06 10:19:34,511] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 151.69 | optimizer_gradients: 2.56 | optimizer_step: 4.35
[2025-01-06 10:19:43,343] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.36 | optimizer_gradients: 2.55 | optimizer_step: 4.35
[2025-01-06 10:19:52,188] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 153.49 | optimizer_gradients: 2.55 | optimizer_step: 4.35
[2025-01-06 10:19:52,188] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[5e-06, 5e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 10:19:52,189] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 519.93 | fwd_microstep: 20623.69 | bwd_microstep: 58349.53 | bwd_inner_microstep: 58349.01 | bwd_allreduce_microstep: 0.07 | step_microstep: 1711.84
[2025-01-06 10:19:52,190] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 20623.59 | bwd: 58349.40 | bwd_inner: 58348.93 | bwd_allreduce: 0.07 | step: 1711.93
steps: 80 loss: 7.3201 iter time (s): 8.821 samples/sec: 48.973
[2025-01-06 10:19:52,190] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 48.932 | iteration       80/  320000 | elapsed time per iteration (ms): 8828.7 | learning rate: 5.000E-06 | approx flops per GPU: 149.0TFLOPS | MFU: 35.81% | HFU: 47.75% | lm_loss: 7.380403E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 10:20:01,038] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 150.38 | optimizer_gradients: 2.59 | optimizer_step: 4.34
[2025-01-06 10:20:09,866] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 155.91 | optimizer_gradients: 2.54 | optimizer_step: 4.38
[2025-01-06 10:20:18,718] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 156.68 | optimizer_gradients: 2.56 | optimizer_step: 4.35
[2025-01-06 10:20:27,532] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 147.15 | optimizer_gradients: 2.57 | optimizer_step: 4.38
[2025-01-06 10:20:36,364] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 155.86 | optimizer_gradients: 2.55 | optimizer_step: 4.36
[2025-01-06 10:20:45,171] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 157.48 | optimizer_gradients: 2.56 | optimizer_step: 4.37
[2025-01-06 10:20:54,021] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 155.42 | optimizer_gradients: 2.57 | optimizer_step: 4.34
[2025-01-06 10:21:02,842] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 148.97 | optimizer_gradients: 2.54 | optimizer_step: 4.34
[2025-01-06 10:21:11,659] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 157.05 | optimizer_gradients: 2.56 | optimizer_step: 4.38
[2025-01-06 10:21:20,472] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.30 | optimizer_gradients: 2.56 | optimizer_step: 4.35
[2025-01-06 10:21:20,472] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[5.625e-06, 5.625e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 10:21:20,474] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 664.85 | fwd_microstep: 20585.46 | bwd_microstep: 58290.13 | bwd_inner_microstep: 58289.64 | bwd_allreduce_microstep: 0.07 | step_microstep: 1689.70
[2025-01-06 10:21:20,474] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 20585.36 | bwd: 58290.02 | bwd_inner: 58289.56 | bwd_allreduce: 0.07 | step: 1689.81
steps: 90 loss: 7.1552 iter time (s): 8.819 samples/sec: 48.985
[2025-01-06 10:21:20,475] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 48.932 | iteration       90/  320000 | elapsed time per iteration (ms): 8828.5 | learning rate: 5.625E-06 | approx flops per GPU: 149.0TFLOPS | MFU: 35.81% | HFU: 47.75% | lm_loss: 7.192962E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 10:21:29,293] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 149.49 | optimizer_gradients: 2.56 | optimizer_step: 4.34
[2025-01-06 10:21:38,099] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 152.97 | optimizer_gradients: 2.55 | optimizer_step: 4.36
[2025-01-06 10:21:46,936] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.79 | optimizer_gradients: 2.57 | optimizer_step: 4.35
[2025-01-06 10:21:55,750] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.31 | optimizer_gradients: 2.55 | optimizer_step: 4.34
[2025-01-06 10:22:04,555] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.28 | optimizer_gradients: 2.55 | optimizer_step: 4.34
[2025-01-06 10:22:13,370] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 152.17 | optimizer_gradients: 2.57 | optimizer_step: 4.35
[2025-01-06 10:22:22,162] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.86 | optimizer_gradients: 2.55 | optimizer_step: 4.34
[2025-01-06 10:22:30,977] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 147.22 | optimizer_gradients: 2.55 | optimizer_step: 4.37
[2025-01-06 10:22:39,778] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.32 | optimizer_gradients: 2.55 | optimizer_step: 4.35
[2025-01-06 10:22:48,579] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.39 | optimizer_gradients: 2.55 | optimizer_step: 4.37
[2025-01-06 10:22:48,579] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[6.25e-06, 6.25e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 10:22:48,580] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 717.38 | fwd_microstep: 20566.11 | bwd_microstep: 58301.26 | bwd_inner_microstep: 58300.74 | bwd_allreduce_microstep: 0.07 | step_microstep: 1702.83
[2025-01-06 10:22:48,581] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 20566.00 | bwd: 58301.13 | bwd_inner: 58300.68 | bwd_allreduce: 0.07 | step: 1702.93
steps: 100 loss: 6.9350 iter time (s): 8.803 samples/sec: 49.075
[2025-01-06 10:22:48,582] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 49.032 | iteration      100/  320000 | elapsed time per iteration (ms): 8810.6 | learning rate: 6.250E-06 | approx flops per GPU: 149.3TFLOPS | MFU: 35.88% | HFU: 47.84% | lm_loss: 7.082063E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 10:22:57,378] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 148.47 | optimizer_gradients: 2.55 | optimizer_step: 4.36
[2025-01-06 10:23:06,205] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.99 | optimizer_gradients: 2.57 | optimizer_step: 4.36
[2025-01-06 10:23:15,025] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 157.88 | optimizer_gradients: 2.55 | optimizer_step: 4.36
[2025-01-06 10:23:23,853] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 150.38 | optimizer_gradients: 2.55 | optimizer_step: 4.37
[2025-01-06 10:23:32,701] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.14 | optimizer_gradients: 2.55 | optimizer_step: 4.36
[2025-01-06 10:23:41,530] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 152.55 | optimizer_gradients: 2.55 | optimizer_step: 4.35
[2025-01-06 10:23:50,380] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.82 | optimizer_gradients: 2.54 | optimizer_step: 4.39
[2025-01-06 10:23:59,216] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 149.74 | optimizer_gradients: 2.57 | optimizer_step: 4.37
[2025-01-06 10:24:08,061] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 152.25 | optimizer_gradients: 2.55 | optimizer_step: 4.34
[2025-01-06 10:24:16,874] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 153.66 | optimizer_gradients: 2.55 | optimizer_step: 4.36
[2025-01-06 10:24:16,874] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[6.875000000000001e-06, 6.875000000000001e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 10:24:16,875] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 725.94 | fwd_microstep: 20606.40 | bwd_microstep: 58353.41 | bwd_inner_microstep: 58352.89 | bwd_allreduce_microstep: 0.07 | step_microstep: 1688.03
[2025-01-06 10:24:16,876] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 20606.28 | bwd: 58353.27 | bwd_inner: 58352.81 | bwd_allreduce: 0.07 | step: 1688.13
steps: 110 loss: 6.8089 iter time (s): 8.822 samples/sec: 48.969
[2025-01-06 10:24:16,877] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 48.927 | iteration      110/  320000 | elapsed time per iteration (ms): 8829.5 | learning rate: 6.875E-06 | approx flops per GPU: 149.0TFLOPS | MFU: 35.81% | HFU: 47.74% | lm_loss: 6.914684E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 10:24:25,702] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 150.82 | optimizer_gradients: 2.58 | optimizer_step: 4.36
[2025-01-06 10:24:34,536] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.98 | optimizer_gradients: 2.56 | optimizer_step: 4.35
[2025-01-06 10:24:43,337] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 152.98 | optimizer_gradients: 2.57 | optimizer_step: 4.38
[2025-01-06 10:24:52,171] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.57 | optimizer_gradients: 2.55 | optimizer_step: 4.36
[2025-01-06 10:25:01,020] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 154.19 | optimizer_gradients: 2.55 | optimizer_step: 4.34
[2025-01-06 10:25:09,868] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 157.20 | optimizer_gradients: 2.57 | optimizer_step: 4.36
[2025-01-06 10:25:18,731] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 147.71 | optimizer_gradients: 2.57 | optimizer_step: 4.38
[2025-01-06 10:25:27,570] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 156.41 | optimizer_gradients: 2.56 | optimizer_step: 4.34
[2025-01-06 10:25:36,416] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 154.40 | optimizer_gradients: 2.57 | optimizer_step: 4.36
[2025-01-06 10:25:45,251] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.91 | optimizer_gradients: 2.55 | optimizer_step: 4.36
[2025-01-06 10:25:45,252] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[7.5e-06, 7.5e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 10:25:45,252] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 520.25 | fwd_microstep: 20653.14 | bwd_microstep: 58436.32 | bwd_inner_microstep: 58435.81 | bwd_allreduce_microstep: 0.07 | step_microstep: 1697.84
[2025-01-06 10:25:45,253] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 20653.02 | bwd: 58436.20 | bwd_inner: 58435.74 | bwd_allreduce: 0.07 | step: 1697.97
steps: 120 loss: 6.7499 iter time (s): 8.831 samples/sec: 48.917
[2025-01-06 10:25:45,254] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 48.881 | iteration      120/  320000 | elapsed time per iteration (ms): 8837.7 | learning rate: 7.500E-06 | approx flops per GPU: 148.8TFLOPS | MFU: 35.77% | HFU: 47.70% | lm_loss: 6.768404E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 10:25:54,093] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 149.33 | optimizer_gradients: 2.57 | optimizer_step: 4.37
[2025-01-06 10:26:02,920] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 156.71 | optimizer_gradients: 2.56 | optimizer_step: 4.35
[2025-01-06 10:26:11,761] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 156.43 | optimizer_gradients: 2.55 | optimizer_step: 4.37
[2025-01-06 10:26:20,572] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 153.89 | optimizer_gradients: 2.55 | optimizer_step: 4.34
[2025-01-06 10:26:29,371] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.41 | optimizer_gradients: 2.57 | optimizer_step: 4.36
[2025-01-06 10:26:38,167] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 147.43 | optimizer_gradients: 2.56 | optimizer_step: 4.36
[2025-01-06 10:26:46,947] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 149.31 | optimizer_gradients: 2.54 | optimizer_step: 4.33
[2025-01-06 10:26:55,760] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 154.69 | optimizer_gradients: 2.57 | optimizer_step: 4.34
[2025-01-06 10:27:04,572] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 160.00 | optimizer_gradients: 2.55 | optimizer_step: 4.36
[2025-01-06 10:27:13,412] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.66 | optimizer_gradients: 2.57 | optimizer_step: 4.36
[2025-01-06 10:27:13,412] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[8.125000000000001e-06, 8.125000000000001e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 10:27:13,414] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 784.32 | fwd_microstep: 20571.20 | bwd_microstep: 58402.73 | bwd_inner_microstep: 58402.21 | bwd_allreduce_microstep: 0.07 | step_microstep: 1693.80
[2025-01-06 10:27:13,415] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 20571.08 | bwd: 58402.59 | bwd_inner: 58402.14 | bwd_allreduce: 0.07 | step: 1693.92
steps: 130 loss: 6.5595 iter time (s): 8.799 samples/sec: 49.095
[2025-01-06 10:27:13,415] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 49.001 | iteration      130/  320000 | elapsed time per iteration (ms): 8816.2 | learning rate: 8.125E-06 | approx flops per GPU: 149.2TFLOPS | MFU: 35.86% | HFU: 47.81% | lm_loss: 6.643758E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 10:27:22,258] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 148.10 | optimizer_gradients: 2.58 | optimizer_step: 4.36
[2025-01-06 10:27:31,093] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.92 | optimizer_gradients: 2.56 | optimizer_step: 4.35
[2025-01-06 10:27:39,936] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 156.72 | optimizer_gradients: 2.57 | optimizer_step: 4.35
[2025-01-06 10:27:48,760] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 157.13 | optimizer_gradients: 2.56 | optimizer_step: 4.34
[2025-01-06 10:27:57,599] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 157.85 | optimizer_gradients: 2.55 | optimizer_step: 4.35
[2025-01-06 10:28:06,433] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.49 | optimizer_gradients: 2.56 | optimizer_step: 4.38
[2025-01-06 10:28:15,265] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.31 | optimizer_gradients: 2.58 | optimizer_step: 4.36
[2025-01-06 10:28:24,089] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 147.05 | optimizer_gradients: 2.56 | optimizer_step: 4.36
[2025-01-06 10:28:32,885] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 147.39 | optimizer_gradients: 2.55 | optimizer_step: 4.34
[2025-01-06 10:28:41,695] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 155.74 | optimizer_gradients: 2.55 | optimizer_step: 4.36
[2025-01-06 10:28:41,695] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[8.750000000000001e-06, 8.750000000000001e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 10:28:41,696] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 693.76 | fwd_microstep: 20609.71 | bwd_microstep: 58566.51 | bwd_inner_microstep: 58566.01 | bwd_allreduce_microstep: 0.07 | step_microstep: 1692.12
[2025-01-06 10:28:41,697] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 20609.59 | bwd: 58566.40 | bwd_inner: 58565.95 | bwd_allreduce: 0.07 | step: 1692.23
steps: 140 loss: 6.4511 iter time (s): 8.817 samples/sec: 48.996
[2025-01-06 10:28:41,697] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 48.934 | iteration      140/  320000 | elapsed time per iteration (ms): 8828.2 | learning rate: 8.750E-06 | approx flops per GPU: 149.0TFLOPS | MFU: 35.81% | HFU: 47.75% | lm_loss: 6.497367E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 10:28:50,531] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 156.91 | optimizer_gradients: 2.57 | optimizer_step: 4.37
[2025-01-06 10:28:59,358] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 147.58 | optimizer_gradients: 2.56 | optimizer_step: 4.35
[2025-01-06 10:29:08,213] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 153.88 | optimizer_gradients: 2.66 | optimizer_step: 4.36
[2025-01-06 10:29:17,062] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 156.79 | optimizer_gradients: 2.57 | optimizer_step: 4.37
[2025-01-06 10:29:25,912] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 149.31 | optimizer_gradients: 2.57 | optimizer_step: 4.35
[2025-01-06 10:29:34,753] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 146.84 | optimizer_gradients: 2.58 | optimizer_step: 4.34
[2025-01-06 10:29:43,613] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.33 | optimizer_gradients: 2.55 | optimizer_step: 4.33
[2025-01-06 10:29:52,432] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 152.12 | optimizer_gradients: 2.56 | optimizer_step: 4.34
[2025-01-06 10:30:01,239] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 155.36 | optimizer_gradients: 2.56 | optimizer_step: 4.37
[2025-01-06 10:30:10,058] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 149.28 | optimizer_gradients: 2.54 | optimizer_step: 4.36
[2025-01-06 10:30:10,058] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[9.375000000000001e-06, 9.375000000000001e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 10:30:10,059] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 836.88 | fwd_microstep: 20578.20 | bwd_microstep: 58516.78 | bwd_inner_microstep: 58516.29 | bwd_allreduce_microstep: 0.07 | step_microstep: 1678.03
[2025-01-06 10:30:10,060] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 20578.08 | bwd: 58516.67 | bwd_inner: 58516.21 | bwd_allreduce: 0.07 | step: 1678.14
steps: 150 loss: 6.2601 iter time (s): 8.830 samples/sec: 48.925
[2025-01-06 10:30:10,060] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 48.889 | iteration      150/  320000 | elapsed time per iteration (ms): 8836.3 | learning rate: 9.375E-06 | approx flops per GPU: 148.8TFLOPS | MFU: 35.78% | HFU: 47.71% | lm_loss: 6.380692E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 10:30:18,860] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 156.18 | optimizer_gradients: 2.55 | optimizer_step: 4.35
[2025-01-06 10:30:27,663] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 154.38 | optimizer_gradients: 2.55 | optimizer_step: 4.35
[2025-01-06 10:30:36,489] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 157.72 | optimizer_gradients: 2.55 | optimizer_step: 4.36
[2025-01-06 10:30:45,303] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 159.44 | optimizer_gradients: 2.55 | optimizer_step: 4.36
[2025-01-06 10:30:54,122] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 149.14 | optimizer_gradients: 2.55 | optimizer_step: 4.34
[2025-01-06 10:31:02,928] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 149.73 | optimizer_gradients: 2.54 | optimizer_step: 4.35
[2025-01-06 10:31:11,724] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 156.05 | optimizer_gradients: 2.56 | optimizer_step: 4.35
[2025-01-06 10:31:20,566] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 158.77 | optimizer_gradients: 2.56 | optimizer_step: 4.36
[2025-01-06 10:31:29,365] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 148.49 | optimizer_gradients: 2.57 | optimizer_step: 4.36
[2025-01-06 10:31:38,179] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 155.54 | optimizer_gradients: 2.55 | optimizer_step: 4.36
[2025-01-06 10:31:38,179] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[1e-05, 1e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 10:31:38,180] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 618.89 | fwd_microstep: 20580.22 | bwd_microstep: 58449.46 | bwd_inner_microstep: 58448.96 | bwd_allreduce_microstep: 0.07 | step_microstep: 1691.18
[2025-01-06 10:31:38,180] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 20580.11 | bwd: 58449.34 | bwd_inner: 58448.88 | bwd_allreduce: 0.07 | step: 1691.29
steps: 160 loss: 6.1008 iter time (s): 8.794 samples/sec: 49.124
[2025-01-06 10:31:38,181] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 49.024 | iteration      160/  320000 | elapsed time per iteration (ms): 8812.1 | learning rate: 1.000E-05 | approx flops per GPU: 149.3TFLOPS | MFU: 35.88% | HFU: 47.84% | lm_loss: 6.202053E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 10:31:46,999] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 150.95 | optimizer_gradients: 2.58 | optimizer_step: 4.35
[2025-01-06 10:31:55,773] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 149.71 | optimizer_gradients: 2.56 | optimizer_step: 4.37
[2025-01-06 10:32:04,575] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 157.88 | optimizer_gradients: 2.55 | optimizer_step: 4.35
[2025-01-06 10:32:13,371] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 151.97 | optimizer_gradients: 2.56 | optimizer_step: 4.36
