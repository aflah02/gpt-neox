Creating hostfile...
Hostfile created.
[2025-01-06 16:14:24,630] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
NeoXArgs.from_ymls() ['./configs/1-3B.yml', './configs/local_setup_wandb_modified_with_slurm.yml']
-------------------- arguments --------------------
  attention_config ................ ['flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash', 'flash']updated
  batch_size ...................... 64..........................updated
  checkpoint_activations .......... True........................updated
  checkpoint_factor ............... 10000.......................updated
  config_files .................... {'1-3B.yml': '# GPT-2 pretraining setup\n{\n   # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages\n   # across the node boundaries )\n   "pipe_parallel_size": 1,\n   "model_parallel_size": 1,\n\n   # model settings\n   "num_layers": 24,\n   "hidden_size": 2048,\n   "num_attention_heads": 16,\n   "seq_length": 2048,\n   "max_position_embeddings": 2048,\n   "norm": "layernorm",\n   "pos_emb": "rotary",\n   "no_weight_tying": true,\n   "gpt_j_residual": false,\n   "output_layer_parallelism": "column",\n\n   # these should provide some speedup but takes a while to build, set to true if desired\n   "scaled_upper_triang_masked_softmax_fusion": false,\n   "bias_gelu_fusion": false,\n   "rope_fusion": false,\n   "layernorm_fusion": false,\n\n   # init methods\n   "init_method": "small_init",\n   "output_layer_init_method": "wang_init",\n\n   # optimizer settings\n   "optimizer": {\n     "type": "Adam",\n     "params": {\n       "lr": 0.0002,\n       "betas": [0.9, 0.95],\n       "eps":  1.0e-8,\n     }\n   },\n   "min_lr": 0.00002,\n\n   # for all zero_optimization options, see https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training\n   "zero_optimization": {\n    "stage": 1,\n    "allgather_partitions": True,\n    "allgather_bucket_size": 500000000,\n    "overlap_comm": True,\n    "reduce_scatter": True,\n    "reduce_bucket_size": 500000000,\n    "contiguous_gradients": True,\n  },\n\n   # batch / data settings\n   "train_micro_batch_size_per_gpu": 64,\n   "data_impl": "mmap",\n\n   # activation checkpointing\n   "checkpoint_activations": true,\n   "checkpoint_num_layers": 1,\n   "partition_activations": true,\n   "synchronize_each_layer": true,\n\n   # regularization\n   "gradient_clipping": 1.0,\n   "weight_decay": 0.1,\n   "hidden_dropout": 0,\n   "attention_dropout": 0,\n\n   # Flash Attention\n   "attention_config": [[["flash"], 24]],\n\n   # precision settings\n   "fp16": {\n     "fp16": true,\n     "enabled": true,\n     "loss_scale": 0,\n     "loss_scale_window": 1000,\n     "hysteresis": 2,\n     "min_loss_scale": 1\n   },\n\n   # misc. training settings\n   "train_iters": 320000,\n   "lr_decay_iters": 320000,\n   "distributed_backend": "nccl",\n   "lr_decay_style": "cosine",\n   "warmup": 0.01,\n   "checkpoint_factor": 10000,\n   "eval_interval": 1000,\n   "eval_iters": 10,\n\n   # logging\n   "log_interval": 10,\n   "steps_per_print": 10,\n   "keep_last_n_checkpoints": 4,\n   "wall_clock_breakdown": true,\n}\n', 'local_setup_wandb_modified_with_slurm.yml': '# Suggested data paths when using GPT-NeoX locally\n{\n  "data_path": "data/enwik8/enwik8_text_document",\n\n  # or for weighted datasets:\n  # "train-data-paths": ["data/enwik8/enwik8_text_document", "data/enwik8/enwik8_text_document"],\n  # "test-data-paths": ["data/enwik8/enwik8_text_document", "data/enwik8/enwik8_text_document"],\n  # "valid-data-paths": ["data/enwik8/enwik8_text_document", "data/enwik8/enwik8_text_document"],\n  # "train-data-weights": [1., 2.],\n  # "test-data-weights": [2., 1.],\n  # "valid-data-weights": [0.5, 0.4],\n\n  # If weight_by_num_documents is True, Builds dataset weights from a multinomial distribution over groups of data according to the number of documents in each group.\n  # WARNING: setting this to True will override any user provided weights\n  # "weight_by_num_documents": false,\n  # "weighted_sampler_alpha": 0.3,\n\n  "vocab_file": "data/gpt2-vocab.json",\n  "merge_file": "data/gpt2-merges.txt",\n\n  "save": "checkpoints",\n  "load": "checkpoints",\n  "checkpoint_validation_with_forward_pass": False,\n\n  "tensorboard_dir": "tensorboard",\n  "log_dir": "logs",\n  "use_wandb": True,\n  "wandb_host": "https://api.wandb.ai",\n  "wandb_project": "neox",\n  "wandb_run_name": "1.3B-FA-BS-64-2x8xA100",\n\n  "peak_theoretical_tflops": 312,\n\n  "launcher": "slurm",\n  "deepspeed_slurm": true,\n  "no_ssh_check": true,\n}\n'}updated
  data_impl ....................... mmap........................updated
  data_path ....................... data/enwik8/enwik8_text_documentupdated
  deepspeed_slurm ................. True........................updated
  dynamic_loss_scale .............. True........................updated
  eval_iters ...................... 10..........................updated
  fp16 ............................ {'fp16': True, 'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}updated
  global_num_gpus ................. 16..........................updated
  hidden_size ..................... 2048........................updated
  init_method ..................... small_init..................updated
  is_pipe_parallel ................ True........................updated
  keep_last_n_checkpoints ......... 4...........................updated
  launcher ........................ slurm.......................updated
  load ............................ checkpoints.................updated
  log_dir ......................... logs........................updated
  log_interval .................... 10..........................updated
  lr .............................. 0.0002......................updated
  lr_decay_iters .................. 320000......................updated
  lr_decay_style .................. cosine......................updated
  max_position_embeddings ......... 2048........................updated
  merge_file ...................... data/gpt2-merges.txt........updated
  min_lr .......................... 2e-05.......................updated
  no_ssh_check .................... True........................updated
  no_weight_tying ................. True........................updated
  num_attention_heads ............. 16..........................updated
  num_layers ...................... 24..........................updated
  optimizer ....................... {'type': 'Adam', 'params': {'lr': 0.0002, 'betas': [0.9, 0.95], 'eps': 1e-08}}updated
  optimizer_type .................. Adam........................updated
  output_layer_init_method ........ wang_init...................updated
  partition_activations ........... True........................updated
  peak_theoretical_tflops ......... 312.........................updated
  pipe_parallel_size .............. 1...........................updated
  pos_emb ......................... rotary......................updated
  precision ....................... fp16........................updated
  save ............................ checkpoints.................updated
  seq_length ...................... 2048........................updated
  sparsity_config ................. {}..........................updated
  synchronize_each_layer .......... True........................updated
  tensorboard_dir ................. tensorboard.................updated
  text_gen_type ................... unconditional...............updated
  train_batch_size ................ 1024........................updated
  train_iters ..................... 320000......................updated
  train_micro_batch_size_per_gpu .. 64..........................updated
  use_wandb ....................... True........................updated
  user_script ..................... train.py....................updated
  vocab_file ...................... data/gpt2-vocab.json........updated
  wall_clock_breakdown ............ True........................updated
  wandb_group ..................... dq60rt0i_o22fsjm5...........updated
  wandb_run_name .................. 1.3B-FA-BS-64-2x8xA100......updated
  zero_allgather_bucket_size ...... 500000000...................updated
  zero_contiguous_gradients ....... True........................updated
  zero_optimization ............... {'stage': 1, 'allgather_partitions': True, 'allgather_bucket_size': 500000000, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 500000000, 'contiguous_gradients': True}updated
  zero_reduce_bucket_size ......... 500000000...................updated
  zero_reduce_scatter ............. True........................updated
  zero_stage ...................... 1...........................updated
  account ......................... None........................default
  activation ...................... gelu........................default
  activation_checkpointing ........ None........................default
  adlr_autoresume ................. False.......................default
  adlr_autoresume_interval ........ 1000........................default
  allow_chopped ................... True........................default
  amp ............................. None........................default
  apply_query_key_layer_scaling ... False.......................default
  attention_dropout ............... 0...........................default
  attention_softmax_in_fp32 ....... False.......................default
  autotuning ...................... None........................default
  autotuning_run .................. None........................default
  base_shapes_file ................ None........................default
  bf16 ............................ None........................default
  bias_dropout_fusion ............. False.......................default
  bias_gelu_fusion ................ False.......................default
  char_level_ppl .................. False.......................default
  checkpoint ...................... None........................default
  checkpoint_in_cpu ............... False.......................default
  checkpoint_num_layers ........... 1...........................default
  checkpoint_scale ................ linear......................default
  checkpoint_validation_with_forward_pass  False................default
  clip_grad ....................... 1.0.........................default
  comet_experiment ................ None........................default
  comet_experiment_name ........... None........................default
  comet_others .................... None........................default
  comet_project ................... None........................default
  comet_tags ...................... None........................default
  comet_workspace ................. None........................default
  comment ......................... None........................default
  comms_logger .................... None........................default
  communication_data_type ......... None........................default
  compression_training ............ None........................default
  contiguous_checkpointing ........ False.......................default
  coord_check ..................... False.......................default
  create_moe_param_group .......... True........................default
  csv_monitor ..................... None........................default
  curriculum_learning ............. None........................default
  curriculum_seqlen ............... 0...........................default
  data_efficiency ................. None........................default
  data_types ...................... None........................default
  dataset_impl .................... gpt2........................default
  deepscale ....................... False.......................default
  deepscale_config ................ None........................default
  deepspeed ....................... True........................default
  deepspeed_activation_checkpointing  True......................default
  deepspeed_extra_args ............ None........................default
  deepspeed_mpi ................... False.......................default
  detect_nvlink_pairs ............. False.......................default
  dim_att ......................... None........................default
  distributed_backend ............. nccl........................default
  do_test ......................... None........................default
  do_train ........................ None........................default
  do_valid ........................ None........................default
  dpo_beta ........................ 0.1.........................default
  dpo_fp32 ........................ True........................default
  dpo_reference_free .............. False.......................default
  dump_state ...................... False.......................default
  elasticity ...................... None........................default
  enable_expert_tensor_parallelism  False.......................default
  eod_mask_loss ................... False.......................default
  eval_interval ................... 1000........................default
  eval_results_prefix ............. ............................default
  eval_tasks ...................... None........................default
  exclude ......................... None........................default
  exit_interval ................... None........................default
  expansion_factor ................ None........................default
  expert_interval ................. 2...........................default
  extra_save_iters ................ None........................default
  ffn_dim ......................... None........................default
  finetune ........................ False.......................default
  flops_profiler .................. None........................default
  force_multi ..................... False.......................default
  fp16_lm_cross_entropy ........... False.......................default
  fp32_allreduce .................. False.......................default
  fp32_reinforce .................. True........................default
  git_hash ........................ 0af74c0.....................default
  gmlp_attn_dim ................... 64..........................default
  gpt_j_residual .................. False.......................default
  gpt_j_tied ...................... False.......................default
  gradient_accumulation_steps ..... 1...........................default
  gradient_clipping ............... 1.0.........................default
  gradient_noise_scale_cpu_offload  False.......................default
  gradient_noise_scale_n_batches .. 5...........................default
  gradient_predivide_factor ....... 1.0.........................default
  head_size ....................... None........................default
  hidden_dropout .................. 0...........................default
  hostfile ........................ None........................default
  hysteresis ...................... 2...........................default
  include ......................... None........................default
  init_method_std ................. 0.02........................default
  intermediate_size ............... None........................default
  iteration ....................... None........................default
  kl_div_beta ..................... 0.1.........................default
  kl_impl ......................... mse.........................default
  kto_beta ........................ 0.1.........................default
  kto_desirable_weight ............ 1.0.........................default
  kto_fp32 ........................ True........................default
  kto_undesirable_weight .......... 1.0.........................default
  layernorm_epsilon ............... 1e-05.......................default
  layernorm_fusion ................ False.......................default
  lazy_mpu_init ................... False.......................default
  local_rank ...................... None........................default
  log_grad_norm ................... False.......................default
  log_grad_pct_zeros .............. False.......................default
  log_gradient_noise_scale ........ False.......................default
  log_optimizer_states ............ False.......................default
  log_param_norm .................. False.......................default
  loss_scale ...................... None........................default
  loss_scale_window ............... 1000.0......................default
  lr_decay_fraction ............... None........................default
  make_vocab_size_divisible_by .... 128.........................default
  mamba_causal_conv_fusion ........ False.......................default
  mamba_inner_func_fusion ......... False.......................default
  mamba_selective_fp32_params ..... True........................default
  mamba_selective_scan_fusion ..... False.......................default
  mamba_use_bias_in_conv .......... True........................default
  mamba_use_bias_in_linears ....... False.......................default
  master_addr ..................... None........................default
  master_port ..................... 29500.......................default
  maximum_tokens .................. 64..........................default
  memory_profiling ................ False.......................default
  memory_profiling_path ........... None........................default
  min_scale ....................... 1.0.........................default
  mlp_multiple_of ................. 1...........................default
  mmap_warmup ..................... False.......................default
  model_parallel_size ............. 1...........................default
  moe_eval_capacity_factor ........ 1.0.........................default
  moe_expert_parallel_size ........ 1...........................default
  moe_glu ......................... False.......................default
  moe_jitter_eps .................. None........................default
  moe_lbl_in_fp32 ................. False.......................default
  moe_loss_coeff .................. 0.1.........................default
  moe_min_capacity ................ 4...........................default
  moe_num_experts ................. 1...........................default
  moe_token_dropping .............. False.......................default
  moe_top_k ....................... 1...........................default
  moe_train_capacity_factor ....... 1.0.........................default
  moe_type ........................ megablocks..................default
  moe_use_residual ................ True........................default
  mup_attn_temp ................... 1.0.........................default
  mup_embedding_mult .............. 1.0.........................default
  mup_init_scale .................. 1.0.........................default
  mup_output_temp ................. 1.0.........................default
  mup_rp_embedding_mult ........... 1.0.........................default
  mup_width_scale ................. 2...........................default
  neg_test_data_paths ............. None........................default
  neg_test_label_data_paths ....... None........................default
  neg_train_data_paths ............ None........................default
  neg_train_label_data_paths ...... None........................default
  neg_valid_data_paths ............ None........................default
  neg_valid_label_data_paths ...... None........................default
  no_load_optim ................... False.......................default
  no_load_rng ..................... False.......................default
  no_save_optim ................... False.......................default
  no_save_rng ..................... False.......................default
  norm ............................ layernorm...................default
  num_gpus ........................ None........................default
  num_kv_heads .................... None........................default
  num_nodes ....................... -1..........................default
  num_samples ..................... 1...........................default
  num_unique_layers ............... None........................default
  num_workers ..................... 2...........................default
  online_dataserver_ips ........... localhost...................default
  online_dataserver_ports ......... 10000.......................default
  onnx_safe ....................... False.......................default
  opt_pos_emb_offset .............. 0...........................default
  output_layer_parallelism ........ column......................default
  override_lr_scheduler ........... False.......................default
  pack_impl ....................... packed......................default
  padded_vocab_size ............... None........................default
  param_sharing_style ............. grouped.....................default
  pipe_partition_method ........... type:transformer|mlp........default
  pos_test_data_paths ............. None........................default
  pos_test_label_data_paths ....... None........................default
  pos_train_data_paths ............ None........................default
  pos_train_label_data_paths ...... None........................default
  pos_valid_data_paths ............ None........................default
  pos_valid_label_data_paths ...... None........................default
  precompute_model_name ........... None........................default
  prescale_gradients .............. False.......................default
  profile ......................... False.......................default
  profile_backward ................ False.......................default
  profile_step_start .............. 10..........................default
  profile_step_stop ............... 12..........................default
  prompt_end ...................... 
...........................default
  rank ............................ None........................default
  recompute ....................... False.......................default
  reinforce_leave_one_out ......... False.......................default
  return_logits ................... False.......................default
  rms_norm_epsilon ................ 1e-08.......................default
  rmsnorm_fusion .................. False.......................default
  rope_fusion ..................... False.......................default
  rotary_emb_base ................. 10000.......................default
  rotary_pct ...................... 1.0.........................default
  rotary_save_freqs_buffer ........ False.......................default
  rpe_max_distance ................ 128.........................default
  rpe_num_buckets ................. 32..........................default
  s3_chunk_size ................... 104857600...................default
  s3_path ......................... None........................default
  sample_input_file ............... None........................default
  sample_output_file .............. samples.txt.................default
  save_base_shapes ................ False.......................default
  scaled_masked_softmax_fusion .... False.......................default
  scaled_upper_triang_masked_softmax_fusion  False..............default
  scalenorm_epsilon ............... 1e-08.......................default
  scheduler ....................... None........................default
  seed ............................ 1234........................default
  sequence_parallel ............... False.......................default
  serve_model_weights ............. False.......................default
  short_seq_prob .................. 0.1.........................default
  sliding_window_width ............ None........................default
  soft_prompt_tuning .............. None........................default
  sparse_attention ................ None........................default
  sparse_gradients ................ False.......................default
  split ........................... 969, 30, 1..................default
  steps_per_print ................. 10..........................default
  te_columnparallel ............... False.......................default
  te_fp8_amax_compute_algo ........ most_recent.................default
  te_fp8_amax_history_len ......... 1...........................default
  te_fp8_format ................... hybrid......................default
  te_fp8_margin ................... 0...........................default
  te_fp8_mha ...................... False.......................default
  te_fp8_wgrad .................... True........................default
  te_layernorm_mlp ................ False.......................default
  te_mha .......................... False.......................default
  te_rowparallel .................. False.......................default
  temperature ..................... 0.0.........................default
  tensorboard ..................... None........................default
  test_data_paths ................. None........................default
  test_data_weights ............... None........................default
  test_label_data_paths ........... None........................default
  test_reward_data_paths .......... None........................default
  tokenizer_type .................. GPT2BPETokenizer............default
  top_k ........................... 0...........................default
  top_p ........................... 0.0.........................default
  train_data_paths ................ None........................default
  train_data_weights .............. None........................default
  train_epochs .................... None........................default
  train_impl ...................... normal......................default
  train_label_data_paths .......... None........................default
  train_reward_data_paths ......... None........................default
  use_bias_in_attn_linear ......... True........................default
  use_bias_in_mlp ................. True........................default
  use_bias_in_norms ............... True........................default
  use_bnb_optimizer ............... False.......................default
  use_checkpoint_lr_scheduler ..... False.......................default
  use_comet ....................... None........................default
  use_cpu_initialization .......... False.......................default
  use_flashattn_swiglu ............ False.......................default
  use_mup ......................... False.......................default
  use_qk_layernorm ................ False.......................default
  use_shared_fs ................... True........................default
  use_tutel ....................... False.......................default
  valid_data_paths ................ None........................default
  valid_data_weights .............. None........................default
  valid_label_data_paths .......... None........................default
  valid_reward_data_paths ......... None........................default
  wandb ........................... None........................default
  wandb_host ...................... https://api.wandb.ai........default
  wandb_init_all_ranks ............ False.......................default
  wandb_project ................... neox........................default
  wandb_team ...................... None........................default
  warmup .......................... 0.01........................default
  weight_by_num_documents ......... False.......................default
  weight_decay .................... 0.1.........................default
  weight_server_port .............. 6000........................default
  weighted_sampler_alpha .......... 1.0.........................default
  world_size ...................... None........................default
  z_loss .......................... 0.0.........................default
---------------- end of arguments ----------------
NeoXArgs.configure_distributed_args() using world size: 16 and model-parallel size: 1 
[2025-01-06 16:14:46,378] [INFO] [runner.py:586:main] cmd = srun -n 16 --export=ALL,WANDB_API_KEY=49ce99ab7e60eea6874a3f95235f6f1908956126,PYTHONPATH=/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox /NS/venvs/work/afkhan/neoxolmo/bin/python -u train.py --deepspeed_config eyJ0cmFpbl9iYXRjaF9zaXplIjogMTAyNCwgInRyYWluX21pY3JvX2JhdGNoX3NpemVfcGVyX2dwdSI6IDY0LCAib3B0aW1pemVyIjogeyJ0eXBlIjogIkFkYW0iLCAicGFyYW1zIjogeyJsciI6IDAuMDAwMiwgImJldGFzIjogWzAuOSwgMC45NV0sICJlcHMiOiAxZS0wOH19LCAiZnAxNiI6IHsiZnAxNiI6IHRydWUsICJlbmFibGVkIjogdHJ1ZSwgImxvc3Nfc2NhbGUiOiAwLCAibG9zc19zY2FsZV93aW5kb3ciOiAxMDAwLCAiaHlzdGVyZXNpcyI6IDIsICJtaW5fbG9zc19zY2FsZSI6IDF9LCAiemVyb19vcHRpbWl6YXRpb24iOiB7InN0YWdlIjogMSwgImFsbGdhdGhlcl9wYXJ0aXRpb25zIjogdHJ1ZSwgImFsbGdhdGhlcl9idWNrZXRfc2l6ZSI6IDUwMDAwMDAwMCwgIm92ZXJsYXBfY29tbSI6IHRydWUsICJyZWR1Y2Vfc2NhdHRlciI6IHRydWUsICJyZWR1Y2VfYnVja2V0X3NpemUiOiA1MDAwMDAwMDAsICJjb250aWd1b3VzX2dyYWRpZW50cyI6IHRydWV9LCAid2FsbF9jbG9ja19icmVha2Rvd24iOiB0cnVlfQ== --megatron_config {"launcher": "slurm", "no_ssh_check": true, "train_batch_size": 1024, "train_micro_batch_size_per_gpu": 64, "optimizer": {"type": "Adam", "params": {"lr": 0.0002, "betas": [0.9, 0.95], "eps": 1e-08}}, "fp16": {"fp16": true, "enabled": true, "loss_scale": 0, "loss_scale_window": 1000, "hysteresis": 2, "min_loss_scale": 1}, "zero_optimization": {"stage": 1, "allgather_partitions": true, "allgather_bucket_size": 500000000, "overlap_comm": true, "reduce_scatter": true, "reduce_bucket_size": 500000000, "contiguous_gradients": true}, "wall_clock_breakdown": true, "precision": "fp16", "num_layers": 24, "hidden_size": 2048, "num_attention_heads": 16, "seq_length": 2048, "max_position_embeddings": 2048, "pos_emb": "rotary", "no_weight_tying": true, "attention_config": ["flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash", "flash"], "sparsity_config": {}, "init_method": "small_init", "output_layer_init_method": "wang_init", "lr_decay_style": "cosine", "lr_decay_iters": 320000, "min_lr": 2e-05, "optimizer_type": "Adam", "zero_stage": 1, "zero_reduce_scatter": true, "zero_contiguous_gradients": true, "zero_reduce_bucket_size": 500000000, "zero_allgather_bucket_size": 500000000, "lr": 0.0002, "data_path": "data/enwik8/enwik8_text_document", "data_impl": "mmap", "save": "checkpoints", "config_files": {"1-3B.yml": "# GPT-2 pretraining setup\n{\n   # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages\n   # across the node boundaries )\n   \"pipe_parallel_size\": 1,\n   \"model_parallel_size\": 1,\n\n   # model settings\n   \"num_layers\": 24,\n   \"hidden_size\": 2048,\n   \"num_attention_heads\": 16,\n   \"seq_length\": 2048,\n   \"max_position_embeddings\": 2048,\n   \"norm\": \"layernorm\",\n   \"pos_emb\": \"rotary\",\n   \"no_weight_tying\": true,\n   \"gpt_j_residual\": false,\n   \"output_layer_parallelism\": \"column\",\n\n   # these should provide some speedup but takes a while to build, set to true if desired\n   \"scaled_upper_triang_masked_softmax_fusion\": false,\n   \"bias_gelu_fusion\": false,\n   \"rope_fusion\": false,\n   \"layernorm_fusion\": false,\n\n   # init methods\n   \"init_method\": \"small_init\",\n   \"output_layer_init_method\": \"wang_init\",\n\n   # optimizer settings\n   \"optimizer\": {\n     \"type\": \"Adam\",\n     \"params\": {\n       \"lr\": 0.0002,\n       \"betas\": [0.9, 0.95],\n       \"eps\":  1.0e-8,\n     }\n   },\n   \"min_lr\": 0.00002,\n\n   # for all zero_optimization options, see https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training\n   \"zero_optimization\": {\n    \"stage\": 1,\n    \"allgather_partitions\": True,\n    \"allgather_bucket_size\": 500000000,\n    \"overlap_comm\": True,\n    \"reduce_scatter\": True,\n    \"reduce_bucket_size\": 500000000,\n    \"contiguous_gradients\": True,\n  },\n\n   # batch / data settings\n   \"train_micro_batch_size_per_gpu\": 64,\n   \"data_impl\": \"mmap\",\n\n   # activation checkpointing\n   \"checkpoint_activations\": true,\n   \"checkpoint_num_layers\": 1,\n   \"partition_activations\": true,\n   \"synchronize_each_layer\": true,\n\n   # regularization\n   \"gradient_clipping\": 1.0,\n   \"weight_decay\": 0.1,\n   \"hidden_dropout\": 0,\n   \"attention_dropout\": 0,\n\n   # Flash Attention\n   \"attention_config\": [[[\"flash\"], 24]],\n\n   # precision settings\n   \"fp16\": {\n     \"fp16\": true,\n     \"enabled\": true,\n     \"loss_scale\": 0,\n     \"loss_scale_window\": 1000,\n     \"hysteresis\": 2,\n     \"min_loss_scale\": 1\n   },\n\n   # misc. training settings\n   \"train_iters\": 320000,\n   \"lr_decay_iters\": 320000,\n   \"distributed_backend\": \"nccl\",\n   \"lr_decay_style\": \"cosine\",\n   \"warmup\": 0.01,\n   \"checkpoint_factor\": 10000,\n   \"eval_interval\": 1000,\n   \"eval_iters\": 10,\n\n   # logging\n   \"log_interval\": 10,\n   \"steps_per_print\": 10,\n   \"keep_last_n_checkpoints\": 4,\n   \"wall_clock_breakdown\": true,\n}\n", "local_setup_wandb_modified_with_slurm.yml": "# Suggested data paths when using GPT-NeoX locally\n{\n  \"data_path\": \"data/enwik8/enwik8_text_document\",\n\n  # or for weighted datasets:\n  # \"train-data-paths\": [\"data/enwik8/enwik8_text_document\", \"data/enwik8/enwik8_text_document\"],\n  # \"test-data-paths\": [\"data/enwik8/enwik8_text_document\", \"data/enwik8/enwik8_text_document\"],\n  # \"valid-data-paths\": [\"data/enwik8/enwik8_text_document\", \"data/enwik8/enwik8_text_document\"],\n  # \"train-data-weights\": [1., 2.],\n  # \"test-data-weights\": [2., 1.],\n  # \"valid-data-weights\": [0.5, 0.4],\n\n  # If weight_by_num_documents is True, Builds dataset weights from a multinomial distribution over groups of data according to the number of documents in each group.\n  # WARNING: setting this to True will override any user provided weights\n  # \"weight_by_num_documents\": false,\n  # \"weighted_sampler_alpha\": 0.3,\n\n  \"vocab_file\": \"data/gpt2-vocab.json\",\n  \"merge_file\": \"data/gpt2-merges.txt\",\n\n  \"save\": \"checkpoints\",\n  \"load\": \"checkpoints\",\n  \"checkpoint_validation_with_forward_pass\": False,\n\n  \"tensorboard_dir\": \"tensorboard\",\n  \"log_dir\": \"logs\",\n  \"use_wandb\": True,\n  \"wandb_host\": \"https://api.wandb.ai\",\n  \"wandb_project\": \"neox\",\n  \"wandb_run_name\": \"1.3B-FA-BS-64-2x8xA100\",\n\n  \"peak_theoretical_tflops\": 312,\n\n  \"launcher\": \"slurm\",\n  \"deepspeed_slurm\": true,\n  \"no_ssh_check\": true,\n}\n"}, "load": "checkpoints", "checkpoint_factor": 10000, "batch_size": 64, "train_iters": 320000, "eval_iters": 10, "keep_last_n_checkpoints": 4, "vocab_file": "data/gpt2-vocab.json", "merge_file": "data/gpt2-merges.txt", "checkpoint_activations": true, "synchronize_each_layer": true, "partition_activations": true, "dynamic_loss_scale": true, "pipe_parallel_size": 1, "world_size": 16, "is_pipe_parallel": true, "use_wandb": true, "wandb_group": "dq60rt0i_o22fsjm5", "wandb_run_name": "1.3B-FA-BS-64-2x8xA100", "log_dir": "logs", "tensorboard_dir": "tensorboard", "peak_theoretical_tflops": 312, "log_interval": 10, "text_gen_type": "unconditional", "local_rank": 0, "rank": 0, "deepspeed_slurm": true, "user_script": "train.py", "global_num_gpus": 16}
[2025-01-06 16:14:53,904] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 16:14:53,918] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 16:14:53,921] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 16:14:53,922] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 16:14:53,927] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 16:14:53,927] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 16:14:53,927] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 16:14:53,927] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 16:14:56,475] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 16:14:56,477] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 16:14:56,481] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 16:14:56,482] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 16:14:56,482] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 16:14:56,485] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 16:14:56,485] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 16:14:56,485] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
NeoXArgs.configure_distributed_args() using world size: 16 and model-parallel size: 1 
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
WARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later and do you have tensorboard installed?), no TensorBoard logs will be written.
[2025-01-06 16:15:07,637] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-06 16:15:07,646] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-06 16:15:07,655] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-06 16:15:07,659] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-06 16:15:07,660] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-06 16:15:07,663] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-06 16:15:07,669] [INFO] [comm.py:637:init_distributed] cdb=None
> initializing torch distributed ...
[2025-01-06 16:15:09,966] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-06 16:15:09,966] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install boto3 either using requirements/requirements-s3.txt or https://github.com/boto/boto3
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
For s3 checkpointing, please install hf_transfer either using requirements/requirements-s3.txt or https://github.com/huggingface/hf_transfer
[2025-01-06 16:15:15,504] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-06 16:15:15,505] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-06 16:15:15,511] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-06 16:15:15,515] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-06 16:15:15,516] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-06 16:15:15,519] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-06 16:15:15,521] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-01-06 16:15:18,016] [INFO] [comm.py:637:init_distributed] cdb=None
> initializing model parallel with size 1
MPU DP: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
MPU PP: [0]
MPU PP: [1]
MPU PP: [2]
MPU PP: [3]
MPU PP: [4]
MPU PP: [5]
MPU PP: [6]
MPU PP: [7]
MPU PP: [8]
MPU PP: [9]
MPU PP: [10]
MPU PP: [11]
MPU PP: [12]
MPU PP: [13]
MPU PP: [14]
make: Entering directory '/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data'
MPU PP: [15]
MPU MP: [0]
MPU MP: [1]
MPU MP: [2]
MPU MP: [3]
MPU MP: [4]
MPU MP: [5]
MPU MP: [6]
MPU MP: [7]
MPU MP: [8]
MPU MP: [9]
MPU MP: [10]
MPU MP: [11]
MPU MP: [12]
MPU MP: [13]
MPU MP: [14]
MPU MP: [15]
> setting random seeds to 1234 ...
[2025-01-06 16:15:18,086] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory '/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data'
> building train, validation, and test datasets ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > dataset split:
    train:
     document indices in [0, 1) total of 1 documents
    validation:
     document indices in [1, 1) total of 0 documents
    test:
     document indices in [1, 1) total of 0 documents
 > loading doc-idx mapping from data/enwik8/enwik8_text_document_train_indexmap_327680000ns_2048sl_1234s_packedpi_ac_doc_idx.npy
 > loading sample-idx mapping from data/enwik8/enwik8_text_document_train_indexmap_327680000ns_2048sl_1234s_packedpi_ac_sample_idx.npy
 > loading shuffle-idx mapping from data/enwik8/enwik8_text_document_train_indexmap_327680000ns_2048sl_1234s_packedpi_ac_shuffle_idx.npy
    loaded indexed file in 0.133 seconds
    total number of samples: 327690956
    total number of epochs: 24210
building GPT2 model ...
SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=1, model=0): 1, ProcessCoord(pipe=0, data=2, model=0): 2, ProcessCoord(pipe=0, data=3, model=0): 3, ProcessCoord(pipe=0, data=4, model=0): 4, ProcessCoord(pipe=0, data=5, model=0): 5, ProcessCoord(pipe=0, data=6, model=0): 6, ProcessCoord(pipe=0, data=7, model=0): 7, ProcessCoord(pipe=0, data=8, model=0): 8, ProcessCoord(pipe=0, data=9, model=0): 9, ProcessCoord(pipe=0, data=10, model=0): 10, ProcessCoord(pipe=0, data=11, model=0): 11, ProcessCoord(pipe=0, data=12, model=0): 12, ProcessCoord(pipe=0, data=13, model=0): 13, ProcessCoord(pipe=0, data=14, model=0): 14, ProcessCoord(pipe=0, data=15, model=0): 15}
[2025-01-06 16:15:25,046] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer|mlp
stage=0 layers=29
     0: EmbeddingPipe
     1: _pre_transformer_block
     2: ParallelTransformerLayerPipe
     3: ParallelTransformerLayerPipe
     4: ParallelTransformerLayerPipe
     5: ParallelTransformerLayerPipe
     6: ParallelTransformerLayerPipe
     7: ParallelTransformerLayerPipe
     8: ParallelTransformerLayerPipe
     9: ParallelTransformerLayerPipe
    10: ParallelTransformerLayerPipe
    11: ParallelTransformerLayerPipe
    12: ParallelTransformerLayerPipe
    13: ParallelTransformerLayerPipe
    14: ParallelTransformerLayerPipe
    15: ParallelTransformerLayerPipe
    16: ParallelTransformerLayerPipe
    17: ParallelTransformerLayerPipe
    18: ParallelTransformerLayerPipe
    19: ParallelTransformerLayerPipe
    20: ParallelTransformerLayerPipe
    21: ParallelTransformerLayerPipe
    22: ParallelTransformerLayerPipe
    23: ParallelTransformerLayerPipe
    24: ParallelTransformerLayerPipe
    25: ParallelTransformerLayerPipe
    26: _post_transformer_block
    27: NormPipe
    28: ParallelLinearPipe
  loss: partial
WARNING: APEX not installed - defaulting to deepspeed's fused adam
WARNING: APEX not installed - defaulting to deepspeed's fused adam
WARNING: APEX not installed - defaulting to deepspeed's fused adam
WARNING: APEX not installed - defaulting to deepspeed's fused adam
WARNING: APEX not installed - defaulting to deepspeed's fused adam
Configuring Optimizer type: Adam with params: {'lr': 0.0002, 'betas': [0.9, 0.95], 'eps': 1e-08}
WARNING: APEX not installed - defaulting to deepspeed's fused adam
WARNING: APEX not installed - defaulting to deepspeed's fused adam
WARNING: APEX not installed - defaulting to deepspeed's fused adam
WARNING: APEX not installed - defaulting to deepspeed's fused adam
WARNING: APEX not installed - defaulting to deepspeed's fused adam
WARNING: APEX not installed - defaulting to deepspeed's fused adam
WARNING: APEX not installed - defaulting to deepspeed's fused adam
WARNING: APEX not installed - defaulting to deepspeed's fused adam
WARNING: APEX not installed - defaulting to deepspeed's fused adam
WARNING: APEX not installed - defaulting to deepspeed's fused adam
WARNING: APEX not installed - defaulting to deepspeed's fused adam
ninja: no work to do.
Time to load fused_adam op: 8.778857946395874 seconds
Time to load fused_adam op: 8.836209535598755 seconds
Time to load fused_adam op: 8.83775520324707 seconds
Time to load fused_adam op: 8.843861103057861 seconds
Time to load fused_adam op: 8.844677925109863 seconds
Time to load fused_adam op: 8.846441745758057 seconds
Time to load fused_adam op: 8.848244905471802 seconds
Time to load fused_adam op: 4.613028526306152 seconds
Time to load fused_adam op: 9.045379638671875 seconds
Time to load fused_adam op: 9.045542240142822 seconds
Time to load fused_adam op: 9.045596361160278 seconds
Time to load fused_adam op: 9.0524423122406 seconds
Time to load fused_adam op: 9.052150964736938 seconds
Time to load fused_adam op: 9.052082300186157 seconds
Time to load fused_adam op: 9.051023483276367 seconds
Time to load fused_adam op: 9.053569793701172 seconds
> learning rate decay style: cosine
DeepSpeed is enabled.
[2025-01-06 16:15:51,537] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.4+02e2ebf, git-hash=02e2ebf, git-branch=HEAD
[2025-01-06 16:15:54,232] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-01-06 16:15:54,233] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2025-01-06 16:15:54,234] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-01-06 16:15:54,251] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-01-06 16:15:54,251] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-01-06 16:15:54,252] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer
[2025-01-06 16:15:54,252] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2025-01-06 16:15:54,252] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2025-01-06 16:15:54,252] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-01-06 16:15:54,252] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-01-06 16:16:05,018] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 16:16:05,070] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 16:16:05,081] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 16:16:05,079] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 16:16:05,141] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 16:16:05,177] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 16:16:05,224] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 16:16:05,247] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 16:16:05,255] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 16:16:05,495] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 16:16:05,610] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 16:16:05,627] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 16:16:05,629] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 16:16:05,630] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2025-01-06 16:16:05,631] [INFO] [utils.py:803:see_memory_usage] MA 2.97 GB         Max_MA 2.97 GB         CA 2.99 GB         Max_CA 3 GB 
[2025-01-06 16:16:05,631] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 21.7 GB, percent = 1.1%
[2025-01-06 16:16:05,711] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 16:16:05,713] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 16:16:05,755] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2025-01-06 16:16:05,756] [INFO] [utils.py:803:see_memory_usage] MA 3.63 GB         Max_MA 3.96 GB         CA 3.98 GB         Max_CA 4 GB 
[2025-01-06 16:16:05,756] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 21.81 GB, percent = 1.1%
[2025-01-06 16:16:05,757] [INFO] [stage_1_and_2.py:517:__init__] optimizer state initialized
[2025-01-06 16:16:05,867] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2025-01-06 16:16:05,868] [INFO] [utils.py:803:see_memory_usage] MA 3.63 GB         Max_MA 3.63 GB         CA 3.98 GB         Max_CA 4 GB 
[2025-01-06 16:16:05,868] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 21.82 GB, percent = 1.1%
[2025-01-06 16:16:05,869] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2025-01-06 16:16:05,870] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2025-01-06 16:16:05,870] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7fe98e169850>
[2025-01-06 16:16:05,870] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 16:16:05,870] [INFO] [config.py:979:print] DeepSpeedEngine configuration:
[2025-01-06 16:16:05,871] [INFO] [config.py:983:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-01-06 16:16:05,871] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2025-01-06 16:16:05,871] [INFO] [config.py:983:print]   amp_enabled .................. False
[2025-01-06 16:16:05,871] [INFO] [config.py:983:print]   amp_params ................... False
[2025-01-06 16:16:05,871] [INFO] [config.py:983:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-01-06 16:16:05,871] [INFO] [config.py:983:print]   bfloat16_enabled ............. False
[2025-01-06 16:16:05,871] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False
[2025-01-06 16:16:05,871] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True
[2025-01-06 16:16:05,871] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False
[2025-01-06 16:16:05,871] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe7bc301c10>
[2025-01-06 16:16:05,871] [INFO] [config.py:983:print]   communication_data_type ...... None
[2025-01-06 16:16:05,871] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-01-06 16:16:05,872] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False
[2025-01-06 16:16:05,872] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False
[2025-01-06 16:16:05,872] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-01-06 16:16:05,872] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False
[2025-01-06 16:16:05,872] [INFO] [config.py:983:print]   dataloader_drop_last ......... False
[2025-01-06 16:16:05,872] [INFO] [config.py:983:print]   disable_allgather ............ False
[2025-01-06 16:16:05,872] [INFO] [config.py:983:print]   dump_state ................... False
[2025-01-06 16:16:05,872] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2025-01-06 16:16:05,872] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False
[2025-01-06 16:16:05,872] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1
[2025-01-06 16:16:05,872] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-01-06 16:16:05,872] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0
[2025-01-06 16:16:05,872] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100
[2025-01-06 16:16:05,872] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06
[2025-01-06 16:16:05,872] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01
[2025-01-06 16:16:05,872] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False
[2025-01-06 16:16:05,872] [INFO] [config.py:983:print]   elasticity_enabled ........... False
[2025-01-06 16:16:05,872] [INFO] [config.py:983:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-01-06 16:16:05,873] [INFO] [config.py:983:print]   fp16_auto_cast ............... False
[2025-01-06 16:16:05,873] [INFO] [config.py:983:print]   fp16_enabled ................. True
[2025-01-06 16:16:05,873] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False
[2025-01-06 16:16:05,873] [INFO] [config.py:983:print]   global_rank .................. 0
[2025-01-06 16:16:05,873] [INFO] [config.py:983:print]   grad_accum_dtype ............. None
[2025-01-06 16:16:05,873] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 1
[2025-01-06 16:16:05,873] [INFO] [config.py:983:print]   gradient_clipping ............ 0.0
[2025-01-06 16:16:05,873] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0
[2025-01-06 16:16:05,873] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-01-06 16:16:05,873] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 65536
[2025-01-06 16:16:05,873] [INFO] [config.py:983:print]   load_universal_checkpoint .... False
[2025-01-06 16:16:05,873] [INFO] [config.py:983:print]   loss_scale ................... 0
[2025-01-06 16:16:05,873] [INFO] [config.py:983:print]   memory_breakdown ............. False
[2025-01-06 16:16:05,873] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False
[2025-01-06 16:16:05,873] [INFO] [config.py:983:print]   mics_shard_size .............. -1
[2025-01-06 16:16:05,873] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2025-01-06 16:16:05,874] [INFO] [config.py:983:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-01-06 16:16:05,874] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False
[2025-01-06 16:16:05,874] [INFO] [config.py:983:print]   optimizer_name ............... adam
[2025-01-06 16:16:05,874] [INFO] [config.py:983:print]   optimizer_params ............. {'lr': 0.0002, 'betas': [0.9, 0.95], 'eps': 1e-08}
[2025-01-06 16:16:05,874] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-01-06 16:16:05,874] [INFO] [config.py:983:print]   pld_enabled .................. False
[2025-01-06 16:16:05,874] [INFO] [config.py:983:print]   pld_params ................... False
[2025-01-06 16:16:05,874] [INFO] [config.py:983:print]   prescale_gradients ........... False
[2025-01-06 16:16:05,874] [INFO] [config.py:983:print]   scheduler_name ............... None
[2025-01-06 16:16:05,874] [INFO] [config.py:983:print]   scheduler_params ............. None
[2025-01-06 16:16:05,874] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32
[2025-01-06 16:16:05,874] [INFO] [config.py:983:print]   sparse_attention ............. None
[2025-01-06 16:16:05,874] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False
[2025-01-06 16:16:05,874] [INFO] [config.py:983:print]   steps_per_print .............. 10
[2025-01-06 16:16:05,874] [INFO] [config.py:983:print]   train_batch_size ............. 1024
[2025-01-06 16:16:05,874] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  64
[2025-01-06 16:16:05,874] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False
[2025-01-06 16:16:05,874] [INFO] [config.py:983:print]   use_node_local_storage ....... False
[2025-01-06 16:16:05,874] [INFO] [config.py:983:print]   wall_clock_breakdown ......... True
[2025-01-06 16:16:05,875] [INFO] [config.py:983:print]   weight_quantization_config ... None
[2025-01-06 16:16:05,875] [INFO] [config.py:983:print]   world_size ................... 16
[2025-01-06 16:16:05,875] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  False
[2025-01-06 16:16:05,875] [INFO] [config.py:983:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2025-01-06 16:16:05,875] [INFO] [config.py:983:print]   zero_enabled ................. True
[2025-01-06 16:16:05,875] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True
[2025-01-06 16:16:05,875] [INFO] [config.py:983:print]   zero_optimization_stage ...... 1
[2025-01-06 16:16:05,875] [INFO] [config.py:969:print_user_config]   json = {
    "train_batch_size": 1.024000e+03, 
    "train_micro_batch_size_per_gpu": 64, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.0002, 
            "betas": [0.9, 0.95], 
            "eps": 1e-08
        }
    }, 
    "fp16": {
        "fp16": true, 
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 1, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true
    }, 
    "wall_clock_breakdown": true
}
[2025-01-06 16:16:05,875] [INFO] [engine.py:99:__init__] CONFIG: micro_batches=1 micro_batch_size=64
[2025-01-06 16:16:05,875] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
[2025-01-06 16:16:05,974] [INFO] [engine.py:158:__init__] RANK=0 STAGE=0 LAYERS=29 [0, 29) STAGE_PARAMS=1414647808 (1414.648M) TOTAL_PARAMS=1414647808 (1414.648M) UNIQUE_PARAMS=1414647808 (1414.648M)
 > number of parameters on model parallel rank 0: 1414647808
[2025-01-06 16:16:05,982] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2025-01-06 16:16:05,982] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2025-01-06 16:16:05,982] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2025-01-06 16:16:05,982] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2025-01-06 16:16:05,982] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2025-01-06 16:16:05,982] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2025-01-06 16:16:05,982] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
 > total params: 1,414,647,808
[2025-01-06 16:16:05,982] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Unable to load checkpoint.
Loading checkpoint and starting from iteration 0
setting training data start iteration to 0
[2025-01-06 16:16:05,981] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2025-01-06 16:16:05,981] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2025-01-06 16:16:05,981] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2025-01-06 16:16:05,981] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2025-01-06 16:16:05,981] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2025-01-06 16:16:05,981] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2025-01-06 16:16:05,981] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2025-01-06 16:16:05,982] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at checkpoints/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
done with setups ...
time (ms) | train/valid/test data loaders: 6646.65 | model and optimizer: 40949.37 | train/valid/test data iterators: 1271.22
training ...
[2025-01-06 16:16:09,076] [INFO] [checkpointing.py:540:forward] Activation Checkpointing Information
[2025-01-06 16:16:09,076] [INFO] [checkpointing.py:541:forward] ----Partition Activations True, CPU CHECKPOINTING False
[2025-01-06 16:16:09,076] [INFO] [checkpointing.py:542:forward] ----contiguous Memory Checkpointing False with 24 total layers
[2025-01-06 16:16:09,076] [INFO] [checkpointing.py:544:forward] ----Synchronization True
[2025-01-06 16:16:09,076] [INFO] [checkpointing.py:545:forward] ----Profiling time in checkpointing False
[2025-01-06 16:16:23,307] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1589.03 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:16:38,163] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1889.17 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:16:51,182] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1972.82 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:17:04,237] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1907.94 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:17:16,988] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1671.46 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:17:29,510] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1448.67 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:17:43,073] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1714.82 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:17:56,252] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1741.62 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:18:09,548] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1794.20 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 16:18:22,949] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1698.89 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:18:22,949] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[6.25e-07, 6.25e-07], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 16:18:22,951] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 2117.29 | fwd_microstep: 24131.35 | bwd_microstep: 67922.83 | bwd_inner_microstep: 67922.20 | bwd_allreduce_microstep: 0.09 | step_microstep: 17851.48
[2025-01-06 16:18:22,952] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 24131.23 | bwd: 67922.64 | bwd_inner: 67922.08 | bwd_allreduce: 0.08 | step: 17851.64
steps: 10 loss: 10.1925 iter time (s): 13.335 samples/sec: 76.792
[2025-01-06 16:18:22,962] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 75.457 | iteration       10/  320000 | elapsed time per iteration (ms): 13570.7 | learning rate: 6.250E-07 | approx flops per GPU: 114.9TFLOPS | MFU: 27.61% | HFU: 36.81% | lm_loss: 1.076132E+01 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
after 10 iterations memory (MB) | allocated: 16313.462890625 | max allocated: 67359.46435546875 | reserved: 75892.0 | max reserved: 75892.0
time (ms)
[2025-01-06 16:18:35,895] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1543.31 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:18:48,729] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1671.59 | optimizer_gradients: 1.02 | optimizer_step: 2.24
[2025-01-06 16:19:02,095] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1644.95 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:19:15,478] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1831.58 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:19:28,081] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1521.54 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:19:41,436] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1364.94 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:19:54,346] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1846.70 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:20:07,351] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1510.67 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:20:20,490] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1603.68 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 16:20:33,492] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1558.92 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:20:33,492] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[1.25e-06, 1.25e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 16:20:33,494] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 485.55 | fwd_microstep: 23631.38 | bwd_microstep: 68075.49 | bwd_inner_microstep: 68074.92 | bwd_allreduce_microstep: 0.07 | step_microstep: 16481.68
[2025-01-06 16:20:33,495] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23631.28 | bwd: 68075.33 | bwd_inner: 68074.80 | bwd_allreduce: 0.07 | step: 16481.90
steps: 20 loss: 9.1949 iter time (s): 13.039 samples/sec: 78.531
[2025-01-06 16:20:33,506] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.441 | iteration       20/  320000 | elapsed time per iteration (ms): 13054.4 | learning rate: 1.250E-06 | approx flops per GPU: 119.4TFLOPS | MFU: 28.70% | HFU: 38.27% | lm_loss: 9.559196E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 16:20:46,817] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1843.84 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:20:59,542] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1683.90 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 16:21:12,108] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1526.06 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:21:25,428] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1697.77 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:21:38,802] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1589.89 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 16:21:51,499] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1672.86 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:22:04,129] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1552.27 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:22:17,043] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1828.90 | optimizer_gradients: 1.01 | optimizer_step: 2.20
[2025-01-06 16:22:30,355] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1585.83 | optimizer_gradients: 1.12 | optimizer_step: 2.21
[2025-01-06 16:22:43,547] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1744.72 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:22:43,547] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[1.875e-06, 1.875e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 16:22:43,549] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 457.64 | fwd_microstep: 23639.67 | bwd_microstep: 68243.12 | bwd_inner_microstep: 68242.53 | bwd_allreduce_microstep: 0.07 | step_microstep: 17068.56
[2025-01-06 16:22:43,550] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23639.59 | bwd: 68242.95 | bwd_inner: 68242.42 | bwd_allreduce: 0.07 | step: 17068.73
steps: 30 loss: 8.7555 iter time (s): 12.973 samples/sec: 78.933
[2025-01-06 16:22:43,553] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.741 | iteration       30/  320000 | elapsed time per iteration (ms): 13004.6 | learning rate: 1.875E-06 | approx flops per GPU: 119.9TFLOPS | MFU: 28.81% | HFU: 38.42% | lm_loss: 8.932634E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 16:22:56,148] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1535.27 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:23:08,678] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1512.07 | optimizer_gradients: 1.03 | optimizer_step: 2.19
[2025-01-06 16:23:21,638] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1889.02 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:23:34,284] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1527.68 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:23:47,677] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1943.06 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 16:24:01,243] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1586.12 | optimizer_gradients: 1.03 | optimizer_step: 2.23
[2025-01-06 16:24:14,314] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1525.44 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:24:26,959] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1623.92 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 16:24:39,582] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1601.17 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:24:52,195] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1554.72 | optimizer_gradients: 1.05 | optimizer_step: 2.21
[2025-01-06 16:24:52,196] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[2.5e-06, 2.5e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 16:24:52,198] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 633.33 | fwd_microstep: 23677.65 | bwd_microstep: 68309.14 | bwd_inner_microstep: 68308.58 | bwd_allreduce_microstep: 0.07 | step_microstep: 16745.55
[2025-01-06 16:24:52,199] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23677.57 | bwd: 68308.99 | bwd_inner: 68308.49 | bwd_allreduce: 0.07 | step: 16745.73
steps: 40 loss: 8.4482 iter time (s): 12.851 samples/sec: 79.683
[2025-01-06 16:24:52,210] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 79.591 | iteration       40/  320000 | elapsed time per iteration (ms): 12865.7 | learning rate: 2.500E-06 | approx flops per GPU: 121.2TFLOPS | MFU: 29.12% | HFU: 38.83% | lm_loss: 8.583755E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 16:25:05,318] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1606.76 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:25:17,997] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1579.10 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:25:30,603] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1554.75 | optimizer_gradients: 1.12 | optimizer_step: 2.20
[2025-01-06 16:25:43,643] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1470.68 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:25:56,429] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1637.58 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:26:09,469] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1473.43 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:26:22,534] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1577.32 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:26:35,492] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1529.82 | optimizer_gradients: 1.01 | optimizer_step: 2.21
[2025-01-06 16:26:48,774] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1606.38 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:27:01,541] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1588.70 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:27:01,542] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[3.125e-06, 3.125e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 16:27:01,543] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 369.99 | fwd_microstep: 23816.11 | bwd_microstep: 68472.76 | bwd_inner_microstep: 68472.18 | bwd_allreduce_microstep: 0.07 | step_microstep: 16032.80
[2025-01-06 16:27:01,544] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23816.01 | bwd: 68472.60 | bwd_inner: 68472.08 | bwd_allreduce: 0.07 | step: 16032.97
steps: 50 loss: 8.0251 iter time (s): 12.914 samples/sec: 79.294
[2025-01-06 16:27:01,546] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 79.173 | iteration       50/  320000 | elapsed time per iteration (ms): 12933.6 | learning rate: 3.125E-06 | approx flops per GPU: 120.5TFLOPS | MFU: 28.97% | HFU: 38.63% | lm_loss: 8.225312E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 16:27:14,275] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1638.33 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 16:27:27,302] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1879.97 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:27:40,069] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1616.10 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:27:53,293] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1685.24 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:28:06,549] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1672.37 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:28:20,172] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2015.00 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:28:33,142] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1845.04 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:28:45,862] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1537.14 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:28:58,837] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1839.79 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:29:11,938] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1941.02 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:29:11,938] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[3.75e-06, 3.75e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 16:29:11,940] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 400.42 | fwd_microstep: 23850.05 | bwd_microstep: 68543.50 | bwd_inner_microstep: 68542.92 | bwd_allreduce_microstep: 0.07 | step_microstep: 18146.61
[2025-01-06 16:29:11,941] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23849.96 | bwd: 68543.34 | bwd_inner: 68542.82 | bwd_allreduce: 0.07 | step: 18146.79
steps: 60 loss: 7.6527 iter time (s): 13.029 samples/sec: 78.596
[2025-01-06 16:29:11,954] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.523 | iteration       60/  320000 | elapsed time per iteration (ms): 13040.8 | learning rate: 3.750E-06 | approx flops per GPU: 119.5TFLOPS | MFU: 28.73% | HFU: 38.31% | lm_loss: 7.844226E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 16:29:24,791] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1774.88 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:29:37,708] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1318.68 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:29:51,906] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1937.42 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 16:30:04,700] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1691.51 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:30:17,359] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1538.57 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:30:30,061] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1572.86 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 16:30:42,899] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1678.58 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:30:55,959] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1921.40 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:31:09,424] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1775.78 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:31:22,507] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1646.42 | optimizer_gradients: 1.03 | optimizer_step: 2.19
[2025-01-06 16:31:22,508] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[4.3750000000000005e-06, 4.3750000000000005e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 16:31:22,509] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 257.26 | fwd_microstep: 23901.81 | bwd_microstep: 68502.88 | bwd_inner_microstep: 68502.31 | bwd_allreduce_microstep: 0.07 | step_microstep: 17266.07
[2025-01-06 16:31:22,511] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23901.72 | bwd: 68502.73 | bwd_inner: 68502.20 | bwd_allreduce: 0.07 | step: 17266.25
steps: 70 loss: 7.4558 iter time (s): 13.032 samples/sec: 78.574
[2025-01-06 16:31:22,514] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.431 | iteration       70/  320000 | elapsed time per iteration (ms): 13056.0 | learning rate: 4.375E-06 | approx flops per GPU: 119.4TFLOPS | MFU: 28.70% | HFU: 38.27% | lm_loss: 7.579667E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 16:31:35,194] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1537.68 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 16:31:47,995] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1665.78 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 16:32:01,333] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1500.23 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:32:14,147] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1690.09 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:32:27,252] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1589.28 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:32:40,530] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1465.33 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:32:53,122] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1416.20 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:33:06,306] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1675.22 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:33:19,585] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1533.75 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:33:33,348] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1651.40 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:33:33,349] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[5e-06, 5e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 16:33:33,350] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 385.98 | fwd_microstep: 23834.16 | bwd_microstep: 68529.47 | bwd_inner_microstep: 68528.90 | bwd_allreduce_microstep: 0.07 | step_microstep: 16211.03
[2025-01-06 16:33:33,352] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23834.07 | bwd: 68529.33 | bwd_inner: 68528.80 | bwd_allreduce: 0.07 | step: 16211.21
steps: 80 loss: 7.2771 iter time (s): 13.056 samples/sec: 78.432
[2025-01-06 16:33:33,366] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.257 | iteration       80/  320000 | elapsed time per iteration (ms): 13085.2 | learning rate: 5.000E-06 | approx flops per GPU: 119.1TFLOPS | MFU: 28.64% | HFU: 38.18% | lm_loss: 7.359283E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 16:33:46,857] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1668.78 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 16:33:59,534] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1521.76 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 16:34:12,285] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1632.03 | optimizer_gradients: 1.07 | optimizer_step: 2.21
[2025-01-06 16:34:25,777] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1513.16 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:34:39,020] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1414.83 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 16:34:52,604] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1856.26 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:35:06,071] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1685.67 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 16:35:18,806] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1621.69 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:35:31,640] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1678.51 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:35:45,167] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1606.88 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:35:45,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[5.625e-06, 5.625e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 16:35:45,169] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 393.34 | fwd_microstep: 23903.06 | bwd_microstep: 68481.34 | bwd_inner_microstep: 68480.77 | bwd_allreduce_microstep: 0.07 | step_microstep: 16561.91
[2025-01-06 16:35:45,170] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23902.97 | bwd: 68481.19 | bwd_inner: 68480.66 | bwd_allreduce: 0.07 | step: 16562.09
steps: 90 loss: 7.1550 iter time (s): 13.157 samples/sec: 77.829
[2025-01-06 16:35:45,186] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.682 | iteration       90/  320000 | elapsed time per iteration (ms): 13182.0 | learning rate: 5.625E-06 | approx flops per GPU: 118.2TFLOPS | MFU: 28.43% | HFU: 37.90% | lm_loss: 7.192416E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 16:35:57,914] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1599.06 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:36:10,746] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1719.08 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 16:36:24,162] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1848.97 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 16:36:37,450] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1685.18 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:36:50,442] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1533.08 | optimizer_gradients: 1.03 | optimizer_step: 2.19
[2025-01-06 16:37:03,662] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1632.40 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:37:16,477] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1707.75 | optimizer_gradients: 1.12 | optimizer_step: 2.20
[2025-01-06 16:37:29,642] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1492.27 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:37:43,206] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1508.89 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:37:56,417] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1629.11 | optimizer_gradients: 1.03 | optimizer_step: 2.23
[2025-01-06 16:37:56,418] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[6.25e-06, 6.25e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 16:37:56,419] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 474.83 | fwd_microstep: 23886.97 | bwd_microstep: 68533.08 | bwd_inner_microstep: 68532.50 | bwd_allreduce_microstep: 0.07 | step_microstep: 16741.01
[2025-01-06 16:37:56,420] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23886.87 | bwd: 68532.92 | bwd_inner: 68532.40 | bwd_allreduce: 0.07 | step: 16741.19
steps: 100 loss: 7.0156 iter time (s): 13.096 samples/sec: 78.194
[2025-01-06 16:37:56,434] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.020 | iteration      100/  320000 | elapsed time per iteration (ms): 13124.8 | learning rate: 6.250E-06 | approx flops per GPU: 118.8TFLOPS | MFU: 28.55% | HFU: 38.07% | lm_loss: 7.062299E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 16:38:09,101] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1504.78 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:38:22,542] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1620.31 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:38:35,899] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1865.69 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:38:48,552] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1581.66 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:39:01,314] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1636.78 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 16:39:14,519] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1626.09 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:39:27,773] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1563.00 | optimizer_gradients: 1.07 | optimizer_step: 2.21
[2025-01-06 16:39:40,835] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1905.97 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:39:53,918] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1881.92 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:40:06,603] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1597.80 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:40:06,603] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[6.875000000000001e-06, 6.875000000000001e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 16:40:06,605] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 606.96 | fwd_microstep: 23920.53 | bwd_microstep: 68490.04 | bwd_inner_microstep: 68489.49 | bwd_allreduce_microstep: 0.07 | step_microstep: 17156.25
[2025-01-06 16:40:06,606] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23920.43 | bwd: 68489.89 | bwd_inner: 68489.40 | bwd_allreduce: 0.07 | step: 17156.44
steps: 110 loss: 6.8359 iter time (s): 12.994 samples/sec: 78.803
[2025-01-06 16:40:06,608] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.664 | iteration      110/  320000 | elapsed time per iteration (ms): 13017.4 | learning rate: 6.875E-06 | approx flops per GPU: 119.7TFLOPS | MFU: 28.78% | HFU: 38.38% | lm_loss: 6.909273E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 16:40:20,659] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1831.40 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:40:34,017] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1796.22 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:40:46,754] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1631.66 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:40:59,281] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1338.10 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:41:12,396] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1608.34 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:41:25,412] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1511.91 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:41:38,480] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1500.03 | optimizer_gradients: 1.03 | optimizer_step: 5.74
[2025-01-06 16:41:51,154] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1544.37 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:42:03,799] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1466.81 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:42:16,669] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1674.45 | optimizer_gradients: 1.03 | optimizer_step: 2.19
[2025-01-06 16:42:16,669] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[7.5e-06, 7.5e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 16:42:16,671] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 548.68 | fwd_microstep: 23862.60 | bwd_microstep: 68524.73 | bwd_inner_microstep: 68524.14 | bwd_allreduce_microstep: 0.07 | step_microstep: 16361.34
[2025-01-06 16:42:16,672] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23862.48 | bwd: 68524.58 | bwd_inner: 68524.04 | bwd_allreduce: 0.07 | step: 16361.52
steps: 120 loss: 6.6647 iter time (s): 12.976 samples/sec: 78.917
[2025-01-06 16:42:16,682] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.725 | iteration      120/  320000 | elapsed time per iteration (ms): 13007.4 | learning rate: 7.500E-06 | approx flops per GPU: 119.8TFLOPS | MFU: 28.81% | HFU: 38.41% | lm_loss: 6.751595E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 16:42:30,101] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2020.11 | optimizer_gradients: 1.12 | optimizer_step: 2.20
[2025-01-06 16:42:43,459] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1614.57 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 16:42:56,579] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1456.81 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:43:09,953] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1500.20 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:43:22,946] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1889.40 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:43:35,719] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1641.81 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:43:48,706] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1850.50 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:44:02,039] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1477.98 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:44:15,002] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1507.22 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:44:27,904] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1791.31 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:44:27,905] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[8.125000000000001e-06, 8.125000000000001e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 16:44:27,907] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 535.93 | fwd_microstep: 23882.64 | bwd_microstep: 68506.03 | bwd_inner_microstep: 68505.42 | bwd_allreduce_microstep: 0.07 | step_microstep: 17159.46
[2025-01-06 16:44:27,908] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23882.53 | bwd: 68505.87 | bwd_inner: 68505.32 | bwd_allreduce: 0.07 | step: 17159.66
steps: 130 loss: 6.5291 iter time (s): 13.104 samples/sec: 78.145
[2025-01-06 16:44:27,909] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.032 | iteration      130/  320000 | elapsed time per iteration (ms): 13122.8 | learning rate: 8.125E-06 | approx flops per GPU: 118.8TFLOPS | MFU: 28.55% | HFU: 38.07% | lm_loss: 6.636702E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 16:44:40,980] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1967.45 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:44:54,047] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1662.43 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:45:07,411] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1772.72 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:45:20,794] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1705.74 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:45:33,940] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1538.32 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:45:46,609] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1544.37 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 16:46:00,153] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1921.85 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:46:13,486] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1966.35 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:46:26,142] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1544.95 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:46:38,729] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1478.13 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:46:38,730] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[8.750000000000001e-06, 8.750000000000001e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 16:46:38,731] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 361.84 | fwd_microstep: 23897.73 | bwd_microstep: 68495.35 | bwd_inner_microstep: 68494.77 | bwd_allreduce_microstep: 0.07 | step_microstep: 17494.36
[2025-01-06 16:46:38,732] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23897.63 | bwd: 68495.19 | bwd_inner: 68494.67 | bwd_allreduce: 0.07 | step: 17494.55
steps: 140 loss: 6.4617 iter time (s): 13.052 samples/sec: 78.456
[2025-01-06 16:46:38,735] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.272 | iteration      140/  320000 | elapsed time per iteration (ms): 13082.6 | learning rate: 8.750E-06 | approx flops per GPU: 119.1TFLOPS | MFU: 28.64% | HFU: 38.19% | lm_loss: 6.477911E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 16:46:51,489] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1557.34 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:47:04,386] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1721.56 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:47:17,094] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1557.10 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:47:30,515] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1634.82 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:47:43,256] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1651.23 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:47:56,581] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1528.13 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:48:09,564] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1439.46 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 16:48:22,366] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1676.48 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:48:35,680] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1600.27 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:48:48,976] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1502.81 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:48:48,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[9.375000000000001e-06, 9.375000000000001e-06], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 16:48:48,978] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 517.36 | fwd_microstep: 23943.25 | bwd_microstep: 68509.90 | bwd_inner_microstep: 68509.35 | bwd_allreduce_microstep: 0.07 | step_microstep: 16259.49
[2025-01-06 16:48:48,980] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23943.16 | bwd: 68509.76 | bwd_inner: 68509.26 | bwd_allreduce: 0.07 | step: 16259.67
steps: 150 loss: 6.3315 iter time (s): 13.003 samples/sec: 78.750
[2025-01-06 16:48:48,982] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.620 | iteration      150/  320000 | elapsed time per iteration (ms): 13024.7 | learning rate: 9.375E-06 | approx flops per GPU: 119.7TFLOPS | MFU: 28.77% | HFU: 38.36% | lm_loss: 6.321779E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 16:49:02,500] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1808.45 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:49:15,819] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1785.50 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:49:28,976] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1666.94 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:49:42,226] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1690.95 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:49:54,966] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1573.73 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:50:07,696] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1572.86 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:50:21,175] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1428.25 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:50:34,638] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1777.61 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:50:47,200] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1427.98 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:51:00,306] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1585.94 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:51:00,307] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[1e-05, 1e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 16:51:00,309] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 500.72 | fwd_microstep: 23848.84 | bwd_microstep: 68481.90 | bwd_inner_microstep: 68481.33 | bwd_allreduce_microstep: 0.07 | step_microstep: 16729.00
[2025-01-06 16:51:00,310] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23848.73 | bwd: 68481.74 | bwd_inner: 68481.23 | bwd_allreduce: 0.07 | step: 16729.20
steps: 160 loss: 6.1258 iter time (s): 13.111 samples/sec: 78.104
[2025-01-06 16:51:00,312] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.972 | iteration      160/  320000 | elapsed time per iteration (ms): 13133.0 | learning rate: 1.000E-05 | approx flops per GPU: 118.7TFLOPS | MFU: 28.53% | HFU: 38.04% | lm_loss: 6.197202E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 16:51:13,139] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1668.53 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 16:51:26,358] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1756.82 | optimizer_gradients: 1.12 | optimizer_step: 2.20
[2025-01-06 16:51:39,406] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1466.31 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 16:51:52,495] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1567.67 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:52:05,526] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1617.22 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:52:18,829] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1836.35 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:52:32,319] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1763.46 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 16:52:45,393] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1552.73 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 16:52:58,761] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1752.04 | optimizer_gradients: 1.01 | optimizer_step: 2.19
[2025-01-06 16:53:11,686] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1409.60 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:53:11,687] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[1.0625e-05, 1.0625e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 16:53:11,688] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 431.82 | fwd_microstep: 23946.15 | bwd_microstep: 68580.80 | bwd_inner_microstep: 68580.23 | bwd_allreduce_microstep: 0.07 | step_microstep: 16846.82
[2025-01-06 16:53:11,690] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23946.05 | bwd: 68580.64 | bwd_inner: 68580.13 | bwd_allreduce: 0.07 | step: 16847.02
steps: 170 loss: 5.9568 iter time (s): 13.126 samples/sec: 78.013
[2025-01-06 16:53:11,701] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.937 | iteration      170/  320000 | elapsed time per iteration (ms): 13138.9 | learning rate: 1.063E-05 | approx flops per GPU: 118.6TFLOPS | MFU: 28.52% | HFU: 38.02% | lm_loss: 6.038593E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 16:53:25,369] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1806.32 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:53:38,723] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1843.06 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:53:52,382] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1606.24 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:54:05,085] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1583.89 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:54:17,839] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1602.30 | optimizer_gradients: 1.11 | optimizer_step: 2.20
[2025-01-06 16:54:30,830] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1805.15 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:54:43,998] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1500.85 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:54:56,787] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1604.38 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:55:10,037] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1418.94 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:55:22,699] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1552.63 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:55:22,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[1.125e-05, 1.125e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 16:55:22,701] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 559.34 | fwd_microstep: 23882.78 | bwd_microstep: 68534.43 | bwd_inner_microstep: 68533.87 | bwd_allreduce_microstep: 0.07 | step_microstep: 16753.71
[2025-01-06 16:55:22,702] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23882.68 | bwd: 68534.29 | bwd_inner: 68533.77 | bwd_allreduce: 0.07 | step: 16753.91
steps: 180 loss: 5.8123 iter time (s): 13.078 samples/sec: 78.302
[2025-01-06 16:55:22,703] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.167 | iteration      180/  320000 | elapsed time per iteration (ms): 13100.2 | learning rate: 1.125E-05 | approx flops per GPU: 119.0TFLOPS | MFU: 28.60% | HFU: 38.14% | lm_loss: 5.896875E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 16:55:35,520] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1647.09 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:55:48,719] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1698.14 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:56:02,080] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1933.62 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 16:56:15,058] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1813.35 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 16:56:29,102] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1990.15 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:56:42,121] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1452.02 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:56:55,229] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1692.49 | optimizer_gradients: 1.03 | optimizer_step: 2.19
[2025-01-06 16:57:07,980] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1636.96 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:57:22,014] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1978.30 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:57:34,754] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1648.26 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:57:34,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[1.1874999999999999e-05, 1.1874999999999999e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 16:57:34,756] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 408.88 | fwd_microstep: 23987.99 | bwd_microstep: 68580.96 | bwd_inner_microstep: 68580.40 | bwd_allreduce_microstep: 0.07 | step_microstep: 17897.35
[2025-01-06 16:57:34,758] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23987.89 | bwd: 68580.82 | bwd_inner: 68580.29 | bwd_allreduce: 0.07 | step: 17897.54
steps: 190 loss: 5.7111 iter time (s): 13.197 samples/sec: 77.592
[2025-01-06 16:57:34,759] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.543 | iteration      190/  320000 | elapsed time per iteration (ms): 13205.5 | learning rate: 1.187E-05 | approx flops per GPU: 118.0TFLOPS | MFU: 28.37% | HFU: 37.83% | lm_loss: 5.758886E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 16:57:47,890] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1460.97 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:58:00,869] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1529.22 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:58:13,615] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1558.54 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:58:26,680] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1331.20 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:58:39,276] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1494.98 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:58:52,494] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1517.39 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 16:59:05,389] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1725.12 | optimizer_gradients: 1.12 | optimizer_step: 2.22
[2025-01-06 16:59:18,255] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1701.87 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 16:59:31,267] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1590.62 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 16:59:44,238] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1816.91 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 16:59:44,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[1.25e-05, 1.25e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 16:59:44,240] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 418.50 | fwd_microstep: 23951.97 | bwd_microstep: 68608.38 | bwd_inner_microstep: 68607.80 | bwd_allreduce_microstep: 0.07 | step_microstep: 16198.99
[2025-01-06 16:59:44,241] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23951.88 | bwd: 68608.22 | bwd_inner: 68607.70 | bwd_allreduce: 0.07 | step: 16199.17
steps: 200 loss: 5.6682 iter time (s): 12.935 samples/sec: 79.162
[2025-01-06 16:59:44,242] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 79.083 | iteration      200/  320000 | elapsed time per iteration (ms): 12948.4 | learning rate: 1.250E-05 | approx flops per GPU: 120.4TFLOPS | MFU: 28.94% | HFU: 38.58% | lm_loss: 5.634277E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 16:59:57,115] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1668.50 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:00:09,914] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1595.77 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:00:23,861] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1543.86 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 17:00:37,779] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1787.08 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:00:51,173] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1581.32 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:01:04,620] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1760.56 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:01:17,429] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1671.71 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:01:30,009] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1428.13 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:01:43,408] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1487.07 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:01:56,194] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1684.68 | optimizer_gradients: 1.01 | optimizer_step: 2.20
[2025-01-06 17:01:56,194] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[1.3125e-05, 1.3125e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:01:56,196] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 501.62 | fwd_microstep: 23915.31 | bwd_microstep: 68603.37 | bwd_inner_microstep: 68602.78 | bwd_allreduce_microstep: 0.07 | step_microstep: 16602.21
[2025-01-06 17:01:56,197] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23915.20 | bwd: 68603.22 | bwd_inner: 68602.68 | bwd_allreduce: 0.07 | step: 16602.40
steps: 210 loss: 5.4387 iter time (s): 13.176 samples/sec: 77.719
[2025-01-06 17:01:56,201] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.600 | iteration      210/  320000 | elapsed time per iteration (ms): 13195.9 | learning rate: 1.313E-05 | approx flops per GPU: 118.1TFLOPS | MFU: 28.40% | HFU: 37.86% | lm_loss: 5.513506E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:02:08,903] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1548.58 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:02:21,618] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1578.26 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:02:34,989] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1849.26 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:02:48,477] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1623.87 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:03:01,183] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1578.24 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:03:14,134] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1518.61 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:03:27,786] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1446.36 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:03:41,474] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1559.63 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:03:54,472] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1560.59 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:04:07,218] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1593.37 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:04:07,219] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[1.3750000000000002e-05, 1.3750000000000002e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:04:07,220] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 460.08 | fwd_microstep: 23941.74 | bwd_microstep: 68600.72 | bwd_inner_microstep: 68600.14 | bwd_allreduce_microstep: 0.07 | step_microstep: 16295.97
[2025-01-06 17:04:07,221] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23941.64 | bwd: 68600.57 | bwd_inner: 68600.04 | bwd_allreduce: 0.07 | step: 16296.16
steps: 220 loss: 5.3569 iter time (s): 13.075 samples/sec: 78.315
[2025-01-06 17:04:07,242] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.118 | iteration      220/  320000 | elapsed time per iteration (ms): 13108.4 | learning rate: 1.375E-05 | approx flops per GPU: 118.9TFLOPS | MFU: 28.58% | HFU: 38.11% | lm_loss: 5.397393E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:04:20,729] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1639.17 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:04:34,101] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1641.17 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:04:47,898] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1836.76 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:05:01,319] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1889.76 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:05:14,490] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1679.03 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:05:27,870] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1859.40 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:05:40,657] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1675.07 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:05:53,439] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1593.02 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:06:06,891] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1493.03 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:06:20,141] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1569.51 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:06:20,142] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[1.4375e-05, 1.4375e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:06:20,143] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 487.51 | fwd_microstep: 23956.64 | bwd_microstep: 68593.37 | bwd_inner_microstep: 68592.77 | bwd_allreduce_microstep: 0.07 | step_microstep: 17257.70
[2025-01-06 17:06:20,144] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23956.53 | bwd: 68593.22 | bwd_inner: 68592.66 | bwd_allreduce: 0.07 | step: 17257.89
steps: 230 loss: 5.3065 iter time (s): 13.262 samples/sec: 77.215
[2025-01-06 17:06:20,145] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.074 | iteration      230/  320000 | elapsed time per iteration (ms): 13286.0 | learning rate: 1.438E-05 | approx flops per GPU: 117.3TFLOPS | MFU: 28.20% | HFU: 37.60% | lm_loss: 5.309148E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:06:33,327] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1428.41 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:06:46,154] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1638.34 | optimizer_gradients: 1.03 | optimizer_step: 2.23
[2025-01-06 17:06:59,319] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1556.48 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 17:07:12,587] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1442.22 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 17:07:26,091] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1583.40 | optimizer_gradients: 1.03 | optimizer_step: 2.19
[2025-01-06 17:07:39,002] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1748.92 | optimizer_gradients: 1.08 | optimizer_step: 2.20
[2025-01-06 17:07:51,803] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1651.43 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:08:04,426] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1444.66 | optimizer_gradients: 1.12 | optimizer_step: 2.21
[2025-01-06 17:08:17,385] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1420.29 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:08:31,009] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1878.52 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:08:31,009] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=0, lr=[1.5e-05, 1.5e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:08:31,011] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 540.70 | fwd_microstep: 23935.52 | bwd_microstep: 68567.95 | bwd_inner_microstep: 68567.36 | bwd_allreduce_microstep: 0.07 | step_microstep: 16230.31
[2025-01-06 17:08:31,012] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23935.41 | bwd: 68567.79 | bwd_inner: 68567.26 | bwd_allreduce: 0.07 | step: 16230.51
steps: 240 loss: 5.2260 iter time (s): 13.075 samples/sec: 78.316
[2025-01-06 17:08:31,019] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.244 | iteration      240/  320000 | elapsed time per iteration (ms): 13087.3 | learning rate: 1.500E-05 | approx flops per GPU: 119.1TFLOPS | MFU: 28.63% | HFU: 38.17% | lm_loss: 5.250388E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:08:44,658] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2073.23 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:08:57,969] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1651.42 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:09:11,196] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1826.96 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:09:25,055] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1834.81 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:09:38,316] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1619.36 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:09:51,794] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1851.69 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:10:05,360] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1649.60 | optimizer_gradients: 1.08 | optimizer_step: 2.20
[2025-01-06 17:10:18,960] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1599.29 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:10:31,755] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1658.99 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:10:44,558] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1683.59 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:10:44,558] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=0, lr=[1.5625e-05, 1.5625e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:10:44,560] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 444.52 | fwd_microstep: 23978.81 | bwd_microstep: 68634.24 | bwd_inner_microstep: 68633.58 | bwd_allreduce_microstep: 0.07 | step_microstep: 17803.65
[2025-01-06 17:10:44,561] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23978.70 | bwd: 68634.02 | bwd_inner: 68633.46 | bwd_allreduce: 0.07 | step: 17803.84
steps: 250 loss: 5.0942 iter time (s): 13.333 samples/sec: 76.799
[2025-01-06 17:10:44,574] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 76.672 | iteration      250/  320000 | elapsed time per iteration (ms): 13355.5 | learning rate: 1.563E-05 | approx flops per GPU: 116.7TFLOPS | MFU: 28.06% | HFU: 37.41% | lm_loss: 5.130378E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:10:57,710] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1481.35 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:11:10,599] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1738.82 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:11:24,096] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1617.96 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:11:36,752] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1475.48 | optimizer_gradients: 1.12 | optimizer_step: 2.21
[2025-01-06 17:11:49,577] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1699.82 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:12:02,836] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1671.77 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:12:15,534] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1595.29 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:12:28,743] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1622.98 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:12:41,413] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1562.62 | optimizer_gradients: 1.13 | optimizer_step: 2.20
[2025-01-06 17:12:54,092] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1487.68 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:12:54,092] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=0, lr=[1.6250000000000002e-05, 1.6250000000000002e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:12:54,094] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 493.83 | fwd_microstep: 23914.83 | bwd_microstep: 68608.42 | bwd_inner_microstep: 68607.79 | bwd_allreduce_microstep: 0.07 | step_microstep: 16323.94
[2025-01-06 17:12:54,095] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23914.73 | bwd: 68608.23 | bwd_inner: 68607.68 | bwd_allreduce: 0.07 | step: 16324.15
steps: 260 loss: 5.0582 iter time (s): 12.935 samples/sec: 79.164
[2025-01-06 17:12:54,106] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 79.054 | iteration      260/  320000 | elapsed time per iteration (ms): 12953.2 | learning rate: 1.625E-05 | approx flops per GPU: 120.3TFLOPS | MFU: 28.93% | HFU: 38.57% | lm_loss: 5.057395E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:13:08,134] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2073.55 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:13:21,134] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1886.74 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:13:34,386] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1819.18 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 17:13:47,100] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1549.25 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:14:00,192] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1494.65 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:14:13,472] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1683.15 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:14:26,288] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1666.79 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:14:39,512] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1640.99 | optimizer_gradients: 1.07 | optimizer_step: 2.21
[2025-01-06 17:14:52,545] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1558.26 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:15:06,296] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1926.55 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:15:06,296] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=0, lr=[1.6875e-05, 1.6875e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:15:06,298] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 500.92 | fwd_microstep: 23907.96 | bwd_microstep: 68581.69 | bwd_inner_microstep: 68581.13 | bwd_allreduce_microstep: 0.07 | step_microstep: 17698.47
[2025-01-06 17:15:06,299] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23907.77 | bwd: 68581.54 | bwd_inner: 68581.03 | bwd_allreduce: 0.07 | step: 17698.68
steps: 270 loss: 5.0229 iter time (s): 13.211 samples/sec: 77.513
[2025-01-06 17:15:06,300] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.462 | iteration      270/  320000 | elapsed time per iteration (ms): 13219.4 | learning rate: 1.688E-05 | approx flops per GPU: 117.9TFLOPS | MFU: 28.34% | HFU: 37.79% | lm_loss: 4.999638E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:15:19,238] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1797.67 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:15:32,063] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1644.88 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:15:44,945] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1706.94 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 17:15:58,708] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1836.43 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:16:11,551] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1656.34 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:16:24,340] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1635.80 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:16:38,058] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1864.63 | optimizer_gradients: 1.03 | optimizer_step: 2.23
[2025-01-06 17:16:51,393] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1816.71 | optimizer_gradients: 1.06 | optimizer_step: 2.20
[2025-01-06 17:17:04,674] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1432.80 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:17:18,122] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1787.43 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 17:17:18,123] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=0, lr=[1.7500000000000002e-05, 1.7500000000000002e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:17:18,124] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 415.61 | fwd_microstep: 23924.74 | bwd_microstep: 68523.77 | bwd_inner_microstep: 68523.17 | bwd_allreduce_microstep: 0.07 | step_microstep: 17604.89
[2025-01-06 17:17:18,125] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23924.63 | bwd: 68523.57 | bwd_inner: 68523.07 | bwd_allreduce: 0.07 | step: 17605.08
steps: 280 loss: 4.8354 iter time (s): 13.169 samples/sec: 77.758
[2025-01-06 17:17:18,127] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.677 | iteration      280/  320000 | elapsed time per iteration (ms): 13182.7 | learning rate: 1.750E-05 | approx flops per GPU: 118.2TFLOPS | MFU: 28.42% | HFU: 37.90% | lm_loss: 4.927608E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:17:31,146] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1930.47 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:17:44,127] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1920.13 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:17:57,696] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1906.93 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:18:10,846] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1671.55 | optimizer_gradients: 1.08 | optimizer_step: 2.21
[2025-01-06 17:18:24,803] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1849.35 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:18:38,342] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1888.79 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:18:50,941] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1500.61 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 17:19:04,102] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1668.26 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:19:16,840] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1544.76 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:19:29,991] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1502.70 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 17:19:29,992] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=0, lr=[1.8125e-05, 1.8125e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:19:29,994] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 482.93 | fwd_microstep: 23911.90 | bwd_microstep: 68548.64 | bwd_inner_microstep: 68548.06 | bwd_allreduce_microstep: 0.07 | step_microstep: 17795.86
[2025-01-06 17:19:29,995] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23911.80 | bwd: 68548.48 | bwd_inner: 68547.95 | bwd_allreduce: 0.07 | step: 17796.05
steps: 290 loss: 4.8681 iter time (s): 13.166 samples/sec: 77.778
[2025-01-06 17:19:29,998] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.652 | iteration      290/  320000 | elapsed time per iteration (ms): 13187.1 | learning rate: 1.813E-05 | approx flops per GPU: 118.2TFLOPS | MFU: 28.41% | HFU: 37.89% | lm_loss: 4.898545E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:19:43,042] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1860.35 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 17:19:56,038] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1851.47 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:20:09,134] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1975.43 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:20:22,638] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1926.33 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 17:20:35,589] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1827.20 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:20:48,383] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1662.44 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:21:01,200] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1658.82 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:21:13,887] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1536.78 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:21:27,356] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1617.50 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:21:40,257] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1800.35 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:21:40,258] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=0, lr=[1.8750000000000002e-05, 1.8750000000000002e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:21:40,259] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 463.61 | fwd_microstep: 23891.77 | bwd_microstep: 68540.50 | bwd_inner_microstep: 68539.90 | bwd_allreduce_microstep: 0.07 | step_microstep: 18148.59
[2025-01-06 17:21:40,260] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23891.67 | bwd: 68540.31 | bwd_inner: 68539.71 | bwd_allreduce: 0.07 | step: 18148.77
steps: 300 loss: 4.7680 iter time (s): 13.017 samples/sec: 78.664
[2025-01-06 17:21:40,261] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.610 | iteration      300/  320000 | elapsed time per iteration (ms): 13026.3 | learning rate: 1.875E-05 | approx flops per GPU: 119.7TFLOPS | MFU: 28.77% | HFU: 38.35% | lm_loss: 4.809310E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:21:53,851] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1986.69 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:22:07,381] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1833.95 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:22:20,062] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1565.68 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:22:32,706] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1509.25 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:22:45,602] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1775.67 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:22:58,545] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1786.68 | optimizer_gradients: 1.03 | optimizer_step: 2.19
[2025-01-06 17:23:12,404] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1869.31 | optimizer_gradients: 1.05 | optimizer_step: 2.21
[2025-01-06 17:23:25,859] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1988.84 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 17:23:38,917] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1990.85 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:23:52,247] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1850.92 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:23:52,248] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=0, lr=[1.9375e-05, 1.9375e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:23:52,249] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 419.31 | fwd_microstep: 23896.03 | bwd_microstep: 68602.17 | bwd_inner_microstep: 68601.59 | bwd_allreduce_microstep: 0.07 | step_microstep: 18617.91
[2025-01-06 17:23:52,250] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23895.92 | bwd: 68602.01 | bwd_inner: 68601.46 | bwd_allreduce: 0.07 | step: 18618.09
steps: 310 loss: 4.7580 iter time (s): 13.173 samples/sec: 77.736
[2025-01-06 17:23:52,267] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.572 | iteration      310/  320000 | elapsed time per iteration (ms): 13200.6 | learning rate: 1.937E-05 | approx flops per GPU: 118.1TFLOPS | MFU: 28.39% | HFU: 37.85% | lm_loss: 4.743563E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:24:05,967] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1835.85 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:24:19,042] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1547.50 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 17:24:32,148] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1930.78 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:24:45,127] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1812.13 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:24:58,467] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1994.19 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:25:11,115] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1469.45 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 17:25:24,407] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1730.29 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:25:36,999] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1466.55 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:25:49,887] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1718.48 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:26:02,497] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1440.52 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 17:26:02,498] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=0, lr=[2e-05, 2e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:26:02,500] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 466.90 | fwd_microstep: 23926.28 | bwd_microstep: 68397.57 | bwd_inner_microstep: 68396.95 | bwd_allreduce_microstep: 0.07 | step_microstep: 17329.42
[2025-01-06 17:26:02,501] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23926.18 | bwd: 68397.38 | bwd_inner: 68396.85 | bwd_allreduce: 0.07 | step: 17329.62
steps: 320 loss: 4.6679 iter time (s): 13.002 samples/sec: 78.755
[2025-01-06 17:26:02,515] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.619 | iteration      320/  320000 | elapsed time per iteration (ms): 13024.8 | learning rate: 2.000E-05 | approx flops per GPU: 119.7TFLOPS | MFU: 28.77% | HFU: 38.36% | lm_loss: 4.705736E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:26:15,614] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1964.43 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:26:28,447] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1684.86 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:26:41,584] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1631.08 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 17:26:54,375] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1642.89 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:27:07,030] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1536.18 | optimizer_gradients: 1.02 | optimizer_step: 2.23
[2025-01-06 17:27:20,555] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1530.94 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:27:33,257] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1598.44 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:27:46,043] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1644.21 | optimizer_gradients: 1.06 | optimizer_step: 2.21
[2025-01-06 17:27:59,472] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1526.37 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 17:28:12,615] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2004.80 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:28:12,615] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=0, lr=[2.0625e-05, 2.0625e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:28:12,617] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 565.59 | fwd_microstep: 23894.84 | bwd_microstep: 68492.83 | bwd_inner_microstep: 68492.27 | bwd_allreduce_microstep: 0.07 | step_microstep: 17173.74
[2025-01-06 17:28:12,618] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23894.73 | bwd: 68492.68 | bwd_inner: 68492.16 | bwd_allreduce: 0.07 | step: 17173.93
steps: 330 loss: 4.6928 iter time (s): 12.992 samples/sec: 78.815
[2025-01-06 17:28:12,620] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.705 | iteration      330/  320000 | elapsed time per iteration (ms): 13010.6 | learning rate: 2.062E-05 | approx flops per GPU: 119.8TFLOPS | MFU: 28.80% | HFU: 38.40% | lm_loss: 4.703973E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:28:25,684] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1980.42 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:28:38,895] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1737.68 | optimizer_gradients: 1.07 | optimizer_step: 2.21
[2025-01-06 17:28:51,653] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1596.22 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:29:04,979] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1850.25 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 17:29:19,056] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1822.32 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:29:32,176] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1578.84 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:29:44,975] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1650.20 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:29:57,723] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1614.09 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:30:10,378] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1537.40 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:30:23,914] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1935.42 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:30:23,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=0, lr=[2.125e-05, 2.125e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:30:23,916] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 398.09 | fwd_microstep: 23960.94 | bwd_microstep: 68541.07 | bwd_inner_microstep: 68540.48 | bwd_allreduce_microstep: 0.07 | step_microstep: 17702.49
[2025-01-06 17:30:23,918] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23960.84 | bwd: 68540.91 | bwd_inner: 68540.37 | bwd_allreduce: 0.07 | step: 17702.67
steps: 340 loss: 4.5284 iter time (s): 13.116 samples/sec: 78.072
[2025-01-06 17:30:23,934] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.981 | iteration      340/  320000 | elapsed time per iteration (ms): 13131.3 | learning rate: 2.125E-05 | approx flops per GPU: 118.7TFLOPS | MFU: 28.54% | HFU: 38.05% | lm_loss: 4.601479E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:30:37,006] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1944.43 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:30:49,771] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1569.65 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:31:02,488] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1547.86 | optimizer_gradients: 1.11 | optimizer_step: 2.20
[2025-01-06 17:31:15,342] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1686.94 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:31:28,327] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1857.37 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:31:41,197] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1723.38 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:31:54,688] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1702.67 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:32:07,394] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1559.98 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:32:20,175] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1608.55 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:32:32,898] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1576.93 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:32:32,899] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=0, lr=[2.1875000000000003e-05, 2.1875000000000003e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:32:32,900] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 487.10 | fwd_microstep: 23941.29 | bwd_microstep: 68609.26 | bwd_inner_microstep: 68608.67 | bwd_allreduce_microstep: 0.07 | step_microstep: 17193.70
[2025-01-06 17:32:32,901] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23941.19 | bwd: 68609.09 | bwd_inner: 68608.57 | bwd_allreduce: 0.07 | step: 17193.89
steps: 350 loss: 4.5546 iter time (s): 12.874 samples/sec: 79.539
[2025-01-06 17:32:32,910] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 79.395 | iteration      350/  320000 | elapsed time per iteration (ms): 12897.6 | learning rate: 2.188E-05 | approx flops per GPU: 120.9TFLOPS | MFU: 29.05% | HFU: 38.74% | lm_loss: 4.551810E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:32:45,604] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1503.15 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:32:59,059] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1568.69 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 17:33:12,116] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1614.67 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:33:24,750] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1529.17 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:33:38,404] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1847.17 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:33:51,554] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2013.08 | optimizer_gradients: 1.04 | optimizer_step: 2.20
[2025-01-06 17:34:05,088] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1863.19 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:34:18,084] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1615.44 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:34:30,714] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1443.90 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 17:34:43,630] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1785.49 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:34:43,631] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=0, lr=[2.25e-05, 2.25e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:34:43,633] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 423.35 | fwd_microstep: 23966.52 | bwd_microstep: 68587.78 | bwd_inner_microstep: 68587.19 | bwd_allreduce_microstep: 0.07 | step_microstep: 17178.75
[2025-01-06 17:34:43,634] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23966.42 | bwd: 68587.62 | bwd_inner: 68587.09 | bwd_allreduce: 0.07 | step: 17178.93
steps: 360 loss: 4.5224 iter time (s): 13.053 samples/sec: 78.451
[2025-01-06 17:34:43,653] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.321 | iteration      360/  320000 | elapsed time per iteration (ms): 13074.4 | learning rate: 2.250E-05 | approx flops per GPU: 119.2TFLOPS | MFU: 28.66% | HFU: 38.21% | lm_loss: 4.564583E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:34:56,597] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1817.89 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:35:09,718] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1909.25 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:35:23,422] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1940.14 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 17:35:36,834] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1901.23 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:35:50,476] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1762.92 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:36:04,151] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1632.82 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:36:17,422] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1657.86 | optimizer_gradients: 1.01 | optimizer_step: 2.20
[2025-01-06 17:36:30,315] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1717.92 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:36:44,029] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1971.32 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:36:56,713] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1507.37 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 17:36:56,714] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=0, lr=[2.3125000000000003e-05, 2.3125000000000003e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:36:56,716] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 584.22 | fwd_microstep: 23981.33 | bwd_microstep: 68646.39 | bwd_inner_microstep: 68645.74 | bwd_allreduce_microstep: 0.07 | step_microstep: 18168.22
[2025-01-06 17:36:56,717] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23981.22 | bwd: 68646.17 | bwd_inner: 68645.64 | bwd_allreduce: 0.07 | step: 18168.41
steps: 370 loss: 4.4308 iter time (s): 13.298 samples/sec: 77.005
[2025-01-06 17:36:56,722] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 76.953 | iteration      370/  320000 | elapsed time per iteration (ms): 13306.8 | learning rate: 2.313E-05 | approx flops per GPU: 117.1TFLOPS | MFU: 28.16% | HFU: 37.55% | lm_loss: 4.478696E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:37:09,897] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1699.21 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:37:22,942] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1866.87 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:37:35,953] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1867.96 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 17:37:49,381] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1952.55 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:38:02,193] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1650.30 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:38:14,918] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1543.65 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:38:27,694] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1621.32 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:38:41,382] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2010.99 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:38:54,134] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1652.98 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:39:08,133] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1920.25 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 17:39:08,134] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=0, lr=[2.3749999999999998e-05, 2.3749999999999998e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:39:08,135] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 455.65 | fwd_microstep: 24015.61 | bwd_microstep: 68620.76 | bwd_inner_microstep: 68620.20 | bwd_allreduce_microstep: 0.07 | step_microstep: 18179.35
[2025-01-06 17:39:08,137] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 24015.51 | bwd: 68620.61 | bwd_inner: 68620.09 | bwd_allreduce: 0.07 | step: 18179.53
steps: 380 loss: 4.4024 iter time (s): 13.129 samples/sec: 77.996
[2025-01-06 17:39:08,147] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.915 | iteration      380/  320000 | elapsed time per iteration (ms): 13142.5 | learning rate: 2.375E-05 | approx flops per GPU: 118.6TFLOPS | MFU: 28.51% | HFU: 38.01% | lm_loss: 4.409412E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:39:20,925] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1647.26 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 17:39:34,567] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1896.91 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:39:47,186] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1434.65 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:39:59,934] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1629.07 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:40:13,141] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1539.94 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:40:25,895] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1630.39 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:40:38,689] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1652.62 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:40:52,257] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1770.40 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:41:05,454] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1582.00 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:41:18,354] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1795.71 | optimizer_gradients: 1.03 | optimizer_step: 2.19
[2025-01-06 17:41:18,354] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=0, lr=[2.4375e-05, 2.4375e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:41:18,356] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 502.72 | fwd_microstep: 24017.23 | bwd_microstep: 68638.94 | bwd_inner_microstep: 68638.37 | bwd_allreduce_microstep: 0.07 | step_microstep: 16979.02
[2025-01-06 17:41:18,357] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 24017.12 | bwd: 68638.79 | bwd_inner: 68638.27 | bwd_allreduce: 0.07 | step: 16979.20
steps: 390 loss: 4.4305 iter time (s): 13.009 samples/sec: 78.716
[2025-01-06 17:41:18,370] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.634 | iteration      390/  320000 | elapsed time per iteration (ms): 13022.3 | learning rate: 2.437E-05 | approx flops per GPU: 119.7TFLOPS | MFU: 28.77% | HFU: 38.37% | lm_loss: 4.384923E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:41:31,534] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2021.12 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:41:44,165] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1472.26 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 17:41:57,718] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1647.82 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:42:10,326] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1481.33 | optimizer_gradients: 1.03 | optimizer_step: 2.19
[2025-01-06 17:42:23,154] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1696.05 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:42:36,261] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1688.84 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:42:49,214] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1801.52 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:43:01,944] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1559.18 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:43:15,137] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1739.51 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 17:43:28,011] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1685.55 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:43:28,011] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=0, lr=[2.5e-05, 2.5e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:43:28,013] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 439.89 | fwd_microstep: 23947.30 | bwd_microstep: 68641.56 | bwd_inner_microstep: 68640.98 | bwd_allreduce_microstep: 0.07 | step_microstep: 17226.14
[2025-01-06 17:43:28,014] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23947.20 | bwd: 68641.39 | bwd_inner: 68640.87 | bwd_allreduce: 0.07 | step: 17226.32
steps: 400 loss: 4.3643 iter time (s): 12.938 samples/sec: 79.145
[2025-01-06 17:43:28,030] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.976 | iteration      400/  320000 | elapsed time per iteration (ms): 12966.0 | learning rate: 2.500E-05 | approx flops per GPU: 120.2TFLOPS | MFU: 28.90% | HFU: 38.53% | lm_loss: 4.380521E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:43:41,387] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1564.68 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 17:43:54,036] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1458.68 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:44:06,974] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1754.29 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:44:19,959] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1839.37 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 17:44:33,391] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1647.01 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:44:46,107] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1580.80 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 17:44:58,967] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1702.17 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:45:12,153] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1554.50 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:45:25,566] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1737.31 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:45:38,181] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1457.39 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:45:38,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=0, lr=[2.5625e-05, 2.5625e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:45:38,183] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 589.37 | fwd_microstep: 23928.03 | bwd_microstep: 68566.10 | bwd_inner_microstep: 68565.53 | bwd_allreduce_microstep: 0.07 | step_microstep: 16603.36
[2025-01-06 17:45:38,184] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23927.92 | bwd: 68565.95 | bwd_inner: 68565.42 | bwd_allreduce: 0.07 | step: 16603.54
steps: 410 loss: 4.2447 iter time (s): 13.002 samples/sec: 78.759
[2025-01-06 17:45:38,197] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.669 | iteration      410/  320000 | elapsed time per iteration (ms): 13016.6 | learning rate: 2.562E-05 | approx flops per GPU: 119.8TFLOPS | MFU: 28.79% | HFU: 38.38% | lm_loss: 4.324698E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:45:52,187] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1826.31 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:46:05,610] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1884.88 | optimizer_gradients: 1.12 | optimizer_step: 2.22
[2025-01-06 17:46:18,890] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1687.36 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:46:31,777] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1768.02 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:46:44,713] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1828.25 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:46:58,166] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1404.83 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:47:11,134] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1782.58 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:47:24,358] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1678.13 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:47:37,569] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1485.55 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:47:50,548] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1535.08 | optimizer_gradients: 1.07 | optimizer_step: 2.21
[2025-01-06 17:47:50,548] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=0, lr=[2.625e-05, 2.625e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:47:50,550] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 646.16 | fwd_microstep: 23898.35 | bwd_microstep: 68430.21 | bwd_inner_microstep: 68429.60 | bwd_allreduce_microstep: 0.07 | step_microstep: 17345.69
[2025-01-06 17:47:50,551] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23898.25 | bwd: 68430.01 | bwd_inner: 68429.50 | bwd_allreduce: 0.07 | step: 17345.87
steps: 420 loss: 4.2674 iter time (s): 13.228 samples/sec: 77.411
[2025-01-06 17:47:50,555] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.366 | iteration      420/  320000 | elapsed time per iteration (ms): 13235.9 | learning rate: 2.625E-05 | approx flops per GPU: 117.8TFLOPS | MFU: 28.31% | HFU: 37.75% | lm_loss: 4.259958E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:48:03,150] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1432.76 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 17:48:16,272] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1632.71 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 17:48:29,005] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1591.32 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:48:43,021] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1862.17 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:48:56,411] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1816.61 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:49:09,478] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1852.60 | optimizer_gradients: 1.12 | optimizer_step: 2.22
[2025-01-06 17:49:22,916] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1587.00 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:49:36,282] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1695.10 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:49:49,637] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1700.37 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:50:02,638] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1832.85 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:50:02,639] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=0, lr=[2.6875000000000003e-05, 2.6875000000000003e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:50:02,640] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 456.70 | fwd_microstep: 23876.51 | bwd_microstep: 68523.92 | bwd_inner_microstep: 68523.23 | bwd_allreduce_microstep: 0.09 | step_microstep: 17447.58
[2025-01-06 17:50:02,641] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23876.41 | bwd: 68523.78 | bwd_inner: 68523.14 | bwd_allreduce: 0.09 | step: 17447.76
steps: 430 loss: 4.2835 iter time (s): 13.202 samples/sec: 77.562
[2025-01-06 17:50:02,650] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.520 | iteration      430/  320000 | elapsed time per iteration (ms): 13209.4 | learning rate: 2.688E-05 | approx flops per GPU: 118.0TFLOPS | MFU: 28.37% | HFU: 37.82% | lm_loss: 4.245298E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:50:15,646] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1839.31 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:50:28,703] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1449.12 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:50:41,422] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1597.51 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:50:54,162] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1639.98 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:51:07,611] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1546.31 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:51:20,630] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1900.34 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:51:33,866] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1688.44 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:51:47,175] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1711.63 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:52:00,855] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1918.17 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:52:13,522] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1552.97 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:52:13,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=0, lr=[2.7500000000000004e-05, 2.7500000000000004e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:52:13,524] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 682.41 | fwd_microstep: 23808.37 | bwd_microstep: 68478.84 | bwd_inner_microstep: 68478.28 | bwd_allreduce_microstep: 0.07 | step_microstep: 17248.50
[2025-01-06 17:52:13,525] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23808.25 | bwd: 68478.70 | bwd_inner: 68478.19 | bwd_allreduce: 0.07 | step: 17248.67
steps: 440 loss: 4.2040 iter time (s): 13.074 samples/sec: 78.321
[2025-01-06 17:52:13,538] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.235 | iteration      440/  320000 | elapsed time per iteration (ms): 13088.8 | learning rate: 2.750E-05 | approx flops per GPU: 119.1TFLOPS | MFU: 28.63% | HFU: 38.17% | lm_loss: 4.226352E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:52:27,088] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1599.08 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:52:40,431] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1643.03 | optimizer_gradients: 1.12 | optimizer_step: 2.22
[2025-01-06 17:52:53,549] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1592.30 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:53:07,126] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1553.03 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:53:19,766] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1491.94 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:53:32,388] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1495.14 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:53:45,163] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1647.21 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:53:58,132] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1826.91 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:54:11,627] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1626.70 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:54:24,404] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1611.69 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:54:24,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=0, lr=[2.8125000000000003e-05, 2.8125000000000003e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:54:24,406] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 622.64 | fwd_microstep: 23882.70 | bwd_microstep: 68547.74 | bwd_inner_microstep: 68547.18 | bwd_allreduce_microstep: 0.07 | step_microstep: 16493.86
[2025-01-06 17:54:24,407] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23882.59 | bwd: 68547.60 | bwd_inner: 68547.06 | bwd_allreduce: 0.07 | step: 16494.04
steps: 450 loss: 4.2304 iter time (s): 13.061 samples/sec: 78.400
[2025-01-06 17:54:24,418] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.240 | iteration      450/  320000 | elapsed time per iteration (ms): 13087.9 | learning rate: 2.813E-05 | approx flops per GPU: 119.1TFLOPS | MFU: 28.63% | HFU: 38.17% | lm_loss: 4.195051E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:54:37,423] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1530.56 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:54:50,778] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1769.14 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:55:03,697] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1767.38 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:55:16,849] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1608.80 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:55:30,672] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1657.92 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 17:55:44,411] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1664.89 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 17:55:57,763] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1687.58 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:56:10,758] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1852.88 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 17:56:24,440] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1798.74 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:56:38,236] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1822.23 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 17:56:38,237] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[2.875e-05, 2.875e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:56:38,238] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 612.08 | fwd_microstep: 23837.07 | bwd_microstep: 68514.01 | bwd_inner_microstep: 68513.42 | bwd_allreduce_microstep: 0.07 | step_microstep: 17599.62
[2025-01-06 17:56:38,239] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23836.96 | bwd: 68513.85 | bwd_inner: 68513.30 | bwd_allreduce: 0.07 | step: 17599.81
steps: 460 loss: 4.1374 iter time (s): 13.366 samples/sec: 76.610
[2025-01-06 17:56:38,250] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 76.513 | iteration      460/  320000 | elapsed time per iteration (ms): 13383.3 | learning rate: 2.875E-05 | approx flops per GPU: 116.5TFLOPS | MFU: 28.00% | HFU: 37.33% | lm_loss: 4.123208E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:56:52,100] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1838.69 | optimizer_gradients: 1.03 | optimizer_step: 2.19
[2025-01-06 17:57:06,082] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1882.67 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:57:18,976] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1770.74 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 17:57:32,410] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1901.51 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:57:45,639] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1817.80 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:57:58,720] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1638.82 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:58:12,049] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1872.73 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 17:58:25,049] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1863.49 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:58:37,857] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1688.61 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:58:50,918] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1917.39 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 17:58:50,918] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=0, lr=[2.9375e-05, 2.9375e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 17:58:50,920] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 592.58 | fwd_microstep: 23841.55 | bwd_microstep: 68440.22 | bwd_inner_microstep: 68439.64 | bwd_allreduce_microstep: 0.07 | step_microstep: 18620.23
[2025-01-06 17:58:50,921] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23841.44 | bwd: 68440.07 | bwd_inner: 68439.53 | bwd_allreduce: 0.07 | step: 18620.40
steps: 470 loss: 4.1731 iter time (s): 13.247 samples/sec: 77.298
[2025-01-06 17:58:50,926] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.181 | iteration      470/  320000 | elapsed time per iteration (ms): 13267.6 | learning rate: 2.937E-05 | approx flops per GPU: 117.5TFLOPS | MFU: 28.24% | HFU: 37.66% | lm_loss: 4.124798E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 17:59:04,737] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1965.01 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 17:59:17,787] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1625.63 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 17:59:31,213] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1860.37 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 17:59:44,708] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1838.97 | optimizer_gradients: 1.04 | optimizer_step: 2.20
[2025-01-06 17:59:57,702] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1605.12 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:00:10,562] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1651.76 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:00:24,177] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1683.99 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:00:37,822] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1628.40 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:00:50,921] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1919.60 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:01:03,575] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1465.38 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:01:03,575] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=0, lr=[3e-05, 3e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:01:03,577] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 534.64 | fwd_microstep: 23887.36 | bwd_microstep: 68585.60 | bwd_inner_microstep: 68585.04 | bwd_allreduce_microstep: 0.07 | step_microstep: 17727.91
[2025-01-06 18:01:03,578] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23887.26 | bwd: 68585.46 | bwd_inner: 68584.91 | bwd_allreduce: 0.07 | step: 17728.10
steps: 480 loss: 4.0687 iter time (s): 13.253 samples/sec: 77.266
[2025-01-06 18:01:03,579] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.194 | iteration      480/  320000 | elapsed time per iteration (ms): 13265.3 | learning rate: 3.000E-05 | approx flops per GPU: 117.5TFLOPS | MFU: 28.25% | HFU: 37.66% | lm_loss: 4.144957E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:01:16,170] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1455.48 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:01:29,394] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1923.83 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:01:42,493] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1485.40 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:01:55,699] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1706.00 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:02:08,278] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1477.43 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:02:21,950] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2060.11 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:02:34,733] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1652.03 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 18:02:48,312] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1786.31 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:03:01,464] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1671.31 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:03:15,176] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2070.77 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:03:15,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=0, lr=[3.0625e-05, 3.0625e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:03:15,178] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 974.69 | fwd_microstep: 23888.55 | bwd_microstep: 68540.25 | bwd_inner_microstep: 68539.66 | bwd_allreduce_microstep: 0.07 | step_microstep: 17716.38
[2025-01-06 18:03:15,179] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23888.44 | bwd: 68540.10 | bwd_inner: 68539.50 | bwd_allreduce: 0.07 | step: 17716.57
steps: 490 loss: 3.9894 iter time (s): 13.153 samples/sec: 77.855
[2025-01-06 18:03:15,181] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.810 | iteration      490/  320000 | elapsed time per iteration (ms): 13160.2 | learning rate: 3.062E-05 | approx flops per GPU: 118.4TFLOPS | MFU: 28.47% | HFU: 37.96% | lm_loss: 4.042378E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:03:28,002] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1661.50 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:03:40,822] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1696.48 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 18:03:53,770] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1592.01 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:04:06,965] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1591.81 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:04:20,226] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1740.96 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:04:33,484] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1695.31 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:04:46,447] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1884.76 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 18:04:59,502] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1660.26 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:05:12,175] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1508.81 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:05:25,556] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1475.61 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:05:25,557] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=0, lr=[3.125e-05, 3.125e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:05:25,559] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 512.79 | fwd_microstep: 23898.18 | bwd_microstep: 68562.63 | bwd_inner_microstep: 68562.04 | bwd_allreduce_microstep: 0.07 | step_microstep: 16880.00
[2025-01-06 18:05:25,560] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23898.08 | bwd: 68562.46 | bwd_inner: 68561.94 | bwd_allreduce: 0.07 | step: 16880.19
steps: 500 loss: 3.9924 iter time (s): 13.021 samples/sec: 78.641
[2025-01-06 18:05:25,578] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.529 | iteration      500/  320000 | elapsed time per iteration (ms): 13039.7 | learning rate: 3.125E-05 | approx flops per GPU: 119.5TFLOPS | MFU: 28.74% | HFU: 38.31% | lm_loss: 4.000087E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:05:38,826] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1651.71 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:05:51,747] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1697.55 | optimizer_gradients: 1.12 | optimizer_step: 2.20
[2025-01-06 18:06:05,570] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1840.64 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:06:19,165] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1904.45 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:06:32,162] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1809.55 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:06:45,595] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1688.41 | optimizer_gradients: 1.12 | optimizer_step: 2.20
[2025-01-06 18:06:58,490] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1712.99 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:07:11,875] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1854.70 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 18:07:25,129] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1493.21 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 18:07:37,837] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1592.50 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:07:37,838] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=0, lr=[3.1875e-05, 3.1875e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:07:37,839] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 412.62 | fwd_microstep: 23941.49 | bwd_microstep: 68593.17 | bwd_inner_microstep: 68592.59 | bwd_allreduce_microstep: 0.07 | step_microstep: 17685.84
[2025-01-06 18:07:37,840] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23941.38 | bwd: 68593.01 | bwd_inner: 68592.47 | bwd_allreduce: 0.07 | step: 17686.07
steps: 510 loss: 4.0204 iter time (s): 13.204 samples/sec: 77.554
[2025-01-06 18:07:37,854] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.414 | iteration      510/  320000 | elapsed time per iteration (ms): 13227.5 | learning rate: 3.188E-05 | approx flops per GPU: 117.8TFLOPS | MFU: 28.33% | HFU: 37.77% | lm_loss: 4.016909E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:07:50,906] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1925.60 | optimizer_gradients: 1.04 | optimizer_step: 2.20
[2025-01-06 18:08:03,965] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1918.97 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 18:08:17,010] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1865.90 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 18:08:30,347] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1544.86 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:08:43,622] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1757.28 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:08:56,622] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1856.89 | optimizer_gradients: 1.12 | optimizer_step: 6.29
[2025-01-06 18:09:09,718] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1940.04 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:09:22,758] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1917.84 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:09:36,122] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1807.07 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:09:48,792] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1553.28 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 18:09:48,792] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=0, lr=[3.2500000000000004e-05, 3.2500000000000004e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:09:48,794] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 583.29 | fwd_microstep: 23871.07 | bwd_microstep: 68515.02 | bwd_inner_microstep: 68514.42 | bwd_allreduce_microstep: 0.07 | step_microstep: 18479.13
[2025-01-06 18:09:48,795] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23870.97 | bwd: 68514.84 | bwd_inner: 68514.29 | bwd_allreduce: 0.07 | step: 18479.33
steps: 520 loss: 3.9692 iter time (s): 13.081 samples/sec: 78.280
[2025-01-06 18:09:48,806] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.197 | iteration      520/  320000 | elapsed time per iteration (ms): 13095.2 | learning rate: 3.250E-05 | approx flops per GPU: 119.0TFLOPS | MFU: 28.61% | HFU: 38.15% | lm_loss: 3.975742E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:10:01,513] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1543.57 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:10:14,106] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1491.32 | optimizer_gradients: 1.11 | optimizer_step: 2.19
[2025-01-06 18:10:27,274] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2015.80 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 18:10:40,386] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1945.24 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:10:53,943] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1692.19 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:11:06,590] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1504.97 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:11:19,836] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1606.15 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:11:32,746] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1749.49 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:11:45,343] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1452.80 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:11:58,286] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1783.20 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 18:11:58,286] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=0, lr=[3.3125000000000006e-05, 3.3125000000000006e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:11:58,288] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 496.39 | fwd_microstep: 23860.76 | bwd_microstep: 68498.80 | bwd_inner_microstep: 68498.22 | bwd_allreduce_microstep: 0.07 | step_microstep: 17206.23
[2025-01-06 18:11:58,289] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23860.66 | bwd: 68498.65 | bwd_inner: 68498.12 | bwd_allreduce: 0.07 | step: 17206.42
steps: 530 loss: 3.8775 iter time (s): 12.921 samples/sec: 79.252
[2025-01-06 18:11:58,291] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 79.082 | iteration      530/  320000 | elapsed time per iteration (ms): 12948.6 | learning rate: 3.313E-05 | approx flops per GPU: 120.4TFLOPS | MFU: 28.94% | HFU: 38.58% | lm_loss: 3.944759E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:12:11,468] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1566.88 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:12:24,482] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1839.54 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 18:12:38,180] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1961.37 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:12:51,411] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1639.29 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:13:03,990] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1384.41 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:13:16,609] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1462.90 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:13:29,620] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1870.32 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 18:13:42,295] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1569.18 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:13:56,030] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1865.61 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:14:09,614] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1543.41 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:14:09,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=0, lr=[3.375e-05, 3.375e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:14:09,616] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 570.25 | fwd_microstep: 23917.68 | bwd_microstep: 68649.52 | bwd_inner_microstep: 68648.93 | bwd_allreduce_microstep: 0.07 | step_microstep: 17125.27
[2025-01-06 18:14:09,617] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23917.57 | bwd: 68649.37 | bwd_inner: 68648.81 | bwd_allreduce: 0.07 | step: 17125.47
steps: 540 loss: 3.9232 iter time (s): 13.112 samples/sec: 78.096
[2025-01-06 18:14:09,630] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.966 | iteration      540/  320000 | elapsed time per iteration (ms): 13133.9 | learning rate: 3.375E-05 | approx flops per GPU: 118.7TFLOPS | MFU: 28.53% | HFU: 38.04% | lm_loss: 3.902160E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:14:23,506] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1993.49 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:14:36,557] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1888.04 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 18:14:49,654] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1635.66 | optimizer_gradients: 1.06 | optimizer_step: 2.22
[2025-01-06 18:15:02,553] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1692.95 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:15:15,343] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1612.87 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:15:29,662] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1944.59 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:15:42,263] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1464.96 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:15:55,515] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1530.50 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:16:08,610] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1932.63 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:16:21,619] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1855.07 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:16:21,620] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=0, lr=[3.4375e-05, 3.4375e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:16:21,622] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 592.78 | fwd_microstep: 23899.68 | bwd_microstep: 68562.27 | bwd_inner_microstep: 68561.70 | bwd_allreduce_microstep: 0.07 | step_microstep: 17968.39
[2025-01-06 18:16:21,623] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23899.57 | bwd: 68562.13 | bwd_inner: 68561.58 | bwd_allreduce: 0.07 | step: 17968.57
steps: 550 loss: 3.8701 iter time (s): 13.187 samples/sec: 77.654
[2025-01-06 18:16:21,632] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.574 | iteration      550/  320000 | elapsed time per iteration (ms): 13200.2 | learning rate: 3.438E-05 | approx flops per GPU: 118.1TFLOPS | MFU: 28.39% | HFU: 37.85% | lm_loss: 3.879387E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:16:34,858] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1587.69 | optimizer_gradients: 1.02 | optimizer_step: 2.18
[2025-01-06 18:16:47,986] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1961.05 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:17:01,069] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1874.56 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:17:14,864] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2008.92 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:17:28,484] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2069.83 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:17:42,200] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1654.47 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:17:55,386] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1629.14 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:18:08,085] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1569.25 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:18:21,318] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1643.78 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:18:34,518] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1653.57 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:18:34,519] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=0, lr=[3.5000000000000004e-05, 3.5000000000000004e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:18:34,521] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 669.30 | fwd_microstep: 23856.82 | bwd_microstep: 68576.40 | bwd_inner_microstep: 68575.82 | bwd_allreduce_microstep: 0.07 | step_microstep: 18056.17
[2025-01-06 18:18:34,522] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23856.71 | bwd: 68576.24 | bwd_inner: 68575.70 | bwd_allreduce: 0.07 | step: 18056.36
steps: 560 loss: 3.9172 iter time (s): 13.284 samples/sec: 77.083
[2025-01-06 18:18:34,541] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.045 | iteration      560/  320000 | elapsed time per iteration (ms): 13290.9 | learning rate: 3.500E-05 | approx flops per GPU: 117.3TFLOPS | MFU: 28.19% | HFU: 37.59% | lm_loss: 3.911697E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:18:48,285] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1805.79 | optimizer_gradients: 1.11 | optimizer_step: 2.21
[2025-01-06 18:19:01,424] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1590.83 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:19:14,318] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1323.34 | optimizer_gradients: 1.08 | optimizer_step: 2.21
[2025-01-06 18:19:27,378] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1559.91 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:19:40,378] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1839.40 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:19:53,342] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1843.70 | optimizer_gradients: 1.03 | optimizer_step: 2.19
[2025-01-06 18:20:06,330] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1868.03 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:20:20,038] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1867.75 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:20:32,754] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1571.80 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:20:45,881] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1983.67 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 18:20:45,881] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=0, lr=[3.5625e-05, 3.5625e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:20:45,883] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 600.48 | fwd_microstep: 23934.38 | bwd_microstep: 68566.41 | bwd_inner_microstep: 68565.80 | bwd_allreduce_microstep: 0.07 | step_microstep: 17588.18
[2025-01-06 18:20:45,884] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23934.28 | bwd: 68566.23 | bwd_inner: 68565.71 | bwd_allreduce: 0.07 | step: 17588.36
steps: 570 loss: 3.8342 iter time (s): 13.127 samples/sec: 78.005
[2025-01-06 18:20:45,898] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.955 | iteration      570/  320000 | elapsed time per iteration (ms): 13135.7 | learning rate: 3.562E-05 | approx flops per GPU: 118.7TFLOPS | MFU: 28.53% | HFU: 38.03% | lm_loss: 3.869301E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:20:58,998] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1538.01 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:21:11,793] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1656.30 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:21:24,933] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1575.00 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:21:37,985] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1451.99 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:21:50,809] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1597.32 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:22:03,851] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1365.39 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:22:16,606] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1655.71 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:22:29,763] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1524.66 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:22:42,468] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1535.76 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:22:56,098] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1910.72 | optimizer_gradients: 1.12 | optimizer_step: 2.21
[2025-01-06 18:22:56,098] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=0, lr=[3.625e-05, 3.625e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:22:56,100] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 526.27 | fwd_microstep: 23972.29 | bwd_microstep: 68681.37 | bwd_inner_microstep: 68680.79 | bwd_allreduce_microstep: 0.07 | step_microstep: 16184.22
[2025-01-06 18:22:56,101] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23972.19 | bwd: 68681.21 | bwd_inner: 68680.69 | bwd_allreduce: 0.07 | step: 16184.42
steps: 580 loss: 3.7421 iter time (s): 12.999 samples/sec: 78.776
[2025-01-06 18:22:56,105] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.644 | iteration      580/  320000 | elapsed time per iteration (ms): 13020.7 | learning rate: 3.625E-05 | approx flops per GPU: 119.7TFLOPS | MFU: 28.78% | HFU: 38.37% | lm_loss: 3.789610E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:23:09,274] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2011.71 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:23:22,051] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1621.96 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:23:35,171] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1942.63 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 18:23:48,150] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1399.26 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:24:01,160] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1850.13 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:24:15,067] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1908.44 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:24:27,824] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1592.22 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:24:41,330] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1561.12 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:24:54,930] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1969.32 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:25:08,468] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1867.17 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:25:08,468] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=0, lr=[3.6875e-05, 3.6875e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:25:08,470] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 475.05 | fwd_microstep: 23916.52 | bwd_microstep: 68527.16 | bwd_inner_microstep: 68526.59 | bwd_allreduce_microstep: 0.07 | step_microstep: 18210.29
[2025-01-06 18:25:08,471] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23916.42 | bwd: 68527.02 | bwd_inner: 68526.49 | bwd_allreduce: 0.07 | step: 18210.47
steps: 590 loss: 3.6910 iter time (s): 13.226 samples/sec: 77.423
[2025-01-06 18:25:08,494] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.348 | iteration      590/  320000 | elapsed time per iteration (ms): 13238.9 | learning rate: 3.688E-05 | approx flops per GPU: 117.7TFLOPS | MFU: 28.30% | HFU: 37.74% | lm_loss: 3.736835E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:25:21,650] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1586.82 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:25:34,868] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2039.86 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:25:48,388] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1823.65 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:26:01,392] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1474.87 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 18:26:14,079] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1546.36 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:26:26,802] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1557.13 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:26:39,885] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1552.22 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:26:52,680] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1636.34 | optimizer_gradients: 1.02 | optimizer_step: 2.23
[2025-01-06 18:27:05,678] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1801.39 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:27:18,768] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1870.92 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 18:27:18,768] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=0, lr=[3.7500000000000003e-05, 3.7500000000000003e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:27:18,770] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 571.80 | fwd_microstep: 23913.08 | bwd_microstep: 68600.01 | bwd_inner_microstep: 68599.44 | bwd_allreduce_microstep: 0.07 | step_microstep: 17318.21
[2025-01-06 18:27:18,771] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23912.97 | bwd: 68599.86 | bwd_inner: 68599.32 | bwd_allreduce: 0.07 | step: 17318.39
steps: 600 loss: 3.8151 iter time (s): 13.017 samples/sec: 78.666
[2025-01-06 18:27:18,773] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.601 | iteration      600/  320000 | elapsed time per iteration (ms): 13027.8 | learning rate: 3.750E-05 | approx flops per GPU: 119.6TFLOPS | MFU: 28.76% | HFU: 38.35% | lm_loss: 3.814764E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:27:32,369] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1705.89 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:27:45,089] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1585.89 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:27:57,878] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1656.99 | optimizer_gradients: 1.02 | optimizer_step: 2.23
[2025-01-06 18:28:10,936] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1849.63 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:28:24,306] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1820.77 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 18:28:36,961] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1495.10 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:28:49,595] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1520.63 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:29:03,167] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1695.77 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:29:16,210] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1657.52 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:29:30,175] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1627.76 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:29:30,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=0, lr=[3.8125000000000005e-05, 3.8125000000000005e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:29:30,177] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 460.65 | fwd_microstep: 23897.75 | bwd_microstep: 68545.46 | bwd_inner_microstep: 68544.88 | bwd_allreduce_microstep: 0.07 | step_microstep: 17061.31
[2025-01-06 18:29:30,178] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23897.65 | bwd: 68545.30 | bwd_inner: 68544.77 | bwd_allreduce: 0.07 | step: 17061.48
steps: 610 loss: 3.7268 iter time (s): 13.118 samples/sec: 78.060
[2025-01-06 18:29:30,180] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.926 | iteration      610/  320000 | elapsed time per iteration (ms): 13140.7 | learning rate: 3.813E-05 | approx flops per GPU: 118.6TFLOPS | MFU: 28.51% | HFU: 38.02% | lm_loss: 3.796500E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:29:43,022] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1691.62 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:29:56,262] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1620.28 | optimizer_gradients: 1.03 | optimizer_step: 2.23
[2025-01-06 18:30:09,069] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1662.94 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 18:30:22,128] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1611.72 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:30:35,231] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1521.96 | optimizer_gradients: 1.09 | optimizer_step: 2.21
[2025-01-06 18:30:48,186] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1860.35 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:31:01,814] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1887.81 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:31:14,954] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1539.82 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:31:27,888] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1495.66 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 18:31:41,703] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1754.59 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 18:31:41,704] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=0, lr=[3.875e-05, 3.875e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:31:41,706] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 483.75 | fwd_microstep: 23904.73 | bwd_microstep: 68439.66 | bwd_inner_microstep: 68439.01 | bwd_allreduce_microstep: 0.07 | step_microstep: 17039.39
[2025-01-06 18:31:41,707] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23904.62 | bwd: 68439.42 | bwd_inner: 68438.90 | bwd_allreduce: 0.07 | step: 17039.58
steps: 620 loss: 3.6448 iter time (s): 13.132 samples/sec: 77.975
[2025-01-06 18:31:41,717] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.849 | iteration      620/  320000 | elapsed time per iteration (ms): 13153.7 | learning rate: 3.875E-05 | approx flops per GPU: 118.5TFLOPS | MFU: 28.49% | HFU: 37.98% | lm_loss: 3.679739E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:31:54,934] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1698.43 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:32:09,077] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1943.84 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:32:21,830] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1591.47 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 18:32:35,165] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1583.25 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:32:48,006] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1673.14 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:33:01,563] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1672.87 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:33:14,416] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1724.75 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:33:27,287] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1663.88 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:33:40,773] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1851.31 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:33:54,274] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1565.44 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:33:54,275] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=0, lr=[3.9375e-05, 3.9375e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:33:54,276] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 431.71 | fwd_microstep: 23898.83 | bwd_microstep: 68478.39 | bwd_inner_microstep: 68477.82 | bwd_allreduce_microstep: 0.07 | step_microstep: 17346.74
[2025-01-06 18:33:54,278] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23898.73 | bwd: 68478.24 | bwd_inner: 68477.71 | bwd_allreduce: 0.07 | step: 17346.92
steps: 630 loss: 3.6235 iter time (s): 13.237 samples/sec: 77.356
[2025-01-06 18:33:54,283] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.244 | iteration      630/  320000 | elapsed time per iteration (ms): 13256.7 | learning rate: 3.938E-05 | approx flops per GPU: 117.6TFLOPS | MFU: 28.27% | HFU: 37.69% | lm_loss: 3.638481E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:34:07,430] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1558.65 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:34:20,325] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1753.93 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:34:33,242] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1793.61 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:34:46,492] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1444.22 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 18:34:59,816] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1488.45 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:35:12,510] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1532.96 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:35:25,735] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1513.24 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:35:38,530] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1624.99 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:35:51,445] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1749.76 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:36:04,192] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1647.05 | optimizer_gradients: 1.08 | optimizer_step: 5.94
[2025-01-06 18:36:04,193] [INFO] [logging.py:96:log_dist] [Rank 0] step=640, skipped=0, lr=[4e-05, 4e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:36:04,194] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 515.50 | fwd_microstep: 23905.97 | bwd_microstep: 68370.63 | bwd_inner_microstep: 68370.02 | bwd_allreduce_microstep: 0.07 | step_microstep: 16481.56
[2025-01-06 18:36:04,196] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23905.86 | bwd: 68370.46 | bwd_inner: 68369.92 | bwd_allreduce: 0.07 | step: 16481.76
steps: 640 loss: 3.6076 iter time (s): 12.978 samples/sec: 78.906
[2025-01-06 18:36:04,206] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.816 | iteration      640/  320000 | elapsed time per iteration (ms): 12992.2 | learning rate: 4.000E-05 | approx flops per GPU: 120.0TFLOPS | MFU: 28.84% | HFU: 38.45% | lm_loss: 3.647236E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:36:16,994] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1623.63 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:36:29,646] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1498.32 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:36:43,014] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1896.48 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:36:56,172] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1653.83 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:37:09,534] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1567.14 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:37:23,106] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1707.13 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:37:36,182] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1473.91 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:37:48,998] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1687.37 | optimizer_gradients: 1.11 | optimizer_step: 2.21
[2025-01-06 18:38:01,867] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1721.22 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 18:38:15,022] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1549.60 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:38:15,022] [INFO] [logging.py:96:log_dist] [Rank 0] step=650, skipped=0, lr=[4.0625e-05, 4.0625e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:38:15,024] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 379.83 | fwd_microstep: 23952.02 | bwd_microstep: 68474.37 | bwd_inner_microstep: 68473.78 | bwd_allreduce_microstep: 0.07 | step_microstep: 16751.63
[2025-01-06 18:38:15,025] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23951.93 | bwd: 68474.20 | bwd_inner: 68473.68 | bwd_allreduce: 0.07 | step: 16751.82
steps: 650 loss: 3.6018 iter time (s): 13.067 samples/sec: 78.367
[2025-01-06 18:38:15,038] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.268 | iteration      650/  320000 | elapsed time per iteration (ms): 13083.2 | learning rate: 4.062E-05 | approx flops per GPU: 119.1TFLOPS | MFU: 28.64% | HFU: 38.19% | lm_loss: 3.616853E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:38:27,740] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1582.11 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:38:40,765] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1889.26 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:38:54,036] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1837.59 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:39:06,708] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1534.83 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:39:20,086] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1872.65 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:39:33,249] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1648.13 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:39:46,456] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1774.72 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:39:59,448] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1778.68 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:40:12,853] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1768.73 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:40:26,034] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1510.69 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:40:26,035] [INFO] [logging.py:96:log_dist] [Rank 0] step=660, skipped=0, lr=[4.125e-05, 4.125e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:40:26,037] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 586.78 | fwd_microstep: 23927.48 | bwd_microstep: 68593.60 | bwd_inner_microstep: 68593.03 | bwd_allreduce_microstep: 0.07 | step_microstep: 17645.90
[2025-01-06 18:40:26,038] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23927.38 | bwd: 68593.44 | bwd_inner: 68592.91 | bwd_allreduce: 0.07 | step: 17646.09
steps: 660 loss: 3.6671 iter time (s): 13.079 samples/sec: 78.295
[2025-01-06 18:40:26,046] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.163 | iteration      660/  320000 | elapsed time per iteration (ms): 13100.8 | learning rate: 4.125E-05 | approx flops per GPU: 119.0TFLOPS | MFU: 28.60% | HFU: 38.14% | lm_loss: 3.620576E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:40:38,733] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1593.84 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:40:51,664] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1708.34 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:41:04,778] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1552.68 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:41:17,410] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1455.71 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:41:30,523] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1958.84 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 18:41:43,599] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1925.72 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:41:56,986] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1913.59 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:42:09,912] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1702.28 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:42:23,481] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1510.99 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:42:36,158] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1577.65 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 18:42:36,158] [INFO] [logging.py:96:log_dist] [Rank 0] step=670, skipped=0, lr=[4.1875e-05, 4.1875e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:42:36,160] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 561.70 | fwd_microstep: 23885.92 | bwd_microstep: 68513.37 | bwd_inner_microstep: 68512.81 | bwd_allreduce_microstep: 0.07 | step_microstep: 17316.73
[2025-01-06 18:42:36,161] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23885.82 | bwd: 68513.22 | bwd_inner: 68512.71 | bwd_allreduce: 0.07 | step: 17316.91
steps: 670 loss: 3.6229 iter time (s): 12.994 samples/sec: 78.807
[2025-01-06 18:42:36,165] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.697 | iteration      670/  320000 | elapsed time per iteration (ms): 13012.0 | learning rate: 4.188E-05 | approx flops per GPU: 119.8TFLOPS | MFU: 28.80% | HFU: 38.40% | lm_loss: 3.690814E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:42:48,707] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1419.58 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:43:02,150] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1486.35 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:43:15,105] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1851.03 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:43:28,314] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1774.50 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:43:41,348] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1892.08 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:43:54,574] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2108.85 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:44:07,876] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1879.85 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:44:21,373] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1703.96 | optimizer_gradients: 1.12 | optimizer_step: 2.21
[2025-01-06 18:44:34,315] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1801.74 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:44:47,149] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1712.24 | optimizer_gradients: 1.08 | optimizer_step: 2.21
[2025-01-06 18:44:47,150] [INFO] [logging.py:96:log_dist] [Rank 0] step=680, skipped=0, lr=[4.25e-05, 4.25e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:44:47,152] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 531.48 | fwd_microstep: 23923.73 | bwd_microstep: 68509.18 | bwd_inner_microstep: 68508.62 | bwd_allreduce_microstep: 0.07 | step_microstep: 17960.39
[2025-01-06 18:44:47,153] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23923.61 | bwd: 68509.03 | bwd_inner: 68508.50 | bwd_allreduce: 0.07 | step: 17960.56
steps: 680 loss: 3.5192 iter time (s): 13.084 samples/sec: 78.266
[2025-01-06 18:44:47,154] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.174 | iteration      680/  320000 | elapsed time per iteration (ms): 13098.9 | learning rate: 4.250E-05 | approx flops per GPU: 119.0TFLOPS | MFU: 28.61% | HFU: 38.14% | lm_loss: 3.555027E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:45:00,326] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1447.69 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 18:45:14,172] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2012.48 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:45:27,545] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1865.52 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:45:40,202] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1504.76 | optimizer_gradients: 1.08 | optimizer_step: 2.22
[2025-01-06 18:45:52,906] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1552.83 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:46:05,619] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1602.88 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:46:19,124] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1667.51 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:46:31,910] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1577.48 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 18:46:45,053] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1514.90 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:46:59,066] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1677.05 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 18:46:59,066] [INFO] [logging.py:96:log_dist] [Rank 0] step=690, skipped=0, lr=[4.3125000000000005e-05, 4.3125000000000005e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:46:59,068] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 550.00 | fwd_microstep: 23880.37 | bwd_microstep: 68434.19 | bwd_inner_microstep: 68433.63 | bwd_allreduce_microstep: 0.07 | step_microstep: 16813.07
[2025-01-06 18:46:59,069] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23880.25 | bwd: 68434.04 | bwd_inner: 68433.53 | bwd_allreduce: 0.07 | step: 16813.26
steps: 690 loss: 3.4383 iter time (s): 13.182 samples/sec: 77.681
[2025-01-06 18:46:59,077] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.621 | iteration      690/  320000 | elapsed time per iteration (ms): 13192.2 | learning rate: 4.313E-05 | approx flops per GPU: 118.2TFLOPS | MFU: 28.40% | HFU: 37.87% | lm_loss: 3.470648E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:47:12,170] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1505.59 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:47:24,887] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1530.42 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:47:37,460] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1437.96 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 18:47:50,299] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1657.91 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:48:03,241] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1842.99 | optimizer_gradients: 1.07 | optimizer_step: 2.20
[2025-01-06 18:48:16,433] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1647.82 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:48:29,830] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1900.89 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:48:43,021] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1566.18 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:48:55,546] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1426.03 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:49:08,402] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1688.89 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:49:08,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=700, skipped=0, lr=[4.3750000000000006e-05, 4.3750000000000006e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:49:08,404] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 564.55 | fwd_microstep: 23846.17 | bwd_microstep: 68436.59 | bwd_inner_microstep: 68436.02 | bwd_allreduce_microstep: 0.07 | step_microstep: 16609.77
[2025-01-06 18:49:08,405] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23846.06 | bwd: 68436.44 | bwd_inner: 68435.91 | bwd_allreduce: 0.07 | step: 16609.96
steps: 700 loss: 3.5393 iter time (s): 12.912 samples/sec: 79.308
[2025-01-06 18:49:08,418] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 79.171 | iteration      700/  320000 | elapsed time per iteration (ms): 12934.0 | learning rate: 4.375E-05 | approx flops per GPU: 120.5TFLOPS | MFU: 28.97% | HFU: 38.63% | lm_loss: 3.477229E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:49:21,035] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1464.64 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:49:33,869] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1682.08 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:49:46,516] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1469.40 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:49:59,549] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1850.18 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:50:12,589] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1906.93 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:50:26,121] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1921.56 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:50:39,167] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1898.75 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:50:52,622] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1826.40 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:51:05,990] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1920.53 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 18:51:19,725] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1948.43 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:51:19,726] [INFO] [logging.py:96:log_dist] [Rank 0] step=710, skipped=0, lr=[4.437500000000001e-05, 4.437500000000001e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:51:19,727] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 450.38 | fwd_microstep: 23919.96 | bwd_microstep: 68571.49 | bwd_inner_microstep: 68570.91 | bwd_allreduce_microstep: 0.07 | step_microstep: 18260.86
[2025-01-06 18:51:19,728] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23919.87 | bwd: 68571.33 | bwd_inner: 68570.80 | bwd_allreduce: 0.07 | step: 18261.05
steps: 710 loss: 3.4808 iter time (s): 13.113 samples/sec: 78.089
[2025-01-06 18:51:19,730] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.982 | iteration      710/  320000 | elapsed time per iteration (ms): 13131.2 | learning rate: 4.438E-05 | approx flops per GPU: 118.7TFLOPS | MFU: 28.54% | HFU: 38.05% | lm_loss: 3.531417E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:51:33,276] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1959.37 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:51:46,288] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1885.08 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:51:59,173] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1426.06 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:52:12,581] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1672.24 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:52:25,570] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1849.15 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:52:38,238] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1551.05 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:52:51,352] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1612.81 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:53:04,489] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1496.28 | optimizer_gradients: 1.09 | optimizer_step: 2.20
[2025-01-06 18:53:17,561] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1443.99 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:53:30,594] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1884.89 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 18:53:30,594] [INFO] [logging.py:96:log_dist] [Rank 0] step=720, skipped=0, lr=[4.5e-05, 4.5e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:53:30,596] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 496.85 | fwd_microstep: 23951.13 | bwd_microstep: 68553.20 | bwd_inner_microstep: 68552.63 | bwd_allreduce_microstep: 0.06 | step_microstep: 17174.72
[2025-01-06 18:53:30,597] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23951.02 | bwd: 68553.04 | bwd_inner: 68552.51 | bwd_allreduce: 0.07 | step: 17174.92
steps: 720 loss: 3.4515 iter time (s): 13.069 samples/sec: 78.352
[2025-01-06 18:53:30,601] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.245 | iteration      720/  320000 | elapsed time per iteration (ms): 13087.1 | learning rate: 4.500E-05 | approx flops per GPU: 119.1TFLOPS | MFU: 28.63% | HFU: 38.18% | lm_loss: 3.473222E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:53:43,787] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2019.60 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:53:56,693] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1453.45 | optimizer_gradients: 1.11 | optimizer_step: 2.21
[2025-01-06 18:54:09,831] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1423.97 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:54:23,290] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1598.67 | optimizer_gradients: 1.08 | optimizer_step: 2.21
[2025-01-06 18:54:36,733] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1469.07 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:54:49,821] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1950.12 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:55:02,630] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1706.55 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:55:15,798] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1668.78 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:55:28,557] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1614.20 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:55:41,453] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1744.70 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 18:55:41,454] [INFO] [logging.py:96:log_dist] [Rank 0] step=730, skipped=0, lr=[4.5625000000000004e-05, 4.5625000000000004e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:55:41,456] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 445.20 | fwd_microstep: 23955.24 | bwd_microstep: 68527.20 | bwd_inner_microstep: 68526.64 | bwd_allreduce_microstep: 0.07 | step_microstep: 17042.52
[2025-01-06 18:55:41,457] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23955.13 | bwd: 68527.05 | bwd_inner: 68526.53 | bwd_allreduce: 0.07 | step: 17042.72
steps: 730 loss: 3.3545 iter time (s): 13.059 samples/sec: 78.416
[2025-01-06 18:55:41,458] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 78.253 | iteration      730/  320000 | elapsed time per iteration (ms): 13085.7 | learning rate: 4.563E-05 | approx flops per GPU: 119.1TFLOPS | MFU: 28.63% | HFU: 38.18% | lm_loss: 3.389336E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:55:55,109] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1470.60 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:56:08,626] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1584.21 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 18:56:21,350] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1603.86 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:56:34,001] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1544.06 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:56:47,914] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1598.28 | optimizer_gradients: 1.05 | optimizer_step: 2.20
[2025-01-06 18:57:00,805] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1698.01 | optimizer_gradients: 1.01 | optimizer_step: 2.21
[2025-01-06 18:57:13,867] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1882.60 | optimizer_gradients: 1.02 | optimizer_step: 5.81
[2025-01-06 18:57:27,166] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1685.86 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:57:39,745] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1451.99 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:57:53,265] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1889.63 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:57:53,266] [INFO] [logging.py:96:log_dist] [Rank 0] step=740, skipped=0, lr=[4.6250000000000006e-05, 4.6250000000000006e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 18:57:53,267] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 484.53 | fwd_microstep: 23884.70 | bwd_microstep: 68443.63 | bwd_inner_microstep: 68443.03 | bwd_allreduce_microstep: 0.07 | step_microstep: 16802.34
[2025-01-06 18:57:53,268] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23884.60 | bwd: 68443.46 | bwd_inner: 68442.93 | bwd_allreduce: 0.07 | step: 16802.53
steps: 740 loss: 3.4026 iter time (s): 13.167 samples/sec: 77.769
[2025-01-06 18:57:53,280] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.680 | iteration      740/  320000 | elapsed time per iteration (ms): 13182.3 | learning rate: 4.625E-05 | approx flops per GPU: 118.2TFLOPS | MFU: 28.42% | HFU: 37.90% | lm_loss: 3.390894E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 18:58:05,989] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1595.10 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:58:19,005] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1501.88 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 18:58:33,213] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1996.67 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 18:58:46,747] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1903.38 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 18:58:59,499] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1601.07 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:59:13,034] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1646.57 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 18:59:25,726] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1585.33 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 18:59:38,603] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1704.74 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 18:59:51,798] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2043.60 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 19:00:04,575] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1612.71 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 19:00:04,575] [INFO] [logging.py:96:log_dist] [Rank 0] step=750, skipped=0, lr=[4.6875e-05, 4.6875e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 19:00:04,577] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 613.60 | fwd_microstep: 23877.12 | bwd_microstep: 68475.00 | bwd_inner_microstep: 68474.42 | bwd_allreduce_microstep: 0.07 | step_microstep: 17552.96
[2025-01-06 19:00:04,578] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23877.01 | bwd: 68474.84 | bwd_inner: 68474.32 | bwd_allreduce: 0.07 | step: 17553.15
steps: 750 loss: 3.3261 iter time (s): 13.121 samples/sec: 78.040
[2025-01-06 19:00:04,594] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.981 | iteration      750/  320000 | elapsed time per iteration (ms): 13131.3 | learning rate: 4.688E-05 | approx flops per GPU: 118.7TFLOPS | MFU: 28.54% | HFU: 38.05% | lm_loss: 3.393283E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 19:00:17,408] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1675.16 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 19:00:30,229] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1678.00 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 19:00:43,011] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1687.06 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 19:00:56,510] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1665.46 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 19:01:09,767] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1699.57 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 19:01:22,288] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1433.04 | optimizer_gradients: 1.03 | optimizer_step: 2.23
[2025-01-06 19:01:35,993] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1948.27 | optimizer_gradients: 1.12 | optimizer_step: 2.21
[2025-01-06 19:01:49,004] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1879.33 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 19:02:02,638] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2003.62 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 19:02:16,241] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1672.47 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 19:02:16,241] [INFO] [logging.py:96:log_dist] [Rank 0] step=760, skipped=0, lr=[4.7499999999999996e-05, 4.7499999999999996e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 19:02:16,243] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 663.28 | fwd_microstep: 23926.32 | bwd_microstep: 68532.11 | bwd_inner_microstep: 68531.50 | bwd_allreduce_microstep: 0.07 | step_microstep: 17863.99
[2025-01-06 19:02:16,244] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23926.21 | bwd: 68531.93 | bwd_inner: 68531.40 | bwd_allreduce: 0.07 | step: 17864.17
steps: 760 loss: 3.3769 iter time (s): 13.153 samples/sec: 77.850
[2025-01-06 19:02:16,254] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.776 | iteration      760/  320000 | elapsed time per iteration (ms): 13166.0 | learning rate: 4.750E-05 | approx flops per GPU: 118.4TFLOPS | MFU: 28.46% | HFU: 37.95% | lm_loss: 3.332676E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 19:02:29,225] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1824.29 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 19:02:42,895] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1927.47 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 19:02:56,046] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1952.86 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 19:03:09,526] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1921.59 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 19:03:22,834] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2188.84 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 19:03:35,653] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1715.57 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 19:03:49,008] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2191.69 | optimizer_gradients: 1.03 | optimizer_step: 2.22
[2025-01-06 19:04:02,335] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1689.96 | optimizer_gradients: 1.02 | optimizer_step: 2.21
[2025-01-06 19:04:15,481] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1582.98 | optimizer_gradients: 1.02 | optimizer_step: 2.19
[2025-01-06 19:04:28,547] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1907.18 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 19:04:28,548] [INFO] [logging.py:96:log_dist] [Rank 0] step=770, skipped=0, lr=[4.8125e-05, 4.8125e-05], mom=[[0.9, 0.95], [0.9, 0.95]]
[2025-01-06 19:04:28,549] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | batch_input: 531.48 | fwd_microstep: 23912.99 | bwd_microstep: 68421.37 | bwd_inner_microstep: 68420.79 | bwd_allreduce_microstep: 0.07 | step_microstep: 19337.24
[2025-01-06 19:04:28,550] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | fwd: 23912.88 | bwd: 68421.22 | bwd_inner: 68420.69 | bwd_allreduce: 0.07 | step: 19337.43
steps: 770 loss: 3.2767 iter time (s): 13.215 samples/sec: 77.486
[2025-01-06 19:04:28,566] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms)
 samples/sec: 77.392 | iteration      770/  320000 | elapsed time per iteration (ms): 13231.3 | learning rate: 4.812E-05 | approx flops per GPU: 117.8TFLOPS | MFU: 28.32% | HFU: 37.76% | lm_loss: 3.320547E+00 | loss scale: 65536.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms)
[2025-01-06 19:04:41,604] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1895.70 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 19:04:54,423] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1647.77 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 19:05:07,017] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1456.72 | optimizer_gradients: 1.03 | optimizer_step: 2.20
[2025-01-06 19:05:20,066] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1481.42 | optimizer_gradients: 1.02 | optimizer_step: 2.20
[2025-01-06 19:05:33,580] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 2030.83 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 19:05:46,542] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1818.62 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 19:05:59,574] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1884.53 | optimizer_gradients: 1.02 | optimizer_step: 2.22
[2025-01-06 19:06:12,956] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1827.64 | optimizer_gradients: 1.03 | optimizer_step: 2.21
[2025-01-06 19:06:25,425] [INFO] [logging.py:96:log_dist] [Rank 0] time (ms) | optimizer_allgather: 1380.21 | optimizer_gradients: 1.02 | optimizer_step: 2.21
