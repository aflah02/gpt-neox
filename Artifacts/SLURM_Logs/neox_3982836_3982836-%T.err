/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
INFO:root:NeoXArgs.calculate_derived() Total number of GPUs determined to be: 16
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
[W106 16:15:07.477500532 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [srv-22-0288.mpi-sws.org]:12802 (errno: 97 - Address family not supported by protocol).
[W106 16:15:07.482738455 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [srv-22-0288.mpi-sws.org]:12802 (errno: 97 - Address family not supported by protocol).
[W106 16:15:07.495043277 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [srv-22-0288.mpi-sws.org]:12802 (errno: 97 - Address family not supported by protocol).
[W106 16:15:07.496440329 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [srv-22-0288.mpi-sws.org]:12802 (errno: 97 - Address family not supported by protocol).
[W106 16:15:07.496689463 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [srv-22-0288.mpi-sws.org]:12802 (errno: 97 - Address family not supported by protocol).
[W106 16:15:07.499324179 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [srv-22-0288.mpi-sws.org]:12802 (errno: 97 - Address family not supported by protocol).
[W106 16:15:07.505745258 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [srv-22-0288.mpi-sws.org]:12802 (errno: 97 - Address family not supported by protocol).
wandb: Currently logged in as: aflah. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/wandb/run-20250106_161508-qqdvhafu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 1.3B-FA-BS-64-2x8xA100
wandb:  View project at https://wandb.ai/aflah/neox
wandb:  View run at https://wandb.ai/aflah/neox/runs/qqdvhafu
[W106 16:15:09.807409333 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [srv-22-0288.mpi-sws.org]:12802 (errno: 97 - Address family not supported by protocol).
/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data/gpt2_dataset.py:373: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data/gpt2_dataset.py:373: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data/gpt2_dataset.py:373: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data/gpt2_dataset.py:373: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data/gpt2_dataset.py:373: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data/gpt2_dataset.py:373: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data/gpt2_dataset.py:373: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
[W106 16:15:15.767172485 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [sws-8a100-02.mpi-sws.org]:12802 (errno: 97 - Address family not supported by protocol).
[W106 16:15:15.767147212 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [sws-8a100-02.mpi-sws.org]:12802 (errno: 97 - Address family not supported by protocol).
[W106 16:15:15.767426989 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [sws-8a100-02.mpi-sws.org]:12802 (errno: 97 - Address family not supported by protocol).
[W106 16:15:15.767616022 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [sws-8a100-02.mpi-sws.org]:12802 (errno: 97 - Address family not supported by protocol).
[W106 16:15:15.767778365 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [sws-8a100-02.mpi-sws.org]:12802 (errno: 97 - Address family not supported by protocol).
[W106 16:15:15.769532421 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [sws-8a100-02.mpi-sws.org]:12802 (errno: 97 - Address family not supported by protocol).
[W106 16:15:15.769823723 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [sws-8a100-02.mpi-sws.org]:12802 (errno: 97 - Address family not supported by protocol).
wandb: Currently logged in as: aflah. Use `wandb login --relogin` to force relogin
/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data/gpt2_dataset.py:373: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data/gpt2_dataset.py:373: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data/gpt2_dataset.py:373: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data/gpt2_dataset.py:373: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data/gpt2_dataset.py:373: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data/gpt2_dataset.py:373: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data/gpt2_dataset.py:373: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/wandb/run-20250106_161516-yyvnogub
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 1.3B-FA-BS-64-2x8xA100
wandb:  View project at https://wandb.ai/aflah/neox
wandb:  View run at https://wandb.ai/aflah/neox/runs/yyvnogub
[W106 16:15:18.263283905 socket.cpp:752] [c10d] The client socket cannot be initialized to connect to [sws-8a100-02.mpi-sws.org]:12802 (errno: 97 - Address family not supported by protocol).
/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data/gpt2_dataset.py:373: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
/NS/llm-pretraining/work/afkhan/USC_Colab/gpt-neox/megatron/data/gpt2_dataset.py:373: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  counts = torch.cuda.LongTensor([1])
Using /home/afkhan/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /home/afkhan/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /home/afkhan/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /home/afkhan/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /home/afkhan/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /home/afkhan/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /home/afkhan/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/afkhan/.cache/torch_extensions/py311_cu118/fused_adam/build.ninja...
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].
  warnings.warn(
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/afkhan/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /home/afkhan/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /home/afkhan/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /home/afkhan/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /home/afkhan/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /home/afkhan/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /home/afkhan/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /home/afkhan/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Using /home/afkhan/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
Loading extension module fused_adam...
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
slurmstepd-sws-8a100-04: error: Detected 1 oom-kill event(s) in StepId=3982836.0. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: sws-8a100-04: task 12: Out Of Memory
[rank10]:[E107 08:49:59.479938261 ProcessGroupNCCL.cpp:616] [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600051 milliseconds before timing out.
[rank1]:[E107 08:49:59.122796957 ProcessGroupNCCL.cpp:616] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600091 milliseconds before timing out.
[rank10]:[E107 08:49:59.562299092 ProcessGroupNCCL.cpp:1785] [PG ID 6 PG GUID 51 Rank 10] Exception (either an error or timeout) detected by watchdog at work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank10]:[E107 08:49:59.562316605 ProcessGroupNCCL.cpp:1834] [PG ID 6 PG GUID 51 Rank 10] Timeout at NCCL work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank10]:[E107 08:49:59.562325006 ProcessGroupNCCL.cpp:630] [Rank 10] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank10]:[E107 08:49:59.562330429 ProcessGroupNCCL.cpp:636] [Rank 10] To avoid data inconsistency, we are taking the entire process down.
[rank5]:[E107 08:49:59.158522590 ProcessGroupNCCL.cpp:616] [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600043 milliseconds before timing out.
[rank1]:[E107 08:49:59.178503868 ProcessGroupNCCL.cpp:1785] [PG ID 6 PG GUID 51 Rank 1] Exception (either an error or timeout) detected by watchdog at work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank1]:[E107 08:49:59.178517236 ProcessGroupNCCL.cpp:1834] [PG ID 6 PG GUID 51 Rank 1] Timeout at NCCL work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank1]:[E107 08:49:59.178524555 ProcessGroupNCCL.cpp:630] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E107 08:49:59.178528492 ProcessGroupNCCL.cpp:636] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank5]:[E107 08:49:59.178499044 ProcessGroupNCCL.cpp:1785] [PG ID 6 PG GUID 51 Rank 5] Exception (either an error or timeout) detected by watchdog at work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank5]:[E107 08:49:59.178511903 ProcessGroupNCCL.cpp:1834] [PG ID 6 PG GUID 51 Rank 5] Timeout at NCCL work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank5]:[E107 08:49:59.178519379 ProcessGroupNCCL.cpp:630] [Rank 5] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank5]:[E107 08:49:59.178523480 ProcessGroupNCCL.cpp:636] [Rank 5] To avoid data inconsistency, we are taking the entire process down.
[rank3]:[E107 08:49:59.185409059 ProcessGroupNCCL.cpp:616] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600076 milliseconds before timing out.
[rank3]:[E107 08:49:59.186014077 ProcessGroupNCCL.cpp:1785] [PG ID 6 PG GUID 51 Rank 3] Exception (either an error or timeout) detected by watchdog at work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank3]:[E107 08:49:59.186025296 ProcessGroupNCCL.cpp:1834] [PG ID 6 PG GUID 51 Rank 3] Timeout at NCCL work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank3]:[E107 08:49:59.186033051 ProcessGroupNCCL.cpp:630] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E107 08:49:59.186036511 ProcessGroupNCCL.cpp:636] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E107 08:49:59.189863759 ProcessGroupNCCL.cpp:1595] [PG ID 6 PG GUID 51 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600091 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f90cd56c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f907f169672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f907f170ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f907f17251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f90cd9ac5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7f90ce9331c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7f90ce9b385c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
[rank5]:[E107 08:49:59.189875237 ProcessGroupNCCL.cpp:1595] [PG ID 6 PG GUID 51 Rank 5] Process group watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600043 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f7e5ff6c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f7e11b69672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f7e11b70ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f7e11b7251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f7e605935c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7f7e613e21c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7f7e6146285c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 6 PG GUID 51 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600091 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f90cd56c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f907f169672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f907f170ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f907f17251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f90cd9ac5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7f90ce9331c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7f90ce9b385c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f90cd56c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe3f297 (0x7f907edea297 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f90cd9ac5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x891c4 (0x7f90ce9331c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x10985c (0x7f90ce9b385c in /lib/x86_64-linux-gnu/libc.so.6)

[rank10]:[E107 08:49:59.601484898 ProcessGroupNCCL.cpp:1595] [PG ID 6 PG GUID 51 Rank 10] Process group watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600051 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f90e216c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f9093d69672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f9093d70ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f9093d7251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f90e25995c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7f90e35371c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7f90e35b785c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
[rank3]:[E107 08:49:59.191707879 ProcessGroupNCCL.cpp:1595] [PG ID 6 PG GUID 51 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600076 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f423feb9446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f41f1b69672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f41f1b70ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f41f1b7251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f42410015c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7f42412611c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7f42412e185c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 6 PG GUID 51 Rank 5] Process group watchdog thread terminated with exception: [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600043 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f7e5ff6c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f7e11b69672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f7e11b70ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f7e11b7251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f7e605935c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7f7e613e21c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7f7e6146285c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f7e5ff6c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe3f297 (0x7f7e117ea297 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f7e605935c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x891c4 (0x7f7e613e21c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x10985c (0x7f7e6146285c in /lib/x86_64-linux-gnu/libc.so.6)

  what():  [PG ID 6 PG GUID 51 Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600076 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f423feb9446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f41f1b69672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f41f1b70ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f41f1b7251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f42410015c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7f42412611c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7f42412e185c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f423feb9446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe3f297 (0x7f41f17ea297 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f42410015c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x891c4 (0x7f42412611c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x10985c (0x7f42412e185c in /lib/x86_64-linux-gnu/libc.so.6)

  what():  [PG ID 6 PG GUID 51 Rank 10] Process group watchdog thread terminated with exception: [Rank 10] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600051 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f90e216c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f9093d69672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f9093d70ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f9093d7251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f90e25995c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7f90e35371c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7f90e35b785c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f90e216c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe3f297 (0x7f90939ea297 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f90e25995c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x891c4 (0x7f90e35371c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x10985c (0x7f90e35b785c in /lib/x86_64-linux-gnu/libc.so.6)

[rank6]:[E107 08:49:59.217258488 ProcessGroupNCCL.cpp:616] [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600096 milliseconds before timing out.
[rank6]:[E107 08:49:59.217880637 ProcessGroupNCCL.cpp:1785] [PG ID 6 PG GUID 51 Rank 6] Exception (either an error or timeout) detected by watchdog at work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank6]:[E107 08:49:59.217893029 ProcessGroupNCCL.cpp:1834] [PG ID 6 PG GUID 51 Rank 6] Timeout at NCCL work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank6]:[E107 08:49:59.217901671 ProcessGroupNCCL.cpp:630] [Rank 6] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank6]:[E107 08:49:59.217905233 ProcessGroupNCCL.cpp:636] [Rank 6] To avoid data inconsistency, we are taking the entire process down.
[rank6]:[E107 08:49:59.220616558 ProcessGroupNCCL.cpp:1595] [PG ID 6 PG GUID 51 Rank 6] Process group watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600096 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f6ed2f6c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f6e84b69672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f6e84b70ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f6e84b7251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f6ed34235c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7f6ed43a71c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7f6ed442785c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 6 PG GUID 51 Rank 6] Process group watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600096 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f6ed2f6c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f6e84b69672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f6e84b70ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f6e84b7251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f6ed34235c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7f6ed43a71c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7f6ed442785c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f6ed2f6c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe3f297 (0x7f6e847ea297 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f6ed34235c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x891c4 (0x7f6ed43a71c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x10985c (0x7f6ed442785c in /lib/x86_64-linux-gnu/libc.so.6)

[rank0]:[E107 08:49:59.222501495 ProcessGroupNCCL.cpp:616] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600027 milliseconds before timing out.
[rank0]:[E107 08:49:59.222800090 ProcessGroupNCCL.cpp:1785] [PG ID 6 PG GUID 51 Rank 0] Exception (either an error or timeout) detected by watchdog at work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank0]:[E107 08:49:59.222807480 ProcessGroupNCCL.cpp:1834] [PG ID 6 PG GUID 51 Rank 0] Timeout at NCCL work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank0]:[E107 08:49:59.222812890 ProcessGroupNCCL.cpp:630] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E107 08:49:59.222816011 ProcessGroupNCCL.cpp:636] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E107 08:49:59.225548766 ProcessGroupNCCL.cpp:1595] [PG ID 6 PG GUID 51 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600027 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7feaa1d6c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7fea53969672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7fea53970ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fea5397251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7feaa22ee5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7feaa31471c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7feaa31c785c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 6 PG GUID 51 Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600027 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7feaa1d6c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7fea53969672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7fea53970ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fea5397251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7feaa22ee5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7feaa31471c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7feaa31c785c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7feaa1d6c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe3f297 (0x7fea535ea297 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7feaa22ee5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x891c4 (0x7feaa31471c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x10985c (0x7feaa31c785c in /lib/x86_64-linux-gnu/libc.so.6)

[rank4]:[E107 08:49:59.323163906 ProcessGroupNCCL.cpp:616] [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600060 milliseconds before timing out.
[rank4]:[E107 08:49:59.323514413 ProcessGroupNCCL.cpp:1785] [PG ID 6 PG GUID 51 Rank 4] Exception (either an error or timeout) detected by watchdog at work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank4]:[E107 08:49:59.323525810 ProcessGroupNCCL.cpp:1834] [PG ID 6 PG GUID 51 Rank 4] Timeout at NCCL work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank4]:[E107 08:49:59.323532911 ProcessGroupNCCL.cpp:630] [Rank 4] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank4]:[E107 08:49:59.323537023 ProcessGroupNCCL.cpp:636] [Rank 4] To avoid data inconsistency, we are taking the entire process down.
[rank4]:[E107 08:49:59.326245958 ProcessGroupNCCL.cpp:1595] [PG ID 6 PG GUID 51 Rank 4] Process group watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600060 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f81666b9446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f8118369672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f8118370ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f811837251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f8166b3e5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7f8167adb1c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7f8167b5b85c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 6 PG GUID 51 Rank 4] Process group watchdog thread terminated with exception: [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600060 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f81666b9446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f8118369672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f8118370ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f811837251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f8166b3e5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7f8167adb1c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7f8167b5b85c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f81666b9446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe3f297 (0x7f8117fea297 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f8166b3e5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x891c4 (0x7f8167adb1c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x10985c (0x7f8167b5b85c in /lib/x86_64-linux-gnu/libc.so.6)

[rank7]:[E107 08:49:59.385624751 ProcessGroupNCCL.cpp:616] [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600040 milliseconds before timing out.
[rank7]:[E107 08:49:59.386036060 ProcessGroupNCCL.cpp:1785] [PG ID 6 PG GUID 51 Rank 7] Exception (either an error or timeout) detected by watchdog at work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank7]:[E107 08:49:59.386047773 ProcessGroupNCCL.cpp:1834] [PG ID 6 PG GUID 51 Rank 7] Timeout at NCCL work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank7]:[E107 08:49:59.386054950 ProcessGroupNCCL.cpp:630] [Rank 7] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank7]:[E107 08:49:59.386058764 ProcessGroupNCCL.cpp:636] [Rank 7] To avoid data inconsistency, we are taking the entire process down.
[rank7]:[E107 08:49:59.388882738 ProcessGroupNCCL.cpp:1595] [PG ID 6 PG GUID 51 Rank 7] Process group watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600040 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f758336c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f7534f69672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f7534f70ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f7534f7251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f75838db5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7f75847281c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7f75847a885c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 6 PG GUID 51 Rank 7] Process group watchdog thread terminated with exception: [Rank 7] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600040 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f758336c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f7534f69672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f7534f70ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f7534f7251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f75838db5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7f75847281c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7f75847a885c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f758336c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe3f297 (0x7f7534bea297 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f75838db5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x891c4 (0x7f75847281c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x10985c (0x7f75847a885c in /lib/x86_64-linux-gnu/libc.so.6)

[rank14]:[E107 08:49:59.869640938 ProcessGroupNCCL.cpp:616] [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600082 milliseconds before timing out.
[rank14]:[E107 08:49:59.869745524 ProcessGroupNCCL.cpp:1785] [PG ID 6 PG GUID 51 Rank 14] Exception (either an error or timeout) detected by watchdog at work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank14]:[E107 08:49:59.869755878 ProcessGroupNCCL.cpp:1834] [PG ID 6 PG GUID 51 Rank 14] Timeout at NCCL work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank14]:[E107 08:49:59.869763664 ProcessGroupNCCL.cpp:630] [Rank 14] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank14]:[E107 08:49:59.869767416 ProcessGroupNCCL.cpp:636] [Rank 14] To avoid data inconsistency, we are taking the entire process down.
[rank14]:[E107 08:49:59.871248467 ProcessGroupNCCL.cpp:1595] [PG ID 6 PG GUID 51 Rank 14] Process group watchdog thread terminated with exception: [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600082 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fe2ecab9446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7fe29e769672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7fe29e770ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fe29e77251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fe2ecfee5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7fe2ede4e1c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7fe2edece85c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 6 PG GUID 51 Rank 14] Process group watchdog thread terminated with exception: [Rank 14] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600082 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fe2ecab9446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7fe29e769672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7fe29e770ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fe29e77251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fe2ecfee5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7fe2ede4e1c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7fe2edece85c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fe2ecab9446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe3f297 (0x7fe29e3ea297 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7fe2ecfee5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x891c4 (0x7fe2ede4e1c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x10985c (0x7fe2edece85c in /lib/x86_64-linux-gnu/libc.so.6)

[rank15]:[E107 08:49:59.879758148 ProcessGroupNCCL.cpp:616] [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600064 milliseconds before timing out.
[rank15]:[E107 08:49:59.879837948 ProcessGroupNCCL.cpp:1785] [PG ID 6 PG GUID 51 Rank 15] Exception (either an error or timeout) detected by watchdog at work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank15]:[E107 08:49:59.879846053 ProcessGroupNCCL.cpp:1834] [PG ID 6 PG GUID 51 Rank 15] Timeout at NCCL work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank15]:[E107 08:49:59.879854120 ProcessGroupNCCL.cpp:630] [Rank 15] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank15]:[E107 08:49:59.879857801 ProcessGroupNCCL.cpp:636] [Rank 15] To avoid data inconsistency, we are taking the entire process down.
[rank15]:[E107 08:49:59.881286706 ProcessGroupNCCL.cpp:1595] [PG ID 6 PG GUID 51 Rank 15] Process group watchdog thread terminated with exception: [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600064 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f374496c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f36f6569672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f36f6570ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f36f657251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f3744e3b5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7f3745dd21c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7f3745e5285c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 6 PG GUID 51 Rank 15] Process group watchdog thread terminated with exception: [Rank 15] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600064 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f374496c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f36f6569672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f36f6570ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f36f657251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f3744e3b5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7f3745dd21c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7f3745e5285c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f374496c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe3f297 (0x7f36f61ea297 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f3744e3b5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x891c4 (0x7f3745dd21c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x10985c (0x7f3745e5285c in /lib/x86_64-linux-gnu/libc.so.6)

[rank2]:[E107 08:49:59.537619995 ProcessGroupNCCL.cpp:616] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600089 milliseconds before timing out.
[rank2]:[E107 08:49:59.538305478 ProcessGroupNCCL.cpp:1785] [PG ID 6 PG GUID 51 Rank 2] Exception (either an error or timeout) detected by watchdog at work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank2]:[E107 08:49:59.538318150 ProcessGroupNCCL.cpp:1834] [PG ID 6 PG GUID 51 Rank 2] Timeout at NCCL work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank2]:[E107 08:49:59.538325314 ProcessGroupNCCL.cpp:630] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E107 08:49:59.538329235 ProcessGroupNCCL.cpp:636] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[E107 08:49:59.541290628 ProcessGroupNCCL.cpp:1595] [PG ID 6 PG GUID 51 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600089 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fcaa02b9446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7fca51f69672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7fca51f70ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fca51f7251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fcaa07ee5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7fcaa164f1c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7fcaa16cf85c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 6 PG GUID 51 Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600089 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fcaa02b9446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7fca51f69672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7fca51f70ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fca51f7251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fcaa07ee5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7fcaa164f1c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7fcaa16cf85c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fcaa02b9446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe3f297 (0x7fca51bea297 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7fcaa07ee5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x891c4 (0x7fcaa164f1c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x10985c (0x7fcaa16cf85c in /lib/x86_64-linux-gnu/libc.so.6)

[rank8]:[E107 08:49:59.976755289 ProcessGroupNCCL.cpp:616] [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600096 milliseconds before timing out.
[rank8]:[E107 08:49:59.976861397 ProcessGroupNCCL.cpp:1785] [PG ID 6 PG GUID 51 Rank 8] Exception (either an error or timeout) detected by watchdog at work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank8]:[E107 08:49:59.976872756 ProcessGroupNCCL.cpp:1834] [PG ID 6 PG GUID 51 Rank 8] Timeout at NCCL work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank8]:[E107 08:49:59.976879722 ProcessGroupNCCL.cpp:630] [Rank 8] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank8]:[E107 08:49:59.976883688 ProcessGroupNCCL.cpp:636] [Rank 8] To avoid data inconsistency, we are taking the entire process down.
[rank8]:[E107 08:49:59.978368713 ProcessGroupNCCL.cpp:1595] [PG ID 6 PG GUID 51 Rank 8] Process group watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600096 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ff3be16c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7ff36fd69672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7ff36fd70ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ff36fd7251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7ff3be64a5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7ff3bf5cd1c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7ff3bf64d85c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 6 PG GUID 51 Rank 8] Process group watchdog thread terminated with exception: [Rank 8] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600096 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ff3be16c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7ff36fd69672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7ff36fd70ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ff36fd7251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7ff3be64a5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7ff3bf5cd1c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7ff3bf64d85c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ff3be16c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe3f297 (0x7ff36f9ea297 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7ff3be64a5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x891c4 (0x7ff3bf5cd1c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x10985c (0x7ff3bf64d85c in /lib/x86_64-linux-gnu/libc.so.6)

[rank13]:[E107 08:49:59.011014503 ProcessGroupNCCL.cpp:616] [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600054 milliseconds before timing out.
[rank13]:[E107 08:49:59.011098376 ProcessGroupNCCL.cpp:1785] [PG ID 6 PG GUID 51 Rank 13] Exception (either an error or timeout) detected by watchdog at work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank13]:[E107 08:49:59.011107184 ProcessGroupNCCL.cpp:1834] [PG ID 6 PG GUID 51 Rank 13] Timeout at NCCL work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank13]:[E107 08:49:59.011113807 ProcessGroupNCCL.cpp:630] [Rank 13] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank13]:[E107 08:49:59.011117324 ProcessGroupNCCL.cpp:636] [Rank 13] To avoid data inconsistency, we are taking the entire process down.
[rank13]:[E107 08:49:59.012569725 ProcessGroupNCCL.cpp:1595] [PG ID 6 PG GUID 51 Rank 13] Process group watchdog thread terminated with exception: [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600054 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f232356c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f22d5169672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f22d5170ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f22d517251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f2323adb5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7f23249241c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7f23249a485c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 6 PG GUID 51 Rank 13] Process group watchdog thread terminated with exception: [Rank 13] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600054 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f232356c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f22d5169672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f22d5170ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f22d517251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f2323adb5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7f23249241c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7f23249a485c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f232356c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe3f297 (0x7f22d4dea297 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f2323adb5c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x891c4 (0x7f23249241c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x10985c (0x7f23249a485c in /lib/x86_64-linux-gnu/libc.so.6)

[rank11]:[E107 08:49:59.060450278 ProcessGroupNCCL.cpp:616] [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600006 milliseconds before timing out.
[rank11]:[E107 08:49:59.060546025 ProcessGroupNCCL.cpp:1785] [PG ID 6 PG GUID 51 Rank 11] Exception (either an error or timeout) detected by watchdog at work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank11]:[E107 08:49:59.060554415 ProcessGroupNCCL.cpp:1834] [PG ID 6 PG GUID 51 Rank 11] Timeout at NCCL work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank11]:[E107 08:49:59.060561000 ProcessGroupNCCL.cpp:630] [Rank 11] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank11]:[E107 08:49:59.060564656 ProcessGroupNCCL.cpp:636] [Rank 11] To avoid data inconsistency, we are taking the entire process down.
[rank11]:[E107 08:49:59.062009789 ProcessGroupNCCL.cpp:1595] [PG ID 6 PG GUID 51 Rank 11] Process group watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600006 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f41aa16c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f415bd69672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f415bd70ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f415bd7251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f41aa7525c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7f41ab5a11c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7f41ab62185c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 6 PG GUID 51 Rank 11] Process group watchdog thread terminated with exception: [Rank 11] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600006 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f41aa16c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f415bd69672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f415bd70ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f415bd7251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7f41aa7525c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7f41ab5a11c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7f41ab62185c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f41aa16c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe3f297 (0x7f415b9ea297 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f41aa7525c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x891c4 (0x7f41ab5a11c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x10985c (0x7f41ab62185c in /lib/x86_64-linux-gnu/libc.so.6)

srun: error: sws-8a100-02: tasks 1,3,5-7: Aborted
[rank9]:[E107 08:50:02.017867690 ProcessGroupNCCL.cpp:616] [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600033 milliseconds before timing out.
[rank9]:[E107 08:50:02.018138951 ProcessGroupNCCL.cpp:1785] [PG ID 6 PG GUID 51 Rank 9] Exception (either an error or timeout) detected by watchdog at work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank9]:[E107 08:50:02.018168721 ProcessGroupNCCL.cpp:1834] [PG ID 6 PG GUID 51 Rank 9] Timeout at NCCL work: 40827, last enqueued NCCL work: 40829, last completed NCCL work: 40826.
[rank9]:[E107 08:50:02.018195457 ProcessGroupNCCL.cpp:630] [Rank 9] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank9]:[E107 08:50:02.018209702 ProcessGroupNCCL.cpp:636] [Rank 9] To avoid data inconsistency, we are taking the entire process down.
[rank9]:[E107 08:50:02.023123325 ProcessGroupNCCL.cpp:1595] [PG ID 6 PG GUID 51 Rank 9] Process group watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600033 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fa004d6c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f9fb6969672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f9fb6970ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f9fb697251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fa0052865c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7fa0062071c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7fa00628785c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 6 PG GUID 51 Rank 9] Process group watchdog thread terminated with exception: [Rank 9] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=40827, OpType=ALLREDUCE, NumelIn=488898560, NumelOut=488898560, Timeout(ms)=600000) ran for 600033 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fa004d6c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7f9fb6969672 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7f9fb6970ab3 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f9fb697251d in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7fa0052865c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x891c4 (0x7fa0062071c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x10985c (0x7fa00628785c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7fa004d6c446 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe3f297 (0x7f9fb65ea297 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7fa0052865c0 in /NS/venvs/work/afkhan/neoxolmo/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x891c4 (0x7fa0062071c4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x10985c (0x7fa00628785c in /lib/x86_64-linux-gnu/libc.so.6)

srun: error: sws-8a100-02: tasks 0,2,4: Aborted
